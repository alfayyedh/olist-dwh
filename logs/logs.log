2024-11-11 00:04:12,576 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-11-11 00:04:13,205 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-11-11 00:04:13,215 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-11-11 00:04:14,605 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-11-11 00:04:14,630 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-11-11 00:04:16,712 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-11-11 00:04:17,719 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-11-11 00:04:18,689 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-11-11 00:04:18,830 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-11-11 00:04:20,375 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-11-11 00:04:20,375 - INFO - Extract All Tables From Sources - SUCCESS
2024-11-11 00:04:20,384 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-11-11 00:04:20,434 - INFO - [pid 93349] Worker Worker(salt=6390486614, workers=1, host=MSI, username=alfayyedh, pid=93349) done      Extract()
2024-11-11 00:04:20,437 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 00:04:20,444 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-11-11 00:04:20,445 - DEBUG - Asking scheduler for work...
2024-11-11 00:04:20,448 - DEBUG - Pending tasks: 2
2024-11-11 00:04:20,448 - INFO - [pid 93349] Worker Worker(salt=6390486614, workers=1, host=MSI, username=alfayyedh, pid=93349) running   Load()
2024-11-11 00:04:20,455 - INFO - Read Load Query - SUCCESS
2024-11-11 00:04:22,362 - INFO - Read Extracted Data - SUCCESS
2024-11-11 00:04:22,363 - INFO - Connect to DWH - SUCCESS
2024-11-11 00:04:22,502 - ERROR - Truncate public Schema in DWH - FAILED
2024-11-11 00:04:22,553 - ERROR - [pid 93349] Worker Worker(salt=6390486614, workers=1, host=MSI, username=alfayyedh, pid=93349) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 142, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 154, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-11-11 00:04:22,578 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 00:04:22,587 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-11 00:04:22,588 - DEBUG - Asking scheduler for work...
2024-11-11 00:04:22,594 - DEBUG - Done
2024-11-11 00:04:22,595 - DEBUG - There are no more tasks to run at this time
2024-11-11 00:04:22,595 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-11 00:04:22,595 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-11 00:04:22,595 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-11 00:04:22,595 - INFO - Worker Worker(salt=6390486614, workers=1, host=MSI, username=alfayyedh, pid=93349) was stopped. Shutting down Keep-Alive thread
2024-11-11 00:04:22,600 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-11 00:11:21,682 - INFO - Read Load Query - SUCCESS
2024-11-11 00:11:22,753 - INFO - Read Extracted Data - SUCCESS
2024-11-11 00:11:22,798 - INFO - Connect to DWH - SUCCESS
2024-11-11 00:11:22,817 - ERROR - Truncate public Schema in DWH - FAILED
2024-11-11 00:11:22,843 - ERROR - [pid 96468] Worker Worker(salt=8300972236, workers=1, host=MSI, username=alfayyedh, pid=96468) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 142, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 154, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-11-11 00:11:22,865 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 00:11:22,870 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-11 00:11:22,870 - DEBUG - Asking scheduler for work...
2024-11-11 00:11:22,872 - DEBUG - Done
2024-11-11 00:11:22,872 - DEBUG - There are no more tasks to run at this time
2024-11-11 00:11:22,872 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-11 00:11:22,872 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-11 00:11:22,872 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-11 00:11:22,872 - INFO - Worker Worker(salt=8300972236, workers=1, host=MSI, username=alfayyedh, pid=96468) was stopped. Shutting down Keep-Alive thread
2024-11-11 00:11:22,873 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-11 00:37:53,912 - INFO - Read Load Query - SUCCESS
2024-11-11 00:37:55,594 - INFO - Read Extracted Data - SUCCESS
2024-11-11 00:37:55,638 - INFO - Connect to DWH - SUCCESS
2024-11-11 00:37:55,681 - ERROR - Truncate public Schema in DWH - FAILED
2024-11-11 00:37:55,715 - ERROR - [pid 107439] Worker Worker(salt=5582145341, workers=1, host=MSI, username=alfayyedh, pid=107439) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-11-11 00:37:55,738 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 00:37:55,746 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-11 00:37:55,746 - DEBUG - Asking scheduler for work...
2024-11-11 00:37:55,749 - DEBUG - Done
2024-11-11 00:37:55,749 - DEBUG - There are no more tasks to run at this time
2024-11-11 00:37:55,749 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-11 00:37:55,749 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-11 00:37:55,749 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-11 00:37:55,750 - INFO - Worker Worker(salt=5582145341, workers=1, host=MSI, username=alfayyedh, pid=107439) was stopped. Shutting down Keep-Alive thread
2024-11-11 00:37:55,751 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-11 00:38:49,662 - INFO - Read Load Query - SUCCESS
2024-11-11 00:38:50,535 - INFO - Read Extracted Data - SUCCESS
2024-11-11 00:38:50,569 - INFO - Connect to DWH - SUCCESS
2024-11-11 00:38:50,593 - ERROR - Truncate public Schema in DWH - FAILED
2024-11-11 00:38:50,622 - ERROR - [pid 107944] Worker Worker(salt=2571960355, workers=1, host=MSI, username=alfayyedh, pid=107944) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-11-11 00:38:50,640 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 00:38:50,647 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-11 00:38:50,648 - DEBUG - Asking scheduler for work...
2024-11-11 00:38:50,651 - DEBUG - Done
2024-11-11 00:38:50,652 - DEBUG - There are no more tasks to run at this time
2024-11-11 00:38:50,652 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-11 00:38:50,652 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-11 00:38:50,652 - INFO - Worker Worker(salt=2571960355, workers=1, host=MSI, username=alfayyedh, pid=107944) was stopped. Shutting down Keep-Alive thread
2024-11-11 00:38:50,656 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-11 00:41:14,478 - INFO - Read Load Query - SUCCESS
2024-11-11 00:41:15,277 - INFO - Read Extracted Data - SUCCESS
2024-11-11 00:41:15,317 - INFO - Connect to DWH - SUCCESS
2024-11-11 00:41:15,337 - ERROR - Truncate public Schema in DWH - FAILED
2024-11-11 00:41:15,365 - ERROR - [pid 109025] Worker Worker(salt=7161782182, workers=1, host=MSI, username=alfayyedh, pid=109025) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-11-11 00:41:15,380 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 00:41:15,385 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-11 00:41:15,385 - DEBUG - Asking scheduler for work...
2024-11-11 00:41:15,387 - DEBUG - Done
2024-11-11 00:41:15,387 - DEBUG - There are no more tasks to run at this time
2024-11-11 00:41:15,387 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-11 00:41:15,387 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-11 00:41:15,387 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-11 00:41:15,387 - INFO - Worker Worker(salt=7161782182, workers=1, host=MSI, username=alfayyedh, pid=109025) was stopped. Shutting down Keep-Alive thread
2024-11-11 00:41:15,388 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-11 00:49:42,714 - INFO - Read Load Query - SUCCESS
2024-11-11 00:49:43,480 - INFO - Read Extracted Data - SUCCESS
2024-11-11 00:49:43,518 - INFO - Connect to DWH - SUCCESS
2024-11-11 00:49:43,541 - ERROR - Truncate public Schema in DWH - FAILED
2024-11-11 00:49:43,571 - ERROR - [pid 112627] Worker Worker(salt=7552725476, workers=1, host=MSI, username=alfayyedh, pid=112627) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-11-11 00:49:43,597 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 00:49:43,605 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-11 00:49:43,606 - DEBUG - Asking scheduler for work...
2024-11-11 00:49:43,610 - DEBUG - Done
2024-11-11 00:49:43,610 - DEBUG - There are no more tasks to run at this time
2024-11-11 00:49:43,610 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-11 00:49:43,610 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-11 00:49:43,610 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-11 00:49:43,611 - INFO - Worker Worker(salt=7552725476, workers=1, host=MSI, username=alfayyedh, pid=112627) was stopped. Shutting down Keep-Alive thread
2024-11-11 00:49:43,612 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-11 07:32:23,244 - INFO - Read Load Query - SUCCESS
2024-11-11 07:32:24,031 - INFO - Read Extracted Data - SUCCESS
2024-11-11 07:32:24,090 - INFO - Connect to DWH - SUCCESS
2024-11-11 07:32:24,090 - ERROR - Truncate public Schema in DWH - FAILED
2024-11-11 07:32:24,116 - ERROR - [pid 5788] Worker Worker(salt=2779076529, workers=1, host=MSI, username=alfayyedh, pid=5788) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5439 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2181, in _execute_internal
    conn = self._connection_for_bind(bind)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2050, in _connection_for_bind
    return trans._connection_for_bind(engine, execution_options)
  File "<string>", line 2, in _connection_for_bind
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1144, in _connection_for_bind
    conn = bind.connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5439 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-11-11 07:32:24,149 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-11 07:32:24,155 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-11 07:32:24,155 - DEBUG - Asking scheduler for work...
2024-11-11 07:32:24,156 - DEBUG - Done
2024-11-11 07:32:24,156 - DEBUG - There are no more tasks to run at this time
2024-11-11 07:32:24,157 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-11 07:32:24,157 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-11 07:32:24,157 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-11 07:32:24,157 - INFO - Worker Worker(salt=2779076529, workers=1, host=MSI, username=alfayyedh, pid=5788) was stopped. Shutting down Keep-Alive thread
2024-11-11 07:32:24,158 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-14 21:16:09,737 - INFO - Read Load Query - SUCCESS
2024-11-14 21:16:11,761 - INFO - Read Extracted Data - SUCCESS
2024-11-14 21:16:11,843 - INFO - Connect to DWH - SUCCESS
2024-11-14 21:16:12,075 - INFO - Truncate public Schema in DWH - SUCCESS
2024-11-14 21:16:12,075 - INFO - ==================================STARTING LOAD DATA=======================================
2024-11-14 21:16:12,581 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-11-14 21:16:12,620 - ERROR - LOAD All Tables To DWH - FAILED
2024-11-14 21:16:12,623 - ERROR - [pid 16741] Worker Worker(salt=2978244729, workers=1, host=MSI, username=alfayyedh, pid=16741) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "product_id" of relation "customers" does not exist
LINE 1: INSERT INTO public.customers (product_id, product_category_n...
                                      ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 170, in run
    customers.to_sql('customers',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "product_id" of relation "customers" does not exist
LINE 1: INSERT INTO public.customers (product_id, product_category_n...
                                      ^

[SQL: INSERT INTO public.customers (product_id, product_category_name, product_name_lenght, product_description_lenght, product_photos_qty, product_weight_g, product_length_cm, product_height_cm, product_width_cm) VALUES (%(product_id__0)s, %(product_categ ... 259873 characters truncated ... _weight_g__999)s, %(product_length_cm__999)s, %(product_height_cm__999)s, %(product_width_cm__999)s)]
[parameters: {'product_category_name__0': 'perfumaria', 'product_width_cm__0': 14.0, 'product_height_cm__0': 10.0, 'product_photos_qty__0': 1.0, 'product_description_lenght__0': 287.0, 'product_name_lenght__0': 40.0, 'product_weight_g__0': 225.0, 'product_length_cm__0': 16.0, 'product_id__0': '1e9e8ef04dbcff4541ed26657ea517e5', 'product_category_name__1': 'artes', 'product_width_cm__1': 20.0, 'product_height_cm__1': 18.0, 'product_photos_qty__1': 1.0, 'product_description_lenght__1': 276.0, 'product_name_lenght__1': 44.0, 'product_weight_g__1': 1000.0, 'product_length_cm__1': 30.0, 'product_id__1': '3aa071139cb16b67ca9e5dea641aaa2f', 'product_category_name__2': 'esporte_lazer', 'product_width_cm__2': 15.0, 'product_height_cm__2': 9.0, 'product_photos_qty__2': 1.0, 'product_description_lenght__2': 250.0, 'product_name_lenght__2': 46.0, 'product_weight_g__2': 154.0, 'product_length_cm__2': 18.0, 'product_id__2': '96bd76ec8810374ed1b65e291975717f', 'product_category_name__3': 'bebes', 'product_width_cm__3': 26.0, 'product_height_cm__3': 4.0, 'product_photos_qty__3': 1.0, 'product_description_lenght__3': 261.0, 'product_name_lenght__3': 27.0, 'product_weight_g__3': 371.0, 'product_length_cm__3': 26.0, 'product_id__3': 'cef67bcfe19066a932b7673e239eb23d', 'product_category_name__4': 'utilidades_domesticas', 'product_width_cm__4': 13.0, 'product_height_cm__4': 17.0, 'product_photos_qty__4': 4.0, 'product_description_lenght__4': 402.0, 'product_name_lenght__4': 37.0, 'product_weight_g__4': 625.0, 'product_length_cm__4': 20.0, 'product_id__4': '9dc1a7de274444849c219cff195d0b71', 'product_category_name__5': 'instrumentos_musicais', 'product_width_cm__5': 11.0, 'product_height_cm__5': 5.0, 'product_photos_qty__5': 1.0, 'product_description_lenght__5': 745.0 ... 8900 parameters truncated ... 'product_description_lenght__994': 267.0, 'product_name_lenght__994': 48.0, 'product_weight_g__994': 1588.0, 'product_length_cm__994': 48.0, 'product_id__994': 'a93ad122024312ca432da7f7b32474b4', 'product_category_name__995': 'pet_shop', 'product_width_cm__995': 22.0, 'product_height_cm__995': 16.0, 'product_photos_qty__995': 6.0, 'product_description_lenght__995': 287.0, 'product_name_lenght__995': 40.0, 'product_weight_g__995': 1400.0, 'product_length_cm__995': 20.0, 'product_id__995': '98bf9f3941be6afbca9ea37f8eb0c466', 'product_category_name__996': 'automotivo', 'product_width_cm__996': 38.0, 'product_height_cm__996': 7.0, 'product_photos_qty__996': 5.0, 'product_description_lenght__996': 797.0, 'product_name_lenght__996': 58.0, 'product_weight_g__996': 1100.0, 'product_length_cm__996': 23.0, 'product_id__996': '1fd95182c672cf5b10c82207ca5dab97', 'product_category_name__997': 'moveis_decoracao', 'product_width_cm__997': 28.0, 'product_height_cm__997': 6.0, 'product_photos_qty__997': 2.0, 'product_description_lenght__997': 250.0, 'product_name_lenght__997': 29.0, 'product_weight_g__997': 800.0, 'product_length_cm__997': 32.0, 'product_id__997': 'ac01b71ddb644fe8806e71a6030accf9', 'product_category_name__998': 'esporte_lazer', 'product_width_cm__998': 18.0, 'product_height_cm__998': 15.0, 'product_photos_qty__998': 1.0, 'product_description_lenght__998': 1134.0, 'product_name_lenght__998': 45.0, 'product_weight_g__998': 867.0, 'product_length_cm__998': 26.0, 'product_id__998': '35e26f65c3f380cfe6257d771fb067d2', 'product_category_name__999': 'informatica_acessorios', 'product_width_cm__999': 20.0, 'product_height_cm__999': 5.0, 'product_photos_qty__999': 3.0, 'product_description_lenght__999': 999.0, 'product_name_lenght__999': 32.0, 'product_weight_g__999': 225.0, 'product_length_cm__999': 17.0, 'product_id__999': '089cc17c8bf2226d13648392e33dabd7'}]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-11-14 21:16:12,740 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-14 21:16:12,747 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-14 21:16:12,747 - DEBUG - Asking scheduler for work...
2024-11-14 21:16:12,749 - DEBUG - Done
2024-11-14 21:16:12,749 - DEBUG - There are no more tasks to run at this time
2024-11-14 21:16:12,749 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-14 21:16:12,749 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-14 21:16:12,749 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-14 21:16:12,750 - INFO - Worker Worker(salt=2978244729, workers=1, host=MSI, username=alfayyedh, pid=16741) was stopped. Shutting down Keep-Alive thread
2024-11-14 21:16:12,751 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-14 21:54:00,945 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-11-14 21:54:02,224 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-11-14 21:54:02,352 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-11-14 21:54:03,686 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-11-14 21:54:04,428 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-11-14 21:54:05,257 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-11-14 21:54:06,603 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-11-14 21:54:06,606 - ERROR - EXTRACT 'public.product_category_name_tranlation' - FAILED.
2024-11-14 21:54:06,634 - INFO - Extract All Tables From Sources - FAILED
2024-11-14 21:54:06,639 - ERROR - [pid 32655] Worker Worker(salt=639479858, workers=1, host=MSI, username=alfayyedh, pid=32655) failed    Extract()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.product_category_name_tranlation" does not exist
LINE 1: SELECT * FROM public.product_category_name_tranlation;
                      ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 526, in read_sql_query
    return pandas_sql.read_query(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1836, in read_query
    result = self.execute(sql, params)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1659, in execute
    return self.con.exec_driver_sql(sql, *args)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1782, in exec_driver_sql
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.product_category_name_tranlation" does not exist
LINE 1: SELECT * FROM public.product_category_name_tranlation;
                      ^

[SQL: SELECT * FROM public.product_category_name_tranlation;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.product_category_name_tranlation' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-11-14 21:54:06,690 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-14 21:54:06,698 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-11-14 21:54:06,699 - DEBUG - Asking scheduler for work...
2024-11-14 21:54:06,701 - DEBUG - Pending tasks: 2
2024-11-14 21:54:06,701 - INFO - [pid 32655] Worker Worker(salt=639479858, workers=1, host=MSI, username=alfayyedh, pid=32655) running   Transform()
2024-11-14 21:54:06,702 - INFO - Read Transform Query - SUCCESS
2024-11-14 21:54:06,703 - INFO - Connect to DWH - SUCCESS
2024-11-14 21:54:06,703 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-11-14 21:54:06,726 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-11-14 21:54:06,730 - ERROR - Transform Tables - FAILED
2024-11-14 21:54:06,732 - ERROR - [pid 32655] Worker Worker(salt=639479858, workers=1, host=MSI, username=alfayyedh, pid=32655) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: syntax error at or near ")"
LINE 5: )
        ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 93, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near ")"
LINE 5: )
        ^

[SQL: INSERT INTO final.dim_customer (
    customer_id,   
    customer_nk,
    geolocation_id,
)

SELECT
    c.id AS customer_id,
    c.customer_id AS customer_nk,
    dg.geolocation_id
FROM
    stg.customers c
JOIN
    final.dim_geolocation dg ON dg.geolocation_zip_code_prefix = c.customer_zip_code_prefix
    
ON CONFLICT(customer_id) 
DO UPDATE SET
    customer_nk = EXCLUDED.customer_nk,
    geolocation_id = EXCLUDED.geolocation_id,
    updated_at = CASE WHEN 
                        final.dim_customer.customer_nk <> EXCLUDED.customer_nk
                        OR final.dim_customer.geolocation_id <> EXCLUDED.geolocation_id
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.dim_customer.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-11-14 21:54:06,747 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-14 21:54:06,751 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-11-14 21:54:06,752 - DEBUG - Asking scheduler for work...
2024-11-14 21:54:06,753 - DEBUG - Done
2024-11-14 21:54:06,754 - DEBUG - There are no more tasks to run at this time
2024-11-14 21:54:06,754 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-14 21:54:06,754 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-14 21:54:06,754 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-14 21:54:06,754 - INFO - Worker Worker(salt=639479858, workers=1, host=MSI, username=alfayyedh, pid=32655) was stopped. Shutting down Keep-Alive thread
2024-11-14 21:54:06,755 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Load()
* 2 failed:
    - 1 Extract()
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-11-14 21:58:01,651 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-11-14 21:58:02,222 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-11-14 21:58:02,324 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-11-14 21:58:03,304 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-11-14 21:58:03,885 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-11-14 21:58:04,522 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-11-14 21:58:05,527 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-11-14 21:58:05,559 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-11-14 21:58:05,905 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-11-14 21:58:05,921 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-11-14 21:58:05,921 - INFO - Extract All Tables From Sources - SUCCESS
2024-11-14 21:58:05,923 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-11-14 21:58:05,923 - INFO - [pid 34459] Worker Worker(salt=9290973015, workers=1, host=MSI, username=alfayyedh, pid=34459) done      Extract()
2024-11-14 21:58:05,924 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-14 21:58:05,927 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-11-14 21:58:05,927 - DEBUG - Asking scheduler for work...
2024-11-14 21:58:05,928 - DEBUG - Pending tasks: 2
2024-11-14 21:58:05,929 - INFO - [pid 34459] Worker Worker(salt=9290973015, workers=1, host=MSI, username=alfayyedh, pid=34459) running   Load()
2024-11-14 21:58:05,929 - INFO - Read Load Query - SUCCESS
2024-11-14 21:58:07,737 - INFO - Read Extracted Data - SUCCESS
2024-11-14 21:58:07,738 - INFO - Connect to DWH - SUCCESS
2024-11-14 21:58:07,952 - INFO - Truncate public Schema in DWH - SUCCESS
2024-11-14 21:58:07,952 - INFO - ==================================STARTING LOAD DATA=======================================
2024-11-14 21:58:08,928 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-11-14 21:58:08,961 - ERROR - LOAD All Tables To DWH - FAILED
2024-11-14 21:58:08,963 - ERROR - [pid 34459] Worker Worker(salt=9290973015, workers=1, host=MSI, username=alfayyedh, pid=34459) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.ForeignKeyViolation: insert or update on table "customers" violates foreign key constraint "customer_zip_code_prefix_fkey"
DETAIL:  Key (customer_zip_code_prefix)=(14409) is not present in table "geolocation".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 170, in run
    customers.to_sql('customers',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.ForeignKeyViolation) insert or update on table "customers" violates foreign key constraint "customer_zip_code_prefix_fkey"
DETAIL:  Key (customer_zip_code_prefix)=(14409) is not present in table "geolocation".

[SQL: INSERT INTO public.customers (customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state) VALUES (%(customer_id__0)s, %(customer_unique_id__0)s, %(customer_zip_code_prefix__0)s, %(customer_city__0)s, %(customer_state__0) ... 136225 characters truncated ... nique_id__999)s, %(customer_zip_code_prefix__999)s, %(customer_city__999)s, %(customer_state__999)s)]
[parameters: {'customer_id__0': '06b8999e2fba1a1fbc88172c00ba8bc7', 'customer_zip_code_prefix__0': 14409, 'customer_unique_id__0': '861eff4711a542e4b93843c6dd7febb0', 'customer_state__0': 'SP', 'customer_city__0': 'franca', 'customer_id__1': '18955e83d337fd6b2def6b18a428ac77', 'customer_zip_code_prefix__1': 9790, 'customer_unique_id__1': '290c77bc529b7ac935b93aa66c333dc3', 'customer_state__1': 'SP', 'customer_city__1': 'sao bernardo do campo', 'customer_id__2': '4e7b3e00288586ebd08712fdd0374a03', 'customer_zip_code_prefix__2': 1151, 'customer_unique_id__2': '060e732b5b29e8181a18229c7b0b2b5e', 'customer_state__2': 'SP', 'customer_city__2': 'sao paulo', 'customer_id__3': 'b2b6027bc5c5109e529d4dc6358b12c3', 'customer_zip_code_prefix__3': 8775, 'customer_unique_id__3': '259dac757896d24d7702b9acbbff3f3c', 'customer_state__3': 'SP', 'customer_city__3': 'mogi das cruzes', 'customer_id__4': '4f2d8ab171c80ec8364f7c12e35b23ad', 'customer_zip_code_prefix__4': 13056, 'customer_unique_id__4': '345ecd01c38d18a9036ed96c73b8d066', 'customer_state__4': 'SP', 'customer_city__4': 'campinas', 'customer_id__5': '879864dab9bc3047522c92c82e1212b8', 'customer_zip_code_prefix__5': 89254, 'customer_unique_id__5': '4c93744516667ad3b8f1fb645a3116a4', 'customer_state__5': 'SC', 'customer_city__5': 'jaragua do sul', 'customer_id__6': 'fd826e7cf63160e536e0908c76c3f441', 'customer_zip_code_prefix__6': 4534, 'customer_unique_id__6': 'addec96d2e059c80c30fe6871d30d177', 'customer_state__6': 'SP', 'customer_city__6': 'sao paulo', 'customer_id__7': '5e274e7a0c3809e14aba7ad5aae0d407', 'customer_zip_code_prefix__7': 35182, 'customer_unique_id__7': '57b2a98a409812fe9618067b6b8ebe4f', 'customer_state__7': 'MG', 'customer_city__7': 'timoteo', 'customer_id__8': '5adf08e34b2e993982a47070956c5c65', 'customer_zip_code_prefix__8': 81560, 'customer_unique_id__8': '1175e95fb47ddff9de6b2b06188f7e0d', 'customer_state__8': 'PR', 'customer_city__8': 'curitiba', 'customer_id__9': '4b7139f34592b3a31687243a302fa75b', 'customer_zip_code_prefix__9': 30575, 'customer_unique_id__9': '9afe194fb833f79e300e37e580171f22', 'customer_state__9': 'MG', 'customer_city__9': 'belo horizonte' ... 4900 parameters truncated ... 'customer_id__990': '4b6484fa6c4406d63dca39c2f421d2b4', 'customer_zip_code_prefix__990': 32143, 'customer_unique_id__990': 'b087812101562e36a7b1c11c1540c3bd', 'customer_state__990': 'MG', 'customer_city__990': 'contagem', 'customer_id__991': '851406b866022c60fdb8a6e862808933', 'customer_zip_code_prefix__991': 85035, 'customer_unique_id__991': '43f0b8fa25a050c02dc18e166ed574ef', 'customer_state__991': 'PR', 'customer_city__991': 'guarapuava', 'customer_id__992': '269e3d5f088fc1f4b2a81eb0ce8ce7d6', 'customer_zip_code_prefix__992': 4040, 'customer_unique_id__992': 'bb4a329975df85aa69684e124f0b48b1', 'customer_state__992': 'SP', 'customer_city__992': 'sao paulo', 'customer_id__993': '84224aa8a8d7d7a16e6ed976990b9cdf', 'customer_zip_code_prefix__993': 12955, 'customer_unique_id__993': '93805ba8be262fe253c4284751762866', 'customer_state__993': 'SP', 'customer_city__993': 'bom jesus dos perdoes', 'customer_id__994': '7795ad64f8c82d9dc38060df2f3ceab2', 'customer_zip_code_prefix__994': 35160, 'customer_unique_id__994': 'db94b7a9542eb59e1d8d0e7191013354', 'customer_state__994': 'MG', 'customer_city__994': 'ipatinga', 'customer_id__995': '3fcc0ad6fc4d2cb5b8ab39309e1ac335', 'customer_zip_code_prefix__995': 69180, 'customer_unique_id__995': '4bac35931842898ef246fe185e61f7b3', 'customer_state__995': 'AM', 'customer_city__995': 'urucurituba', 'customer_id__996': '58010882ac48cc7801627ad2bcd383e4', 'customer_zip_code_prefix__996': 6624, 'customer_unique_id__996': 'f1bb87ce3b511f772f77765bd3fcef6a', 'customer_state__996': 'SP', 'customer_city__996': 'jandira', 'customer_id__997': '897b42a91c4c99a830882d3470ecbb5f', 'customer_zip_code_prefix__997': 53130, 'customer_unique_id__997': 'fbcafae33b8ac7b85dcbb5ff0cad932e', 'customer_state__997': 'PE', 'customer_city__997': 'olinda', 'customer_id__998': '591cb16ea1a6d8f7eebd0a349593bea2', 'customer_zip_code_prefix__998': 78015, 'customer_unique_id__998': '0019e8c501c85848ac0966d45226fa1d', 'customer_state__998': 'MT', 'customer_city__998': 'cuiaba', 'customer_id__999': 'ba3f81b4928e851009ff39ebeddd520e', 'customer_zip_code_prefix__999': 9920, 'customer_unique_id__999': '1b5908d160c0942cc1b26bc725536d42', 'customer_state__999': 'SP', 'customer_city__999': 'diadema'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-11-14 21:58:09,053 - DEBUG - 1 running tasks, waiting for next task to finish
2024-11-14 21:58:09,062 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-11-14 21:58:09,062 - DEBUG - Asking scheduler for work...
2024-11-14 21:58:09,065 - DEBUG - Done
2024-11-14 21:58:09,065 - DEBUG - There are no more tasks to run at this time
2024-11-14 21:58:09,066 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-11-14 21:58:09,066 - DEBUG - There are 2 pending tasks unique to this worker
2024-11-14 21:58:09,066 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-11-14 21:58:09,066 - INFO - Worker Worker(salt=9290973015, workers=1, host=MSI, username=alfayyedh, pid=34459) was stopped. Shutting down Keep-Alive thread
2024-11-14 21:58:09,068 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-19 20:31:18,695 - INFO - Read Transform Query - SUCCESS
2024-12-19 20:31:18,864 - INFO - Connect to DWH - SUCCESS
2024-12-19 20:31:18,865 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-19 20:31:18,956 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-19 20:31:18,958 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-19 20:31:18,961 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-19 20:31:19,018 - ERROR - Transform Tables - FAILED
2024-12-19 20:31:19,021 - ERROR - [pid 10194] Worker Worker(salt=9484662368, workers=1, host=MSI, username=alfayyedh, pid=10194) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: syntax error at or near ","
LINE 22: ...cts.product_category_name <> EXCLUDED.product_category_name,
                                                                       ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 103, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near ","
LINE 22: ...cts.product_category_name <> EXCLUDED.product_category_name,
                                                                       ^

[SQL: INSERT INTO final.dim_product (
    product_id,
    product_nk,
    product_category_name,
    product_category_name_english
)

SELECT
    p.id AS product_id,
    p.product_id AS product_nk,
    product_category_name
	
FROM
    stg.products p
    
ON CONFLICT(product_id) 
DO UPDATE SET
    product_nk = EXCLUDED.product_nk,
    product_category_name = EXCLUDED.product_category_name,
    updated_at = CASE WHEN 
                        final.dim_products.product_nk <> EXCLUDED.product_nk
                        OR final.dim_products.product_category_name <> EXCLUDED.product_category_name,
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.dim_products.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-19 20:31:19,127 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-19 20:31:19,135 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-19 20:31:19,136 - DEBUG - Asking scheduler for work...
2024-12-19 20:31:19,138 - DEBUG - Done
2024-12-19 20:31:19,138 - DEBUG - There are no more tasks to run at this time
2024-12-19 20:31:19,138 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-19 20:31:19,138 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-19 20:31:19,139 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-19 20:31:19,139 - INFO - Worker Worker(salt=9484662368, workers=1, host=MSI, username=alfayyedh, pid=10194) was stopped. Shutting down Keep-Alive thread
2024-12-19 20:31:19,141 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-19 20:56:40,336 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-19 20:56:42,803 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-19 20:56:43,112 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-19 20:56:46,058 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-19 20:56:47,602 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-19 20:56:49,846 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-19 20:56:53,267 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-19 20:56:53,339 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-19 20:56:54,111 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-19 20:56:54,155 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-19 20:56:54,157 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-19 20:56:54,173 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-19 20:56:54,175 - INFO - [pid 21180] Worker Worker(salt=5209110014, workers=1, host=MSI, username=alfayyedh, pid=21180) done      Extract()
2024-12-19 20:56:54,177 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-19 20:56:54,181 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-19 20:56:54,181 - DEBUG - Asking scheduler for work...
2024-12-19 20:56:54,184 - DEBUG - Pending tasks: 2
2024-12-19 20:56:54,185 - INFO - [pid 21180] Worker Worker(salt=5209110014, workers=1, host=MSI, username=alfayyedh, pid=21180) running   Load()
2024-12-19 20:56:54,190 - INFO - Read Load Query - SUCCESS
2024-12-19 20:56:57,664 - INFO - Read Extracted Data - SUCCESS
2024-12-19 20:56:57,667 - INFO - Connect to DWH - SUCCESS
2024-12-19 20:56:58,194 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-19 20:56:58,195 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-19 20:57:02,230 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-19 20:57:02,379 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-19 20:57:02,404 - ERROR - [pid 21180] Worker Worker(salt=5209110014, workers=1, host=MSI, username=alfayyedh, pid=21180) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.ForeignKeyViolation: insert or update on table "customers" violates foreign key constraint "customer_zip_code_prefix_fkey"
DETAIL:  Key (customer_zip_code_prefix)=(14409) is not present in table "geolocation".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 170, in run
    customers.to_sql('customers',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.ForeignKeyViolation) insert or update on table "customers" violates foreign key constraint "customer_zip_code_prefix_fkey"
DETAIL:  Key (customer_zip_code_prefix)=(14409) is not present in table "geolocation".

[SQL: INSERT INTO public.customers (customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state) VALUES (%(customer_id__0)s, %(customer_unique_id__0)s, %(customer_zip_code_prefix__0)s, %(customer_city__0)s, %(customer_state__0) ... 136225 characters truncated ... nique_id__999)s, %(customer_zip_code_prefix__999)s, %(customer_city__999)s, %(customer_state__999)s)]
[parameters: {'customer_city__0': 'franca', 'customer_unique_id__0': '861eff4711a542e4b93843c6dd7febb0', 'customer_zip_code_prefix__0': 14409, 'customer_state__0': 'SP', 'customer_id__0': '06b8999e2fba1a1fbc88172c00ba8bc7', 'customer_city__1': 'sao bernardo do campo', 'customer_unique_id__1': '290c77bc529b7ac935b93aa66c333dc3', 'customer_zip_code_prefix__1': 9790, 'customer_state__1': 'SP', 'customer_id__1': '18955e83d337fd6b2def6b18a428ac77', 'customer_city__2': 'sao paulo', 'customer_unique_id__2': '060e732b5b29e8181a18229c7b0b2b5e', 'customer_zip_code_prefix__2': 1151, 'customer_state__2': 'SP', 'customer_id__2': '4e7b3e00288586ebd08712fdd0374a03', 'customer_city__3': 'mogi das cruzes', 'customer_unique_id__3': '259dac757896d24d7702b9acbbff3f3c', 'customer_zip_code_prefix__3': 8775, 'customer_state__3': 'SP', 'customer_id__3': 'b2b6027bc5c5109e529d4dc6358b12c3', 'customer_city__4': 'campinas', 'customer_unique_id__4': '345ecd01c38d18a9036ed96c73b8d066', 'customer_zip_code_prefix__4': 13056, 'customer_state__4': 'SP', 'customer_id__4': '4f2d8ab171c80ec8364f7c12e35b23ad', 'customer_city__5': 'jaragua do sul', 'customer_unique_id__5': '4c93744516667ad3b8f1fb645a3116a4', 'customer_zip_code_prefix__5': 89254, 'customer_state__5': 'SC', 'customer_id__5': '879864dab9bc3047522c92c82e1212b8', 'customer_city__6': 'sao paulo', 'customer_unique_id__6': 'addec96d2e059c80c30fe6871d30d177', 'customer_zip_code_prefix__6': 4534, 'customer_state__6': 'SP', 'customer_id__6': 'fd826e7cf63160e536e0908c76c3f441', 'customer_city__7': 'timoteo', 'customer_unique_id__7': '57b2a98a409812fe9618067b6b8ebe4f', 'customer_zip_code_prefix__7': 35182, 'customer_state__7': 'MG', 'customer_id__7': '5e274e7a0c3809e14aba7ad5aae0d407', 'customer_city__8': 'curitiba', 'customer_unique_id__8': '1175e95fb47ddff9de6b2b06188f7e0d', 'customer_zip_code_prefix__8': 81560, 'customer_state__8': 'PR', 'customer_id__8': '5adf08e34b2e993982a47070956c5c65', 'customer_city__9': 'belo horizonte', 'customer_unique_id__9': '9afe194fb833f79e300e37e580171f22', 'customer_zip_code_prefix__9': 30575, 'customer_state__9': 'MG', 'customer_id__9': '4b7139f34592b3a31687243a302fa75b' ... 4900 parameters truncated ... 'customer_city__990': 'contagem', 'customer_unique_id__990': 'b087812101562e36a7b1c11c1540c3bd', 'customer_zip_code_prefix__990': 32143, 'customer_state__990': 'MG', 'customer_id__990': '4b6484fa6c4406d63dca39c2f421d2b4', 'customer_city__991': 'guarapuava', 'customer_unique_id__991': '43f0b8fa25a050c02dc18e166ed574ef', 'customer_zip_code_prefix__991': 85035, 'customer_state__991': 'PR', 'customer_id__991': '851406b866022c60fdb8a6e862808933', 'customer_city__992': 'sao paulo', 'customer_unique_id__992': 'bb4a329975df85aa69684e124f0b48b1', 'customer_zip_code_prefix__992': 4040, 'customer_state__992': 'SP', 'customer_id__992': '269e3d5f088fc1f4b2a81eb0ce8ce7d6', 'customer_city__993': 'bom jesus dos perdoes', 'customer_unique_id__993': '93805ba8be262fe253c4284751762866', 'customer_zip_code_prefix__993': 12955, 'customer_state__993': 'SP', 'customer_id__993': '84224aa8a8d7d7a16e6ed976990b9cdf', 'customer_city__994': 'ipatinga', 'customer_unique_id__994': 'db94b7a9542eb59e1d8d0e7191013354', 'customer_zip_code_prefix__994': 35160, 'customer_state__994': 'MG', 'customer_id__994': '7795ad64f8c82d9dc38060df2f3ceab2', 'customer_city__995': 'urucurituba', 'customer_unique_id__995': '4bac35931842898ef246fe185e61f7b3', 'customer_zip_code_prefix__995': 69180, 'customer_state__995': 'AM', 'customer_id__995': '3fcc0ad6fc4d2cb5b8ab39309e1ac335', 'customer_city__996': 'jandira', 'customer_unique_id__996': 'f1bb87ce3b511f772f77765bd3fcef6a', 'customer_zip_code_prefix__996': 6624, 'customer_state__996': 'SP', 'customer_id__996': '58010882ac48cc7801627ad2bcd383e4', 'customer_city__997': 'olinda', 'customer_unique_id__997': 'fbcafae33b8ac7b85dcbb5ff0cad932e', 'customer_zip_code_prefix__997': 53130, 'customer_state__997': 'PE', 'customer_id__997': '897b42a91c4c99a830882d3470ecbb5f', 'customer_city__998': 'cuiaba', 'customer_unique_id__998': '0019e8c501c85848ac0966d45226fa1d', 'customer_zip_code_prefix__998': 78015, 'customer_state__998': 'MT', 'customer_id__998': '591cb16ea1a6d8f7eebd0a349593bea2', 'customer_city__999': 'diadema', 'customer_unique_id__999': '1b5908d160c0942cc1b26bc725536d42', 'customer_zip_code_prefix__999': 9920, 'customer_state__999': 'SP', 'customer_id__999': 'ba3f81b4928e851009ff39ebeddd520e'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-19 20:57:02,850 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-19 20:57:02,866 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-19 20:57:02,867 - DEBUG - Asking scheduler for work...
2024-12-19 20:57:02,877 - DEBUG - Done
2024-12-19 20:57:02,878 - DEBUG - There are no more tasks to run at this time
2024-12-19 20:57:02,878 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-19 20:57:02,878 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-19 20:57:02,878 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-19 20:57:02,878 - INFO - Worker Worker(salt=5209110014, workers=1, host=MSI, username=alfayyedh, pid=21180) was stopped. Shutting down Keep-Alive thread
2024-12-19 20:57:02,880 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 20:04:29,676 - INFO - Read Transform Query - SUCCESS
2024-12-21 20:04:29,787 - INFO - Connect to DWH - SUCCESS
2024-12-21 20:04:29,788 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 20:04:29,923 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 20:04:29,925 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 20:04:29,926 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 20:04:29,979 - ERROR - Transform Tables - FAILED
2024-12-21 20:04:29,981 - ERROR - [pid 9515] Worker Worker(salt=501985052, workers=1, host=MSI, username=alfayyedh, pid=9515) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: INSERT has more target columns than expressions
LINE 5:     product_category_name_english
            ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 103, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) INSERT has more target columns than expressions
LINE 5:     product_category_name_english
            ^

[SQL: INSERT INTO final.dim_product (
    product_id,
    product_nk,
    product_category_name,
    product_category_name_english
)

SELECT
    p.id AS product_id,
    p.product_id AS product_nk,
    product_category_name
	
FROM
    stg.products p
    
ON CONFLICT(product_id) 
DO UPDATE SET
    product_nk = EXCLUDED.product_nk,
    product_category_name = EXCLUDED.product_category_name,
    updated_at = CASE WHEN 
                        final.dim_products.product_nk <> EXCLUDED.product_nk
                        OR final.dim_products.product_category_name <> EXCLUDED.product_category_name
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.dim_products.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 20:04:30,017 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 20:04:30,023 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 20:04:30,024 - DEBUG - Asking scheduler for work...
2024-12-21 20:04:30,031 - DEBUG - Done
2024-12-21 20:04:30,032 - DEBUG - There are no more tasks to run at this time
2024-12-21 20:04:30,032 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 20:04:30,032 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-21 20:04:30,032 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 20:04:30,032 - INFO - Worker Worker(salt=501985052, workers=1, host=MSI, username=alfayyedh, pid=9515) was stopped. Shutting down Keep-Alive thread
2024-12-21 20:04:30,034 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 20:09:11,102 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 20:09:12,008 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 20:09:12,128 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 20:09:13,345 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 20:09:13,990 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 20:09:14,712 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 20:09:15,777 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 20:09:15,809 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 20:09:16,073 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 20:09:16,089 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 20:09:16,089 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 20:09:16,092 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 20:09:16,092 - INFO - [pid 11573] Worker Worker(salt=3097037973, workers=1, host=MSI, username=alfayyedh, pid=11573) done      Extract()
2024-12-21 20:09:16,093 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 20:09:16,096 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 20:09:16,097 - DEBUG - Asking scheduler for work...
2024-12-21 20:09:16,100 - DEBUG - Pending tasks: 2
2024-12-21 20:09:16,100 - INFO - [pid 11573] Worker Worker(salt=3097037973, workers=1, host=MSI, username=alfayyedh, pid=11573) running   Load()
2024-12-21 20:09:16,106 - INFO - Read Load Query - SUCCESS
2024-12-21 20:09:17,565 - INFO - Read Extracted Data - SUCCESS
2024-12-21 20:09:17,567 - INFO - Connect to DWH - SUCCESS
2024-12-21 20:09:17,821 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 20:09:17,821 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 20:09:18,689 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 20:09:18,721 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 20:09:18,724 - ERROR - [pid 11573] Worker Worker(salt=3097037973, workers=1, host=MSI, username=alfayyedh, pid=11573) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.ForeignKeyViolation: insert or update on table "customers" violates foreign key constraint "customer_zip_code_prefix_fkey"
DETAIL:  Key (customer_zip_code_prefix)=(14409) is not present in table "geolocation".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 170, in run
    customers.to_sql('customers',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.ForeignKeyViolation) insert or update on table "customers" violates foreign key constraint "customer_zip_code_prefix_fkey"
DETAIL:  Key (customer_zip_code_prefix)=(14409) is not present in table "geolocation".

[SQL: INSERT INTO public.customers (customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state) VALUES (%(customer_id__0)s, %(customer_unique_id__0)s, %(customer_zip_code_prefix__0)s, %(customer_city__0)s, %(customer_state__0) ... 136225 characters truncated ... nique_id__999)s, %(customer_zip_code_prefix__999)s, %(customer_city__999)s, %(customer_state__999)s)]
[parameters: {'customer_zip_code_prefix__0': 14409, 'customer_city__0': 'franca', 'customer_state__0': 'SP', 'customer_unique_id__0': '861eff4711a542e4b93843c6dd7febb0', 'customer_id__0': '06b8999e2fba1a1fbc88172c00ba8bc7', 'customer_zip_code_prefix__1': 9790, 'customer_city__1': 'sao bernardo do campo', 'customer_state__1': 'SP', 'customer_unique_id__1': '290c77bc529b7ac935b93aa66c333dc3', 'customer_id__1': '18955e83d337fd6b2def6b18a428ac77', 'customer_zip_code_prefix__2': 1151, 'customer_city__2': 'sao paulo', 'customer_state__2': 'SP', 'customer_unique_id__2': '060e732b5b29e8181a18229c7b0b2b5e', 'customer_id__2': '4e7b3e00288586ebd08712fdd0374a03', 'customer_zip_code_prefix__3': 8775, 'customer_city__3': 'mogi das cruzes', 'customer_state__3': 'SP', 'customer_unique_id__3': '259dac757896d24d7702b9acbbff3f3c', 'customer_id__3': 'b2b6027bc5c5109e529d4dc6358b12c3', 'customer_zip_code_prefix__4': 13056, 'customer_city__4': 'campinas', 'customer_state__4': 'SP', 'customer_unique_id__4': '345ecd01c38d18a9036ed96c73b8d066', 'customer_id__4': '4f2d8ab171c80ec8364f7c12e35b23ad', 'customer_zip_code_prefix__5': 89254, 'customer_city__5': 'jaragua do sul', 'customer_state__5': 'SC', 'customer_unique_id__5': '4c93744516667ad3b8f1fb645a3116a4', 'customer_id__5': '879864dab9bc3047522c92c82e1212b8', 'customer_zip_code_prefix__6': 4534, 'customer_city__6': 'sao paulo', 'customer_state__6': 'SP', 'customer_unique_id__6': 'addec96d2e059c80c30fe6871d30d177', 'customer_id__6': 'fd826e7cf63160e536e0908c76c3f441', 'customer_zip_code_prefix__7': 35182, 'customer_city__7': 'timoteo', 'customer_state__7': 'MG', 'customer_unique_id__7': '57b2a98a409812fe9618067b6b8ebe4f', 'customer_id__7': '5e274e7a0c3809e14aba7ad5aae0d407', 'customer_zip_code_prefix__8': 81560, 'customer_city__8': 'curitiba', 'customer_state__8': 'PR', 'customer_unique_id__8': '1175e95fb47ddff9de6b2b06188f7e0d', 'customer_id__8': '5adf08e34b2e993982a47070956c5c65', 'customer_zip_code_prefix__9': 30575, 'customer_city__9': 'belo horizonte', 'customer_state__9': 'MG', 'customer_unique_id__9': '9afe194fb833f79e300e37e580171f22', 'customer_id__9': '4b7139f34592b3a31687243a302fa75b' ... 4900 parameters truncated ... 'customer_zip_code_prefix__990': 32143, 'customer_city__990': 'contagem', 'customer_state__990': 'MG', 'customer_unique_id__990': 'b087812101562e36a7b1c11c1540c3bd', 'customer_id__990': '4b6484fa6c4406d63dca39c2f421d2b4', 'customer_zip_code_prefix__991': 85035, 'customer_city__991': 'guarapuava', 'customer_state__991': 'PR', 'customer_unique_id__991': '43f0b8fa25a050c02dc18e166ed574ef', 'customer_id__991': '851406b866022c60fdb8a6e862808933', 'customer_zip_code_prefix__992': 4040, 'customer_city__992': 'sao paulo', 'customer_state__992': 'SP', 'customer_unique_id__992': 'bb4a329975df85aa69684e124f0b48b1', 'customer_id__992': '269e3d5f088fc1f4b2a81eb0ce8ce7d6', 'customer_zip_code_prefix__993': 12955, 'customer_city__993': 'bom jesus dos perdoes', 'customer_state__993': 'SP', 'customer_unique_id__993': '93805ba8be262fe253c4284751762866', 'customer_id__993': '84224aa8a8d7d7a16e6ed976990b9cdf', 'customer_zip_code_prefix__994': 35160, 'customer_city__994': 'ipatinga', 'customer_state__994': 'MG', 'customer_unique_id__994': 'db94b7a9542eb59e1d8d0e7191013354', 'customer_id__994': '7795ad64f8c82d9dc38060df2f3ceab2', 'customer_zip_code_prefix__995': 69180, 'customer_city__995': 'urucurituba', 'customer_state__995': 'AM', 'customer_unique_id__995': '4bac35931842898ef246fe185e61f7b3', 'customer_id__995': '3fcc0ad6fc4d2cb5b8ab39309e1ac335', 'customer_zip_code_prefix__996': 6624, 'customer_city__996': 'jandira', 'customer_state__996': 'SP', 'customer_unique_id__996': 'f1bb87ce3b511f772f77765bd3fcef6a', 'customer_id__996': '58010882ac48cc7801627ad2bcd383e4', 'customer_zip_code_prefix__997': 53130, 'customer_city__997': 'olinda', 'customer_state__997': 'PE', 'customer_unique_id__997': 'fbcafae33b8ac7b85dcbb5ff0cad932e', 'customer_id__997': '897b42a91c4c99a830882d3470ecbb5f', 'customer_zip_code_prefix__998': 78015, 'customer_city__998': 'cuiaba', 'customer_state__998': 'MT', 'customer_unique_id__998': '0019e8c501c85848ac0966d45226fa1d', 'customer_id__998': '591cb16ea1a6d8f7eebd0a349593bea2', 'customer_zip_code_prefix__999': 9920, 'customer_city__999': 'diadema', 'customer_state__999': 'SP', 'customer_unique_id__999': '1b5908d160c0942cc1b26bc725536d42', 'customer_id__999': 'ba3f81b4928e851009ff39ebeddd520e'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 20:09:18,816 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 20:09:18,822 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 20:09:18,823 - DEBUG - Asking scheduler for work...
2024-12-21 20:09:18,825 - DEBUG - Done
2024-12-21 20:09:18,825 - DEBUG - There are no more tasks to run at this time
2024-12-21 20:09:18,825 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 20:09:18,825 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 20:09:18,825 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 20:09:18,825 - INFO - Worker Worker(salt=3097037973, workers=1, host=MSI, username=alfayyedh, pid=11573) was stopped. Shutting down Keep-Alive thread
2024-12-21 20:09:18,826 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:00:02,928 - INFO - Read Transform Query - SUCCESS
2024-12-21 22:00:03,016 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:00:03,016 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 22:00:03,107 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 22:00:03,115 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 22:00:03,119 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 22:00:03,175 - ERROR - Transform Tables - FAILED
2024-12-21 22:00:03,177 - ERROR - [pid 57263] Worker Worker(salt=4440285055, workers=1, host=MSI, username=alfayyedh, pid=57263) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: missing FROM-clause entry for table "dim_products"
LINE 25:                         final.dim_products.product_nk <> EXC...
                                 ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 103, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) missing FROM-clause entry for table "dim_products"
LINE 25:                         final.dim_products.product_nk <> EXC...
                                 ^

[SQL: INSERT INTO final.dim_product (
    product_id,
    product_nk,
    product_category_name,
    product_category_name_english
)

SELECT
    p.id AS product_id,
    p.product_id AS product_nk,
    p.product_category_name,
    pc.product_category_name_english
	
FROM
    stg.products p
JOIN
    stg.product_category_name_translation pc ON p.product_category_name = pc.product_category_name
    
ON CONFLICT(product_id) 
DO UPDATE SET
    product_nk = EXCLUDED.product_nk,
    product_category_name = EXCLUDED.product_category_name,
    product_category_name_english = EXCLUDED.product_category_name_english,
    updated_at = CASE WHEN 
                        final.dim_products.product_nk <> EXCLUDED.product_nk
                        OR final.dim_products.product_category_name <> EXCLUDED.product_category_name
                        OR final.dim_products.product_category_name_english <> EXCLUDED.product_category_name_english
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.dim_products.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 22:00:03,206 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:00:03,214 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 22:00:03,214 - DEBUG - Asking scheduler for work...
2024-12-21 22:00:03,216 - DEBUG - Done
2024-12-21 22:00:03,216 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:00:03,216 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 22:00:03,217 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-21 22:00:03,217 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 22:00:03,217 - INFO - Worker Worker(salt=4440285055, workers=1, host=MSI, username=alfayyedh, pid=57263) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:00:03,218 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:04:57,590 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 22:04:57,634 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 22:04:57,967 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 22:04:58,094 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 22:04:58,117 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 22:04:58,930 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 22:04:59,777 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 22:05:00,482 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 22:05:01,749 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 22:05:02,814 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 22:05:02,814 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 22:05:02,816 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 22:05:02,841 - INFO - [pid 59389] Worker Worker(salt=2065959507, workers=1, host=MSI, username=alfayyedh, pid=59389) done      Extract()
2024-12-21 22:05:02,844 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:05:02,846 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 22:05:02,846 - DEBUG - Asking scheduler for work...
2024-12-21 22:05:02,849 - DEBUG - Pending tasks: 2
2024-12-21 22:05:02,849 - INFO - [pid 59389] Worker Worker(salt=2065959507, workers=1, host=MSI, username=alfayyedh, pid=59389) running   Load()
2024-12-21 22:05:02,854 - INFO - Read Load Query - SUCCESS
2024-12-21 22:05:03,820 - INFO - Read Extracted Data - SUCCESS
2024-12-21 22:05:03,821 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:05:04,133 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 22:05:04,133 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 22:05:04,151 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 22:05:06,155 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 22:05:06,604 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 22:05:06,701 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 22:05:08,812 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 22:05:08,851 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 22:05:08,857 - ERROR - [pid 59389] Worker Worker(salt=2065959507, workers=1, host=MSI, username=alfayyedh, pid=59389) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_comment_title__0': None, 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_score__0': 1, 'review_creation_date__0': '2018-04-11 09:55:40', 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_comment_message__1': None, 'review_comment_title__1': None, 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_score__1': 4, 'review_creation_date__1': '2018-03-31 14:25:05', 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_comment_message__2': None, 'review_comment_title__2': None, 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_score__2': 5, 'review_creation_date__2': '2017-11-03 11:21:09', 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_comment_message__3': None, 'review_comment_title__3': None, 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_score__3': 4, 'review_creation_date__3': '2017-05-04 01:11:07', 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_comment_message__4': 'Recomendo', 'review_comment_title__4': None, 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_score__4': 5, 'review_creation_date__4': '2017-12-13 01:16:52', 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_comment_message__5': 'Produto não foi entregue!', 'review_comment_title__5': None, 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_score__5': 1, 'review_creation_date__5': '2018-08-23 13:20:20', 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_comment_title__6': None, 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_score__6': 1, 'review_creation_date__6': '2018-02-09 15:11:21', 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_comment_title__7': None, 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_score__7': 1, 'review_creation_date__7': '2018-03-29 23:50:09', 'review_id__8': 'c27c301777803a6c53b4306aa298bfbd', 'review_comment_message__8': 'Adoro comprar nas lojas lannister' ... 5900 parameters truncated ... 'review_score__991': 5, 'review_creation_date__991': '2018-05-23 11:32:06', 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_comment_title__992': None, 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_score__992': 5, 'review_creation_date__992': '2017-03-22 13:00:09', 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_comment_message__993': None, 'review_comment_title__993': None, 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_score__993': 5, 'review_creation_date__993': '2018-02-22 10:46:59', 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_comment_message__994': None, 'review_comment_title__994': None, 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_score__994': 5, 'review_creation_date__994': '2018-01-26 01:07:05', 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_comment_message__995': 'recomendo', 'review_comment_title__995': None, 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_score__995': 4, 'review_creation_date__995': '2017-11-06 21:31:59', 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_comment_message__996': 'Loja confiável ', 'review_comment_title__996': 'Ótimo atendimento ', 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_score__996': 5, 'review_creation_date__996': '2018-07-23 20:39:23', 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_comment_message__997': None, 'review_comment_title__997': None, 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_score__997': 2, 'review_creation_date__997': '2018-04-09 11:36:53', 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_comment_message__998': None, 'review_comment_title__998': None, 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_score__998': 5, 'review_creation_date__998': '2017-06-23 03:33:40', 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.', 'review_comment_title__999': None, 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_score__999': 5, 'review_creation_date__999': '2018-03-27 01:42:14'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 22:05:09,054 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:05:09,066 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 22:05:09,066 - DEBUG - Asking scheduler for work...
2024-12-21 22:05:09,069 - DEBUG - Done
2024-12-21 22:05:09,070 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:05:09,070 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 22:05:09,070 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 22:05:09,070 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 22:05:09,070 - INFO - Worker Worker(salt=2065959507, workers=1, host=MSI, username=alfayyedh, pid=59389) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:05:09,074 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:16:41,784 - INFO - Read Transform Query - SUCCESS
2024-12-21 22:16:41,836 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:16:41,836 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 22:16:41,873 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 22:16:41,875 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 22:16:41,878 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 22:16:41,885 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 22:16:41,945 - ERROR - Transform Tables - FAILED
2024-12-21 22:16:41,947 - ERROR - [pid 64303] Worker Worker(salt=6995818681, workers=1, host=MSI, username=alfayyedh, pid=64303) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column so.shipping_limit_date does not exist
LINE 64:     final.dim_date dd6 ON dd6.date_actual = DATE(so.shipping...
                                                          ^
HINT:  Perhaps you meant to reference the column "oi.shipping_limit_date".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column so.shipping_limit_date does not exist
LINE 64:     final.dim_date dd6 ON dd6.date_actual = DATE(so.shipping...
                                                          ^
HINT:  Perhaps you meant to reference the column "oi.shipping_limit_date".

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id,
	dc.customer_id,
    so.order_status,
    dd1.dim_date AS order_purchase_date,
    dt1.dim_time AS order_purchase_time,
    dd2.dim_date AS order_approved_at_date,
    dt2.dim_time AS order_approved_at_time,
    dd3.dim_date AS order_delivered_carrier_date,
    dt3.dim_time AS order_delivered_carrier_time,
    dd4.dim_date AS order_delivered_customer_date,
    dt4.dim_time AS order_delivered_customer_time,
    dd5.dim_date AS order_estimated_delivery_date,
    dt5.dim_time AS order_estimated_delivery_time,
    dd6.dim_date AS shipping_limit_date,
    dt6.dim_date AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    final.dim_date dd1 ON dd1.date_actual = DATE(so.order_purchase_timestamp)
JOIN
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN
    final.dim_date dd2 ON dd2.date_actual = DATE(so.order_approved_at)
JOIN
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN
    final.dim_date dd3 ON dd3.date_actual = DATE(so.order_delivered_carrier_date)
JOIN
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN
    final.dim_date dd4 ON dd4.date_actual = DATE(so.order_delivered_customer_date)
JOIN
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN
    final.dim_date dd5 ON dd5.date_actual = DATE(so.order_estimated_delivery_date)
JOIN
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN
    final.dim_date dd6 ON dd6.date_actual = DATE(so.shipping_limit_date)
JOIN
    final.dim_time dt6 ON dt6.time_actual::time = (so.shipping_limit_date)::time

	

ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 22:16:41,968 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:16:41,980 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 22:16:41,981 - DEBUG - Asking scheduler for work...
2024-12-21 22:16:41,983 - DEBUG - Done
2024-12-21 22:16:41,983 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:16:41,983 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 22:16:41,984 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-21 22:16:41,984 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 22:16:41,984 - INFO - Worker Worker(salt=6995818681, workers=1, host=MSI, username=alfayyedh, pid=64303) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:16:41,985 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:18:49,921 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 22:18:49,961 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 22:18:50,280 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 22:18:50,411 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 22:18:50,428 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 22:18:51,089 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 22:18:51,712 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 22:18:52,258 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 22:18:53,275 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 22:18:54,196 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 22:18:54,196 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 22:18:54,199 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 22:18:54,225 - INFO - [pid 65267] Worker Worker(salt=5728467909, workers=1, host=MSI, username=alfayyedh, pid=65267) done      Extract()
2024-12-21 22:18:54,226 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:18:54,229 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 22:18:54,229 - DEBUG - Asking scheduler for work...
2024-12-21 22:18:54,231 - DEBUG - Pending tasks: 2
2024-12-21 22:18:54,231 - INFO - [pid 65267] Worker Worker(salt=5728467909, workers=1, host=MSI, username=alfayyedh, pid=65267) running   Load()
2024-12-21 22:18:54,232 - INFO - Read Load Query - SUCCESS
2024-12-21 22:18:55,265 - INFO - Read Extracted Data - SUCCESS
2024-12-21 22:18:55,266 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:18:55,532 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 22:18:55,532 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 22:18:55,547 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 22:18:57,276 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 22:18:57,714 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 22:18:57,802 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 22:18:58,786 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 22:18:58,823 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 22:18:58,828 - ERROR - [pid 65267] Worker Worker(salt=5728467909, workers=1, host=MSI, username=alfayyedh, pid=65267) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_creation_date__0': '2018-04-11 09:55:40', 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_comment_title__0': None, 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_score__0': 1, 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_creation_date__1': '2018-03-31 14:25:05', 'review_comment_message__1': None, 'review_comment_title__1': None, 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_score__1': 4, 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_creation_date__2': '2017-11-03 11:21:09', 'review_comment_message__2': None, 'review_comment_title__2': None, 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_score__2': 5, 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_creation_date__3': '2017-05-04 01:11:07', 'review_comment_message__3': None, 'review_comment_title__3': None, 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_score__3': 4, 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_creation_date__4': '2017-12-13 01:16:52', 'review_comment_message__4': 'Recomendo', 'review_comment_title__4': None, 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_score__4': 5, 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_creation_date__5': '2018-08-23 13:20:20', 'review_comment_message__5': 'Produto não foi entregue!', 'review_comment_title__5': None, 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_score__5': 1, 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_creation_date__6': '2018-02-09 15:11:21', 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_comment_title__6': None, 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_score__6': 1, 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_creation_date__7': '2018-03-29 23:50:09', 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_comment_title__7': None, 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_score__7': 1, 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_creation_date__8': '2018-08-31 23:20:43', 'review_comment_message__8': 'Adoro comprar nas lojas lannister' ... 5900 parameters truncated ... 'review_score__991': 5, 'review_id__991': '5764bbed44b798460013b2e4f9512d10', 'review_creation_date__992': '2017-03-22 13:00:09', 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_comment_title__992': None, 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_score__992': 5, 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_creation_date__993': '2018-02-22 10:46:59', 'review_comment_message__993': None, 'review_comment_title__993': None, 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_score__993': 5, 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_creation_date__994': '2018-01-26 01:07:05', 'review_comment_message__994': None, 'review_comment_title__994': None, 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_score__994': 5, 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_creation_date__995': '2017-11-06 21:31:59', 'review_comment_message__995': 'recomendo', 'review_comment_title__995': None, 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_score__995': 4, 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_creation_date__996': '2018-07-23 20:39:23', 'review_comment_message__996': 'Loja confiável ', 'review_comment_title__996': 'Ótimo atendimento ', 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_score__996': 5, 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_creation_date__997': '2018-04-09 11:36:53', 'review_comment_message__997': None, 'review_comment_title__997': None, 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_score__997': 2, 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_creation_date__998': '2017-06-23 03:33:40', 'review_comment_message__998': None, 'review_comment_title__998': None, 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_score__998': 5, 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_creation_date__999': '2018-03-27 01:42:14', 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.', 'review_comment_title__999': None, 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_score__999': 5, 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 22:18:58,926 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:18:58,934 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 22:18:58,935 - DEBUG - Asking scheduler for work...
2024-12-21 22:18:58,937 - DEBUG - Done
2024-12-21 22:18:58,937 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:18:58,937 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 22:18:58,937 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 22:18:58,937 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 22:18:58,938 - INFO - Worker Worker(salt=5728467909, workers=1, host=MSI, username=alfayyedh, pid=65267) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:18:58,939 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:29:37,986 - INFO - Read Transform Query - SUCCESS
2024-12-21 22:29:38,042 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:29:38,042 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 22:29:38,088 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 22:29:38,091 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 22:29:38,093 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 22:29:38,099 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 22:29:38,138 - ERROR - Transform Tables - FAILED
2024-12-21 22:29:38,140 - ERROR - [pid 69825] Worker Worker(salt=8565285200, workers=1, host=MSI, username=alfayyedh, pid=69825) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column dd1.dim_date does not exist
LINE 25:     dd1.dim_date AS order_purchase_date,
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column dd1.dim_date does not exist
LINE 25:     dd1.dim_date AS order_purchase_date,
             ^

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id,
	dc.customer_id,
    so.order_status,
    dd1.dim_date AS order_purchase_date,
    dt1.dim_time AS order_purchase_time,
    dd2.dim_date AS order_approved_at_date,
    dt2.dim_time AS order_approved_at_time,
    dd3.dim_date AS order_delivered_carrier_date,
    dt3.dim_time AS order_delivered_carrier_time,
    dd4.dim_date AS order_delivered_customer_date,
    dt4.dim_time AS order_delivered_customer_time,
    dd5.dim_date AS order_estimated_delivery_date,
    dt5.dim_time AS order_estimated_delivery_time,
    dd6.dim_date AS shipping_limit_date,
    dt6.dim_date AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    final.dim_date dd1 ON dd1.date_actual = DATE(so.order_purchase_timestamp)
JOIN
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN
    final.dim_date dd2 ON dd2.date_actual = DATE(so.order_approved_at)
JOIN
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN
    final.dim_date dd3 ON dd3.date_actual = DATE(so.order_delivered_carrier_date)
JOIN
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN
    final.dim_date dd4 ON dd4.date_actual = DATE(so.order_delivered_customer_date)
JOIN
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN
    final.dim_date dd5 ON dd5.date_actual = DATE(so.order_estimated_delivery_date)
JOIN
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN
    final.dim_date dd6 ON dd6.date_actual = DATE(oi.shipping_limit_date)
JOIN
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time

	

ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 22:29:38,159 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:29:38,169 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 22:29:38,170 - DEBUG - Asking scheduler for work...
2024-12-21 22:29:38,171 - DEBUG - Done
2024-12-21 22:29:38,172 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:29:38,172 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 22:29:38,172 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-21 22:29:38,172 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 22:29:38,172 - INFO - Worker Worker(salt=8565285200, workers=1, host=MSI, username=alfayyedh, pid=69825) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:29:38,173 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:43:27,834 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 22:43:27,898 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 22:43:28,196 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 22:43:28,318 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 22:43:28,334 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 22:43:29,039 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 22:43:29,653 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 22:43:30,218 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 22:43:31,319 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 22:43:32,258 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 22:43:32,258 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 22:43:32,262 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 22:43:32,288 - INFO - [pid 75594] Worker Worker(salt=6219416271, workers=1, host=MSI, username=alfayyedh, pid=75594) done      Extract()
2024-12-21 22:43:32,290 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:43:32,293 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 22:43:32,293 - DEBUG - Asking scheduler for work...
2024-12-21 22:43:32,295 - DEBUG - Pending tasks: 2
2024-12-21 22:43:32,295 - INFO - [pid 75594] Worker Worker(salt=6219416271, workers=1, host=MSI, username=alfayyedh, pid=75594) running   Load()
2024-12-21 22:43:32,296 - INFO - Read Load Query - SUCCESS
2024-12-21 22:43:33,360 - INFO - Read Extracted Data - SUCCESS
2024-12-21 22:43:33,361 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:43:33,630 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 22:43:33,631 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 22:43:33,652 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 22:43:35,484 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 22:43:35,938 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 22:43:36,025 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 22:43:36,679 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 22:43:36,725 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 22:43:36,730 - ERROR - [pid 75594] Worker Worker(salt=6219416271, workers=1, host=MSI, username=alfayyedh, pid=75594) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_comment_title__0': None, 'review_creation_date__0': '2018-04-11 09:55:40', 'review_score__0': 1, 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_comment_title__1': None, 'review_creation_date__1': '2018-03-31 14:25:05', 'review_score__1': 4, 'review_comment_message__1': None, 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_comment_title__2': None, 'review_creation_date__2': '2017-11-03 11:21:09', 'review_score__2': 5, 'review_comment_message__2': None, 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_comment_title__3': None, 'review_creation_date__3': '2017-05-04 01:11:07', 'review_score__3': 4, 'review_comment_message__3': None, 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_comment_title__4': None, 'review_creation_date__4': '2017-12-13 01:16:52', 'review_score__4': 5, 'review_comment_message__4': 'Recomendo', 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_comment_title__5': None, 'review_creation_date__5': '2018-08-23 13:20:20', 'review_score__5': 1, 'review_comment_message__5': 'Produto não foi entregue!', 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_comment_title__6': None, 'review_creation_date__6': '2018-02-09 15:11:21', 'review_score__6': 1, 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_comment_title__7': None, 'review_creation_date__7': '2018-03-29 23:50:09', 'review_score__7': 1, 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'order_id__8': '4cae9d93ca1f1266e9ddfc79f7ff85db', 'review_id__8': 'c27c301777803a6c53b4306aa298bfbd' ... 5900 parameters truncated ... 'review_score__991': 5, 'review_comment_message__991': None, 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_comment_title__992': None, 'review_creation_date__992': '2017-03-22 13:00:09', 'review_score__992': 5, 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_comment_title__993': None, 'review_creation_date__993': '2018-02-22 10:46:59', 'review_score__993': 5, 'review_comment_message__993': None, 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_comment_title__994': None, 'review_creation_date__994': '2018-01-26 01:07:05', 'review_score__994': 5, 'review_comment_message__994': None, 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_comment_title__995': None, 'review_creation_date__995': '2017-11-06 21:31:59', 'review_score__995': 4, 'review_comment_message__995': 'recomendo', 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_comment_title__996': 'Ótimo atendimento ', 'review_creation_date__996': '2018-07-23 20:39:23', 'review_score__996': 5, 'review_comment_message__996': 'Loja confiável ', 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_comment_title__997': None, 'review_creation_date__997': '2018-04-09 11:36:53', 'review_score__997': 2, 'review_comment_message__997': None, 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_comment_title__998': None, 'review_creation_date__998': '2017-06-23 03:33:40', 'review_score__998': 5, 'review_comment_message__998': None, 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'review_comment_title__999': None, 'review_creation_date__999': '2018-03-27 01:42:14', 'review_score__999': 5, 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 22:43:36,810 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:43:36,820 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 22:43:36,821 - DEBUG - Asking scheduler for work...
2024-12-21 22:43:36,823 - DEBUG - Done
2024-12-21 22:43:36,823 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:43:36,823 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 22:43:36,823 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 22:43:36,823 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 22:43:36,823 - INFO - Worker Worker(salt=6219416271, workers=1, host=MSI, username=alfayyedh, pid=75594) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:43:36,825 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:43:53,622 - INFO - Read Transform Query - SUCCESS
2024-12-21 22:43:53,675 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:43:53,675 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 22:43:53,712 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 22:43:53,714 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 22:43:53,716 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 22:43:53,722 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 22:43:53,757 - ERROR - Transform Tables - FAILED
2024-12-21 22:43:53,758 - ERROR - [pid 75889] Worker Worker(salt=4855321625, workers=1, host=MSI, username=alfayyedh, pid=75889) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: INSERT has more target columns than expressions
LINE 18:     shipping_limit_time
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) INSERT has more target columns than expressions
LINE 18:     shipping_limit_time
             ^

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id,
	dc.customer_id,
    so.order_status,
    dd1.date_actual  AS order_purchase_date,
    dt1.time_actual AS order_purchase_time,
    dd2.date_actual AS order_approved_at_date,
    dt2.time_actual AS order_approved_at_time,
    dd3.date_actual AS order_delivered_carrier_date,
    dt3.time_actual AS order_delivered_carrier_time,
    dd4.date_actual AS order_delivered_customer_date,
    dt4.time_actual AS order_delivered_customer_time,
    dd5.date_actual AS order_estimated_delivery_date,
    dt5.time_actual AS order_estimated_delivery_time,
    dd6.date_actual AS shipping_limit_date,
    dt6.time_actual AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    final.dim_date dd1 ON dd1.date_actual = DATE(so.order_purchase_timestamp)
JOIN
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN
    final.dim_date dd2 ON dd2.date_actual = DATE(so.order_approved_at)
JOIN
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN
    final.dim_date dd3 ON dd3.date_actual = DATE(so.order_delivered_carrier_date)
JOIN
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN
    final.dim_date dd4 ON dd4.date_actual = DATE(so.order_delivered_customer_date)
JOIN
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN
    final.dim_date dd5 ON dd5.date_actual = DATE(so.order_estimated_delivery_date)
JOIN
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN
    final.dim_date dd6 ON dd6.date_actual = DATE(oi.shipping_limit_date)
JOIN
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time

	

ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 22:43:53,777 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:43:53,783 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 22:43:53,783 - DEBUG - Asking scheduler for work...
2024-12-21 22:43:53,786 - DEBUG - Done
2024-12-21 22:43:53,786 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:43:53,786 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 22:43:53,786 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 22:43:53,786 - INFO - Worker Worker(salt=4855321625, workers=1, host=MSI, username=alfayyedh, pid=75889) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:43:53,787 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:46:16,073 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 22:46:16,107 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 22:46:16,360 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 22:46:16,472 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 22:46:16,486 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 22:46:17,143 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 22:46:17,704 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 22:46:18,156 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 22:46:19,034 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 22:46:19,902 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 22:46:19,902 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 22:46:19,904 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 22:46:19,929 - INFO - [pid 76933] Worker Worker(salt=1566298011, workers=1, host=MSI, username=alfayyedh, pid=76933) done      Extract()
2024-12-21 22:46:19,931 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:46:19,933 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 22:46:19,934 - DEBUG - Asking scheduler for work...
2024-12-21 22:46:19,935 - DEBUG - Pending tasks: 2
2024-12-21 22:46:19,935 - INFO - [pid 76933] Worker Worker(salt=1566298011, workers=1, host=MSI, username=alfayyedh, pid=76933) running   Load()
2024-12-21 22:46:19,936 - INFO - Read Load Query - SUCCESS
2024-12-21 22:46:20,957 - INFO - Read Extracted Data - SUCCESS
2024-12-21 22:46:20,958 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:46:21,205 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 22:46:21,206 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 22:46:21,225 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 22:46:22,959 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 22:46:23,388 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 22:46:23,508 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 22:46:24,476 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 22:46:24,508 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 22:46:24,514 - ERROR - [pid 76933] Worker Worker(salt=1566298011, workers=1, host=MSI, username=alfayyedh, pid=76933) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_comment_title__0': None, 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_creation_date__0': '2018-04-11 09:55:40', 'review_score__0': 1, 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_comment_title__1': None, 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_creation_date__1': '2018-03-31 14:25:05', 'review_score__1': 4, 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_comment_message__1': None, 'review_comment_title__2': None, 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_creation_date__2': '2017-11-03 11:21:09', 'review_score__2': 5, 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_comment_message__2': None, 'review_comment_title__3': None, 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_creation_date__3': '2017-05-04 01:11:07', 'review_score__3': 4, 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_comment_message__3': None, 'review_comment_title__4': None, 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_creation_date__4': '2017-12-13 01:16:52', 'review_score__4': 5, 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_comment_message__4': 'Recomendo', 'review_comment_title__5': None, 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_creation_date__5': '2018-08-23 13:20:20', 'review_score__5': 1, 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_comment_message__5': 'Produto não foi entregue!', 'review_comment_title__6': None, 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_creation_date__6': '2018-02-09 15:11:21', 'review_score__6': 1, 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_comment_title__7': None, 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_creation_date__7': '2018-03-29 23:50:09', 'review_score__7': 1, 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_comment_title__8': 'Ótimo', 'order_id__8': '4cae9d93ca1f1266e9ddfc79f7ff85db' ... 5900 parameters truncated ... 'review_id__991': '5764bbed44b798460013b2e4f9512d10', 'review_comment_message__991': None, 'review_comment_title__992': None, 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_creation_date__992': '2017-03-22 13:00:09', 'review_score__992': 5, 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_comment_title__993': None, 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_creation_date__993': '2018-02-22 10:46:59', 'review_score__993': 5, 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_comment_message__993': None, 'review_comment_title__994': None, 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_creation_date__994': '2018-01-26 01:07:05', 'review_score__994': 5, 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_comment_message__994': None, 'review_comment_title__995': None, 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_creation_date__995': '2017-11-06 21:31:59', 'review_score__995': 4, 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_comment_message__995': 'recomendo', 'review_comment_title__996': 'Ótimo atendimento ', 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_creation_date__996': '2018-07-23 20:39:23', 'review_score__996': 5, 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_comment_message__996': 'Loja confiável ', 'review_comment_title__997': None, 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_creation_date__997': '2018-04-09 11:36:53', 'review_score__997': 2, 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_comment_message__997': None, 'review_comment_title__998': None, 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_creation_date__998': '2017-06-23 03:33:40', 'review_score__998': 5, 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_comment_message__998': None, 'review_comment_title__999': None, 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_creation_date__999': '2018-03-27 01:42:14', 'review_score__999': 5, 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 22:46:24,606 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:46:24,613 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 22:46:24,613 - DEBUG - Asking scheduler for work...
2024-12-21 22:46:24,616 - DEBUG - Done
2024-12-21 22:46:24,616 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:46:24,616 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 22:46:24,616 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 22:46:24,616 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 22:46:24,616 - INFO - Worker Worker(salt=1566298011, workers=1, host=MSI, username=alfayyedh, pid=76933) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:46:24,618 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:46:32,840 - INFO - Read Transform Query - SUCCESS
2024-12-21 22:46:32,902 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:46:32,902 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 22:46:32,931 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 22:46:32,935 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 22:46:32,938 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 22:46:32,947 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 22:46:32,986 - ERROR - Transform Tables - FAILED
2024-12-21 22:46:32,989 - ERROR - [pid 77165] Worker Worker(salt=2074922856, workers=1, host=MSI, username=alfayyedh, pid=77165) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_id" is of type uuid but expression is of type text
LINE 22:  so.order_id,
          ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_id" is of type uuid but expression is of type text
LINE 22:  so.order_id,
          ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id,
	dc.customer_id,
    so.order_status,
    oi.price,
    dd1.date_actual  AS order_purchase_date,
    dt1.time_actual AS order_purchase_time,
    dd2.date_actual AS order_approved_at_date,
    dt2.time_actual AS order_approved_at_time,
    dd3.date_actual AS order_delivered_carrier_date,
    dt3.time_actual AS order_delivered_carrier_time,
    dd4.date_actual AS order_delivered_customer_date,
    dt4.time_actual AS order_delivered_customer_time,
    dd5.date_actual AS order_estimated_delivery_date,
    dt5.time_actual AS order_estimated_delivery_time,
    dd6.date_actual AS shipping_limit_date,
    dt6.time_actual AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    final.dim_date dd1 ON dd1.date_actual = DATE(so.order_purchase_timestamp)
JOIN
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN
    final.dim_date dd2 ON dd2.date_actual = DATE(so.order_approved_at)
JOIN
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN
    final.dim_date dd3 ON dd3.date_actual = DATE(so.order_delivered_carrier_date)
JOIN
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN
    final.dim_date dd4 ON dd4.date_actual = DATE(so.order_delivered_customer_date)
JOIN
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN
    final.dim_date dd5 ON dd5.date_actual = DATE(so.order_estimated_delivery_date)
JOIN
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN
    final.dim_date dd6 ON dd6.date_actual = DATE(oi.shipping_limit_date)
JOIN
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time

	

ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 22:46:33,007 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:46:33,013 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 22:46:33,013 - DEBUG - Asking scheduler for work...
2024-12-21 22:46:33,015 - DEBUG - Done
2024-12-21 22:46:33,016 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:46:33,016 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 22:46:33,016 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 22:46:33,016 - INFO - Worker Worker(salt=2074922856, workers=1, host=MSI, username=alfayyedh, pid=77165) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:46:33,017 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:49:44,517 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 22:49:44,544 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 22:49:44,795 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 22:49:44,908 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 22:49:44,922 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 22:49:45,543 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 22:49:46,136 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 22:49:46,605 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 22:49:47,502 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 22:49:48,310 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 22:49:48,310 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 22:49:48,311 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 22:49:48,336 - INFO - [pid 78539] Worker Worker(salt=2492951295, workers=1, host=MSI, username=alfayyedh, pid=78539) done      Extract()
2024-12-21 22:49:48,337 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:49:48,340 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 22:49:48,340 - DEBUG - Asking scheduler for work...
2024-12-21 22:49:48,341 - DEBUG - Pending tasks: 2
2024-12-21 22:49:48,341 - INFO - [pid 78539] Worker Worker(salt=2492951295, workers=1, host=MSI, username=alfayyedh, pid=78539) running   Load()
2024-12-21 22:49:48,342 - INFO - Read Load Query - SUCCESS
2024-12-21 22:49:49,323 - INFO - Read Extracted Data - SUCCESS
2024-12-21 22:49:49,324 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:49:49,574 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 22:49:49,574 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 22:49:49,586 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 22:49:51,235 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 22:49:51,717 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 22:49:51,804 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 22:49:52,392 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 22:49:52,425 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 22:49:52,430 - ERROR - [pid 78539] Worker Worker(salt=2492951295, workers=1, host=MSI, username=alfayyedh, pid=78539) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_comment_title__0': None, 'review_score__0': 1, 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_creation_date__0': '2018-04-11 09:55:40', 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_comment_title__1': None, 'review_score__1': 4, 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_creation_date__1': '2018-03-31 14:25:05', 'review_comment_message__1': None, 'review_comment_title__2': None, 'review_score__2': 5, 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_creation_date__2': '2017-11-03 11:21:09', 'review_comment_message__2': None, 'review_comment_title__3': None, 'review_score__3': 4, 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_creation_date__3': '2017-05-04 01:11:07', 'review_comment_message__3': None, 'review_comment_title__4': None, 'review_score__4': 5, 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_creation_date__4': '2017-12-13 01:16:52', 'review_comment_message__4': 'Recomendo', 'review_comment_title__5': None, 'review_score__5': 1, 'review_id__5': '6be6676c6938834589c04bf934756e79', 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_creation_date__5': '2018-08-23 13:20:20', 'review_comment_message__5': 'Produto não foi entregue!', 'review_comment_title__6': None, 'review_score__6': 1, 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_creation_date__6': '2018-02-09 15:11:21', 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_comment_title__7': None, 'review_score__7': 1, 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_creation_date__7': '2018-03-29 23:50:09', 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_comment_title__8': 'Ótimo', 'review_score__8': 4 ... 5900 parameters truncated ... 'review_creation_date__991': '2018-05-23 11:32:06', 'review_comment_message__991': None, 'review_comment_title__992': None, 'review_score__992': 5, 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_creation_date__992': '2017-03-22 13:00:09', 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_comment_title__993': None, 'review_score__993': 5, 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_creation_date__993': '2018-02-22 10:46:59', 'review_comment_message__993': None, 'review_comment_title__994': None, 'review_score__994': 5, 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_creation_date__994': '2018-01-26 01:07:05', 'review_comment_message__994': None, 'review_comment_title__995': None, 'review_score__995': 4, 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_creation_date__995': '2017-11-06 21:31:59', 'review_comment_message__995': 'recomendo', 'review_comment_title__996': 'Ótimo atendimento ', 'review_score__996': 5, 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_creation_date__996': '2018-07-23 20:39:23', 'review_comment_message__996': 'Loja confiável ', 'review_comment_title__997': None, 'review_score__997': 2, 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_creation_date__997': '2018-04-09 11:36:53', 'review_comment_message__997': None, 'review_comment_title__998': None, 'review_score__998': 5, 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_creation_date__998': '2017-06-23 03:33:40', 'review_comment_message__998': None, 'review_comment_title__999': None, 'review_score__999': 5, 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_creation_date__999': '2018-03-27 01:42:14', 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 22:49:52,499 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:49:52,507 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 22:49:52,507 - DEBUG - Asking scheduler for work...
2024-12-21 22:49:52,510 - DEBUG - Done
2024-12-21 22:49:52,510 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:49:52,510 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 22:49:52,510 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 22:49:52,510 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 22:49:52,511 - INFO - Worker Worker(salt=2492951295, workers=1, host=MSI, username=alfayyedh, pid=78539) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:49:52,513 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 22:50:00,304 - INFO - Read Transform Query - SUCCESS
2024-12-21 22:50:00,351 - INFO - Connect to DWH - SUCCESS
2024-12-21 22:50:00,351 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 22:50:00,373 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 22:50:00,375 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 22:50:00,376 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 22:50:00,380 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 22:50:00,410 - ERROR - Transform Tables - FAILED
2024-12-21 22:50:00,412 - ERROR - [pid 78756] Worker Worker(salt=1978512532, workers=1, host=MSI, username=alfayyedh, pid=78756) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_purchase_date" is of type integer but expression is of type date
LINE 26:     dd1.date_actual AS order_purchase_date,
             ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_purchase_date" is of type integer but expression is of type date
LINE 26:     dd1.date_actual AS order_purchase_date,
             ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id::uuid,
	dc.customer_id,
    so.order_status,
    oi.price,
    dd1.date_actual AS order_purchase_date,
    dt1.time_actual AS order_purchase_time,
    dd2.date_actual AS order_approved_at_date,
    dt2.time_actual AS order_approved_at_time,
    dd3.date_actual AS order_delivered_carrier_date,
    dt3.time_actual AS order_delivered_carrier_time,
    dd4.date_actual AS order_delivered_customer_date,
    dt4.time_actual AS order_delivered_customer_time,
    dd5.date_actual AS order_estimated_delivery_date,
    dt5.time_actual AS order_estimated_delivery_time,
    dd6.date_actual AS shipping_limit_date,
    dt6.time_actual AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    final.dim_date dd1 ON dd1.date_actual = DATE(so.order_purchase_timestamp)
JOIN
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN
    final.dim_date dd2 ON dd2.date_actual = DATE(so.order_approved_at)
JOIN
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN
    final.dim_date dd3 ON dd3.date_actual = DATE(so.order_delivered_carrier_date)
JOIN
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN
    final.dim_date dd4 ON dd4.date_actual = DATE(so.order_delivered_customer_date)
JOIN
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN
    final.dim_date dd5 ON dd5.date_actual = DATE(so.order_estimated_delivery_date)
JOIN
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN
    final.dim_date dd6 ON dd6.date_actual = DATE(oi.shipping_limit_date)
JOIN
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time

	

ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 22:50:00,427 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 22:50:00,436 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 22:50:00,436 - DEBUG - Asking scheduler for work...
2024-12-21 22:50:00,440 - DEBUG - Done
2024-12-21 22:50:00,440 - DEBUG - There are no more tasks to run at this time
2024-12-21 22:50:00,440 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 22:50:00,440 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 22:50:00,440 - INFO - Worker Worker(salt=1978512532, workers=1, host=MSI, username=alfayyedh, pid=78756) was stopped. Shutting down Keep-Alive thread
2024-12-21 22:50:00,441 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:28:49,271 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 23:28:49,335 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 23:28:49,610 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 23:28:49,730 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 23:28:49,747 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 23:28:50,496 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 23:28:51,127 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 23:28:51,662 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 23:28:52,654 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 23:28:53,722 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 23:28:53,722 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 23:28:53,726 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 23:28:53,754 - INFO - [pid 94605] Worker Worker(salt=3049985556, workers=1, host=MSI, username=alfayyedh, pid=94605) done      Extract()
2024-12-21 23:28:53,755 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:28:53,758 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 23:28:53,758 - DEBUG - Asking scheduler for work...
2024-12-21 23:28:53,760 - DEBUG - Pending tasks: 2
2024-12-21 23:28:53,760 - INFO - [pid 94605] Worker Worker(salt=3049985556, workers=1, host=MSI, username=alfayyedh, pid=94605) running   Load()
2024-12-21 23:28:53,761 - INFO - Read Load Query - SUCCESS
2024-12-21 23:28:54,768 - INFO - Read Extracted Data - SUCCESS
2024-12-21 23:28:54,769 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:28:55,061 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 23:28:55,061 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 23:28:55,081 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 23:28:56,888 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 23:28:57,358 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 23:28:57,443 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 23:28:58,349 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 23:28:58,387 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 23:28:58,392 - ERROR - [pid 94605] Worker Worker(salt=3049985556, workers=1, host=MSI, username=alfayyedh, pid=94605) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_score__0': 1, 'review_comment_title__0': None, 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_creation_date__0': '2018-04-11 09:55:40', 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_score__1': 4, 'review_comment_title__1': None, 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_creation_date__1': '2018-03-31 14:25:05', 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_comment_message__1': None, 'review_score__2': 5, 'review_comment_title__2': None, 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_creation_date__2': '2017-11-03 11:21:09', 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_comment_message__2': None, 'review_score__3': 4, 'review_comment_title__3': None, 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_creation_date__3': '2017-05-04 01:11:07', 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_comment_message__3': None, 'review_score__4': 5, 'review_comment_title__4': None, 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_creation_date__4': '2017-12-13 01:16:52', 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_comment_message__4': 'Recomendo', 'review_score__5': 1, 'review_comment_title__5': None, 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_creation_date__5': '2018-08-23 13:20:20', 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_comment_message__5': 'Produto não foi entregue!', 'review_score__6': 1, 'review_comment_title__6': None, 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_creation_date__6': '2018-02-09 15:11:21', 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_score__7': 1, 'review_comment_title__7': None, 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_creation_date__7': '2018-03-29 23:50:09', 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_score__8': 4, 'review_comment_title__8': 'Ótimo' ... 5900 parameters truncated ... 'order_id__991': 'f056d4857c146c38ff02566b3e152bca', 'review_comment_message__991': None, 'review_score__992': 5, 'review_comment_title__992': None, 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_creation_date__992': '2017-03-22 13:00:09', 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_score__993': 5, 'review_comment_title__993': None, 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_creation_date__993': '2018-02-22 10:46:59', 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_comment_message__993': None, 'review_score__994': 5, 'review_comment_title__994': None, 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_creation_date__994': '2018-01-26 01:07:05', 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_comment_message__994': None, 'review_score__995': 4, 'review_comment_title__995': None, 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_creation_date__995': '2017-11-06 21:31:59', 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_comment_message__995': 'recomendo', 'review_score__996': 5, 'review_comment_title__996': 'Ótimo atendimento ', 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_creation_date__996': '2018-07-23 20:39:23', 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_comment_message__996': 'Loja confiável ', 'review_score__997': 2, 'review_comment_title__997': None, 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_creation_date__997': '2018-04-09 11:36:53', 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_comment_message__997': None, 'review_score__998': 5, 'review_comment_title__998': None, 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_creation_date__998': '2017-06-23 03:33:40', 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_comment_message__998': None, 'review_score__999': 5, 'review_comment_title__999': None, 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'review_creation_date__999': '2018-03-27 01:42:14', 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 23:28:58,580 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:28:58,589 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 23:28:58,589 - DEBUG - Asking scheduler for work...
2024-12-21 23:28:58,592 - DEBUG - Done
2024-12-21 23:28:58,592 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:28:58,592 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 23:28:58,592 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 23:28:58,592 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 23:28:58,592 - INFO - Worker Worker(salt=3049985556, workers=1, host=MSI, username=alfayyedh, pid=94605) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:28:58,593 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:29:08,867 - INFO - Read Transform Query - SUCCESS
2024-12-21 23:29:08,908 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:29:08,908 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 23:29:08,934 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 23:29:08,936 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 23:29:08,938 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 23:29:08,944 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 23:29:08,975 - ERROR - Transform Tables - FAILED
2024-12-21 23:29:08,977 - ERROR - [pid 94868] Worker Worker(salt=5887188114, workers=1, host=MSI, username=alfayyedh, pid=94868) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: function to_date(text) does not exist
LINE 45:     final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order...
                                                     ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) function to_date(text) does not exist
LINE 45:     final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order...
                                                     ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id::uuid,
	dc.customer_id,
    so.order_status,
    oi.price,
    dd1.date_actual AS order_purchase_date,
    dt1.time_actual AS order_purchase_time,
    dd2.date_actual AS order_approved_at_date,
    dt2.time_actual AS order_approved_at_time,
    dd3.date_actual AS order_delivered_carrier_date,
    dt3.time_actual AS order_delivered_carrier_time,
    dd4.date_actual AS order_delivered_customer_date,
    dt4.time_actual AS order_delivered_customer_time,
    dd5.date_actual AS order_estimated_delivery_date,
    dt5.time_actual AS order_estimated_delivery_time,
    dd6.date_actual AS shipping_limit_date,
    dt6.time_actual AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order_purchase_timestamp)
JOIN
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN
    final.dim_date dd2 ON dd2.date_actual = TO_DATE(so.order_approved_at)
JOIN
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN
    final.dim_date dd3 ON dd3.date_actual = TO_DATE(so.order_delivered_carrier_date)
JOIN
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN
    final.dim_date dd4 ON dd4.date_actual = TO_DATE(so.order_delivered_customer_date)
JOIN
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN
    final.dim_date dd5 ON dd5.date_actual = TO_DATE(so.order_estimated_delivery_date)
JOIN
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN
    final.dim_date dd6 ON dd6.date_actual = TO_DATE(oi.shipping_limit_date)
JOIN
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time

	

ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 23:29:08,997 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:29:09,003 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 23:29:09,003 - DEBUG - Asking scheduler for work...
2024-12-21 23:29:09,005 - DEBUG - Done
2024-12-21 23:29:09,005 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:29:09,005 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 23:29:09,005 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 23:29:09,006 - INFO - Worker Worker(salt=5887188114, workers=1, host=MSI, username=alfayyedh, pid=94868) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:29:09,007 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:32:26,837 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 23:32:26,876 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 23:32:27,129 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 23:32:27,234 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 23:32:27,247 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 23:32:27,846 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 23:32:28,580 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 23:32:29,131 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 23:32:29,996 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 23:32:30,901 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 23:32:30,902 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 23:32:30,904 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 23:32:30,958 - INFO - [pid 96287] Worker Worker(salt=2182930204, workers=1, host=MSI, username=alfayyedh, pid=96287) done      Extract()
2024-12-21 23:32:30,959 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:32:30,962 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 23:32:30,962 - DEBUG - Asking scheduler for work...
2024-12-21 23:32:30,964 - DEBUG - Pending tasks: 2
2024-12-21 23:32:30,964 - INFO - [pid 96287] Worker Worker(salt=2182930204, workers=1, host=MSI, username=alfayyedh, pid=96287) running   Load()
2024-12-21 23:32:30,964 - INFO - Read Load Query - SUCCESS
2024-12-21 23:32:32,073 - INFO - Read Extracted Data - SUCCESS
2024-12-21 23:32:32,074 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:32:32,330 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 23:32:32,330 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 23:32:32,343 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 23:32:34,094 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 23:32:34,528 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 23:32:34,613 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 23:32:35,225 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 23:32:35,258 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 23:32:35,263 - ERROR - [pid 96287] Worker Worker(salt=2182930204, workers=1, host=MSI, username=alfayyedh, pid=96287) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_comment_title__0': None, 'review_creation_date__0': '2018-04-11 09:55:40', 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_score__0': 1, 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_comment_title__1': None, 'review_creation_date__1': '2018-03-31 14:25:05', 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_score__1': 4, 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_comment_message__1': None, 'review_comment_title__2': None, 'review_creation_date__2': '2017-11-03 11:21:09', 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_score__2': 5, 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_comment_message__2': None, 'review_comment_title__3': None, 'review_creation_date__3': '2017-05-04 01:11:07', 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_score__3': 4, 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_comment_message__3': None, 'review_comment_title__4': None, 'review_creation_date__4': '2017-12-13 01:16:52', 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_score__4': 5, 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_comment_message__4': 'Recomendo', 'review_comment_title__5': None, 'review_creation_date__5': '2018-08-23 13:20:20', 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_score__5': 1, 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_comment_message__5': 'Produto não foi entregue!', 'review_comment_title__6': None, 'review_creation_date__6': '2018-02-09 15:11:21', 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_score__6': 1, 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_comment_title__7': None, 'review_creation_date__7': '2018-03-29 23:50:09', 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_score__7': 1, 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_comment_title__8': 'Ótimo', 'review_creation_date__8': '2018-08-31 23:20:43' ... 5900 parameters truncated ... 'review_id__991': '5764bbed44b798460013b2e4f9512d10', 'review_comment_message__991': None, 'review_comment_title__992': None, 'review_creation_date__992': '2017-03-22 13:00:09', 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_score__992': 5, 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_comment_title__993': None, 'review_creation_date__993': '2018-02-22 10:46:59', 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_score__993': 5, 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_comment_message__993': None, 'review_comment_title__994': None, 'review_creation_date__994': '2018-01-26 01:07:05', 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_score__994': 5, 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_comment_message__994': None, 'review_comment_title__995': None, 'review_creation_date__995': '2017-11-06 21:31:59', 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_score__995': 4, 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_comment_message__995': 'recomendo', 'review_comment_title__996': 'Ótimo atendimento ', 'review_creation_date__996': '2018-07-23 20:39:23', 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_score__996': 5, 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_comment_message__996': 'Loja confiável ', 'review_comment_title__997': None, 'review_creation_date__997': '2018-04-09 11:36:53', 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_score__997': 2, 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_comment_message__997': None, 'review_comment_title__998': None, 'review_creation_date__998': '2017-06-23 03:33:40', 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_score__998': 5, 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_comment_message__998': None, 'review_comment_title__999': None, 'review_creation_date__999': '2018-03-27 01:42:14', 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_score__999': 5, 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 23:32:35,337 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:32:35,343 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 23:32:35,344 - DEBUG - Asking scheduler for work...
2024-12-21 23:32:35,346 - DEBUG - Done
2024-12-21 23:32:35,346 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:32:35,346 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 23:32:35,346 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 23:32:35,346 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 23:32:35,347 - INFO - Worker Worker(salt=2182930204, workers=1, host=MSI, username=alfayyedh, pid=96287) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:32:35,348 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:32:38,937 - INFO - Read Transform Query - SUCCESS
2024-12-21 23:32:38,976 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:32:38,977 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 23:32:38,998 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 23:32:39,000 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 23:32:39,001 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 23:32:39,006 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 23:32:39,037 - ERROR - Transform Tables - FAILED
2024-12-21 23:32:39,039 - ERROR - [pid 96483] Worker Worker(salt=6132671008, workers=1, host=MSI, username=alfayyedh, pid=96483) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_purchase_date" is of type integer but expression is of type date
LINE 26:     dd1.date_actual AS order_purchase_date,
             ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_purchase_date" is of type integer but expression is of type date
LINE 26:     dd1.date_actual AS order_purchase_date,
             ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id::uuid,
	dc.customer_id,
    so.order_status,
    oi.price,
    dd1.date_actual AS order_purchase_date,
    dt1.time_actual AS order_purchase_time,
    dd2.date_actual AS order_approved_at_date,
    dt2.time_actual AS order_approved_at_time,
    dd3.date_actual AS order_delivered_carrier_date,
    dt3.time_actual AS order_delivered_carrier_time,
    dd4.date_actual AS order_delivered_customer_date,
    dt4.time_actual AS order_delivered_customer_time,
    dd5.date_actual AS order_estimated_delivery_date,
    dt5.time_actual AS order_estimated_delivery_time,
    dd6.date_actual AS shipping_limit_date,
    dt6.time_actual AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN 
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order_purchase_timestamp::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN 
    final.dim_date dd2 ON dd2.date_actual = TO_DATE(so.order_approved_at::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN 
    final.dim_date dd3 ON dd3.date_actual = TO_DATE(so.order_delivered_carrier_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN 
    final.dim_date dd4 ON dd4.date_actual = TO_DATE(so.order_delivered_customer_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN 
    final.dim_date dd5 ON dd5.date_actual = TO_DATE(so.order_estimated_delivery_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN 
    final.dim_date dd6 ON dd6.date_actual = TO_DATE(oi.shipping_limit_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time


ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 23:32:39,053 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:32:39,058 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 23:32:39,058 - DEBUG - Asking scheduler for work...
2024-12-21 23:32:39,060 - DEBUG - Done
2024-12-21 23:32:39,060 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:32:39,060 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 23:32:39,060 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 23:32:39,060 - INFO - Worker Worker(salt=6132671008, workers=1, host=MSI, username=alfayyedh, pid=96483) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:32:39,061 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:46:38,867 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 23:46:38,924 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 23:46:39,193 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 23:46:39,299 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 23:46:39,314 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 23:46:40,077 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 23:46:40,684 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 23:46:41,215 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 23:46:42,187 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 23:46:43,133 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 23:46:43,134 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 23:46:43,138 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 23:46:43,166 - INFO - [pid 102329] Worker Worker(salt=9852257563, workers=1, host=MSI, username=alfayyedh, pid=102329) done      Extract()
2024-12-21 23:46:43,167 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:46:43,170 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 23:46:43,170 - DEBUG - Asking scheduler for work...
2024-12-21 23:46:43,172 - DEBUG - Pending tasks: 2
2024-12-21 23:46:43,172 - INFO - [pid 102329] Worker Worker(salt=9852257563, workers=1, host=MSI, username=alfayyedh, pid=102329) running   Load()
2024-12-21 23:46:43,173 - INFO - Read Load Query - SUCCESS
2024-12-21 23:46:44,288 - INFO - Read Extracted Data - SUCCESS
2024-12-21 23:46:44,289 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:46:44,572 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 23:46:44,572 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 23:46:44,587 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 23:46:46,347 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 23:46:46,833 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 23:46:46,932 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 23:46:47,708 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 23:46:47,749 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 23:46:47,754 - ERROR - [pid 102329] Worker Worker(salt=9852257563, workers=1, host=MSI, username=alfayyedh, pid=102329) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_comment_title__0': None, 'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_score__0': 1, 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_creation_date__0': '2018-04-11 09:55:40', 'review_comment_title__1': None, 'review_comment_message__1': None, 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_score__1': 4, 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_creation_date__1': '2018-03-31 14:25:05', 'review_comment_title__2': None, 'review_comment_message__2': None, 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_score__2': 5, 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_creation_date__2': '2017-11-03 11:21:09', 'review_comment_title__3': None, 'review_comment_message__3': None, 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_score__3': 4, 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_creation_date__3': '2017-05-04 01:11:07', 'review_comment_title__4': None, 'review_comment_message__4': 'Recomendo', 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_score__4': 5, 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_creation_date__4': '2017-12-13 01:16:52', 'review_comment_title__5': None, 'review_comment_message__5': 'Produto não foi entregue!', 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_score__5': 1, 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_creation_date__5': '2018-08-23 13:20:20', 'review_comment_title__6': None, 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_score__6': 1, 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_creation_date__6': '2018-02-09 15:11:21', 'review_comment_title__7': None, 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_score__7': 1, 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_creation_date__7': '2018-03-29 23:50:09', 'review_comment_title__8': 'Ótimo', 'review_comment_message__8': 'Adoro comprar nas lojas lannister' ... 5900 parameters truncated ... 'order_id__991': 'f056d4857c146c38ff02566b3e152bca', 'review_creation_date__991': '2018-05-23 11:32:06', 'review_comment_title__992': None, 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_score__992': 5, 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_creation_date__992': '2017-03-22 13:00:09', 'review_comment_title__993': None, 'review_comment_message__993': None, 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_score__993': 5, 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_creation_date__993': '2018-02-22 10:46:59', 'review_comment_title__994': None, 'review_comment_message__994': None, 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_score__994': 5, 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_creation_date__994': '2018-01-26 01:07:05', 'review_comment_title__995': None, 'review_comment_message__995': 'recomendo', 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_score__995': 4, 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_creation_date__995': '2017-11-06 21:31:59', 'review_comment_title__996': 'Ótimo atendimento ', 'review_comment_message__996': 'Loja confiável ', 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_score__996': 5, 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_creation_date__996': '2018-07-23 20:39:23', 'review_comment_title__997': None, 'review_comment_message__997': None, 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_score__997': 2, 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_creation_date__997': '2018-04-09 11:36:53', 'review_comment_title__998': None, 'review_comment_message__998': None, 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_score__998': 5, 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_creation_date__998': '2017-06-23 03:33:40', 'review_comment_title__999': None, 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.', 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'review_score__999': 5, 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_creation_date__999': '2018-03-27 01:42:14'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 23:46:47,873 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:46:47,883 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 23:46:47,883 - DEBUG - Asking scheduler for work...
2024-12-21 23:46:47,886 - DEBUG - Done
2024-12-21 23:46:47,886 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:46:47,886 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 23:46:47,886 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 23:46:47,886 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 23:46:47,887 - INFO - Worker Worker(salt=9852257563, workers=1, host=MSI, username=alfayyedh, pid=102329) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:46:47,889 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:46:52,969 - INFO - Read Transform Query - SUCCESS
2024-12-21 23:46:53,020 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:46:53,020 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 23:46:53,049 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 23:46:53,052 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 23:46:53,054 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 23:46:53,070 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 23:46:53,101 - ERROR - Transform Tables - FAILED
2024-12-21 23:46:53,103 - ERROR - [pid 102544] Worker Worker(salt=5729488995, workers=1, host=MSI, username=alfayyedh, pid=102544) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidColumnReference: there is no unique or exclusion constraint matching the ON CONFLICT specification


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.InvalidColumnReference) there is no unique or exclusion constraint matching the ON CONFLICT specification

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id::uuid,
	dc.customer_id,
    so.order_status,
    oi.price,
    dd1.date_id AS order_purchase_date,
    dt1.time_id AS order_purchase_time,
    dd2.date_id AS order_approved_at_date,
    dt2.time_id AS order_approved_at_time,
    dd3.date_id AS order_delivered_carrier_date,
    dt3.time_id AS order_delivered_carrier_time,
    dd4.date_id AS order_delivered_customer_date,
    dt4.time_id AS order_delivered_customer_time,
    dd5.date_id AS order_estimated_delivery_date,
    dt5.time_id AS order_estimated_delivery_time,
    dd6.date_id AS shipping_limit_date,
    dt6.time_id AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN 
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order_purchase_timestamp::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN 
    final.dim_date dd2 ON dd2.date_actual = TO_DATE(so.order_approved_at::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN 
    final.dim_date dd3 ON dd3.date_actual = TO_DATE(so.order_delivered_carrier_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN 
    final.dim_date dd4 ON dd4.date_actual = TO_DATE(so.order_delivered_customer_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN 
    final.dim_date dd5 ON dd5.date_actual = TO_DATE(so.order_estimated_delivery_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN 
    final.dim_date dd6 ON dd6.date_actual = TO_DATE(oi.shipping_limit_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time


ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 23:46:53,120 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:46:53,127 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 23:46:53,128 - DEBUG - Asking scheduler for work...
2024-12-21 23:46:53,134 - DEBUG - Done
2024-12-21 23:46:53,135 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:46:53,135 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 23:46:53,135 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 23:46:53,136 - INFO - Worker Worker(salt=5729488995, workers=1, host=MSI, username=alfayyedh, pid=102544) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:46:53,137 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:54:06,653 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 23:54:06,688 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 23:54:06,945 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 23:54:07,072 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 23:54:07,088 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 23:54:07,690 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 23:54:08,287 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 23:54:08,788 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 23:54:09,664 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 23:54:10,449 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 23:54:10,449 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 23:54:10,451 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 23:54:10,475 - INFO - [pid 105639] Worker Worker(salt=3010515216, workers=1, host=MSI, username=alfayyedh, pid=105639) done      Extract()
2024-12-21 23:54:10,477 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:54:10,482 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 23:54:10,482 - DEBUG - Asking scheduler for work...
2024-12-21 23:54:10,486 - DEBUG - Pending tasks: 2
2024-12-21 23:54:10,486 - INFO - [pid 105639] Worker Worker(salt=3010515216, workers=1, host=MSI, username=alfayyedh, pid=105639) running   Load()
2024-12-21 23:54:10,487 - INFO - Read Load Query - SUCCESS
2024-12-21 23:54:11,489 - INFO - Read Extracted Data - SUCCESS
2024-12-21 23:54:11,490 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:54:11,723 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 23:54:11,724 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 23:54:11,736 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 23:54:13,338 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 23:54:13,766 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 23:54:13,847 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 23:54:14,577 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 23:54:14,623 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 23:54:14,629 - ERROR - [pid 105639] Worker Worker(salt=3010515216, workers=1, host=MSI, username=alfayyedh, pid=105639) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (order_id)=(5040757d4e06a4be96d3827b860b4e7c) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_comment_message__0': 'O produto foi encaminhado para cidade de destino no dia 31/03 e até o momento não recebi.', 'review_comment_title__0': None, 'review_score__0': 1, 'order_id__0': '62fd709303ada1fdabb94af48092000b', 'review_id__0': '536c631d5f15eab8260d69b603779f99', 'review_creation_date__0': '2018-04-11 09:55:40', 'review_comment_message__1': None, 'review_comment_title__1': None, 'review_score__1': 4, 'order_id__1': 'd0951ca2244d54908c43b6de2aec3974', 'review_id__1': '678f7d06016ed2f20072ed190c70fe7e', 'review_creation_date__1': '2018-03-31 14:25:05', 'review_comment_message__2': None, 'review_comment_title__2': None, 'review_score__2': 5, 'order_id__2': '1578453b93b542fbde0dd65c46a7dbf7', 'review_id__2': 'ee6b5f3a217f7427d73615be68c17146', 'review_creation_date__2': '2017-11-03 11:21:09', 'review_comment_message__3': None, 'review_comment_title__3': None, 'review_score__3': 4, 'order_id__3': '27a7ac440630f9de788e4d66e72c6424', 'review_id__3': '05c968cc75440c647f58e6bfe1ee15bb', 'review_creation_date__3': '2017-05-04 01:11:07', 'review_comment_message__4': 'Recomendo', 'review_comment_title__4': None, 'review_score__4': 5, 'order_id__4': '264deac888717a54971fa3109f6709e3', 'review_id__4': 'fadbb6e8c12f707455054fbd9efcd289', 'review_creation_date__4': '2017-12-13 01:16:52', 'review_comment_message__5': 'Produto não foi entregue!', 'review_comment_title__5': None, 'review_score__5': 1, 'order_id__5': 'a7c96c67b28612698cc5e0fb735dad94', 'review_id__5': '6be6676c6938834589c04bf934756e79', 'review_creation_date__5': '2018-08-23 13:20:20', 'review_comment_message__6': 'Simplesmente não recebi meu produto e nao tenho um posicionamento decente do que acontecerá.', 'review_comment_title__6': None, 'review_score__6': 1, 'order_id__6': 'f4a8cf5dcabdbe8dd7e65e1674db3015', 'review_id__6': '1a8dab4afb8c80d8d257fe0156d764b1', 'review_creation_date__6': '2018-02-09 15:11:21', 'review_comment_message__7': 'Não recebi o braço de ducha com desviador, somente a ducha 20x20 com braço de 30cm.', 'review_comment_title__7': None, 'review_score__7': 1, 'order_id__7': 'af1ffd059ac0a5f3996b4fe6c2863116', 'review_id__7': 'bceb8ea45e016d1107b14400a154e4b1', 'review_creation_date__7': '2018-03-29 23:50:09', 'review_comment_message__8': 'Adoro comprar nas lojas lannister', 'review_comment_title__8': 'Ótimo' ... 5900 parameters truncated ... 'review_id__991': '5764bbed44b798460013b2e4f9512d10', 'review_creation_date__991': '2018-05-23 11:32:06', 'review_comment_message__992': 'Produto adequado as minhas necessidades', 'review_comment_title__992': None, 'review_score__992': 5, 'order_id__992': '8ac915a1f508baa87e8fc8d596aa6fc1', 'review_id__992': 'ea14652d369f12aa5604c83139ba287a', 'review_creation_date__992': '2017-03-22 13:00:09', 'review_comment_message__993': None, 'review_comment_title__993': None, 'review_score__993': 5, 'order_id__993': 'f72e1fcd8c83f302bbb4a322b118531d', 'review_id__993': '0f44a5f45a6a68d5d328d1ba2ab6eb77', 'review_creation_date__993': '2018-02-22 10:46:59', 'review_comment_message__994': None, 'review_comment_title__994': None, 'review_score__994': 5, 'order_id__994': 'afeb3354fc3b7d5c309239667cbee2b8', 'review_id__994': 'cede16d08530c51116a77077faf1bf13', 'review_creation_date__994': '2018-01-26 01:07:05', 'review_comment_message__995': 'recomendo', 'review_comment_title__995': None, 'review_score__995': 4, 'order_id__995': 'a4521fa62b4f3bbe260aecf4dbc78935', 'review_id__995': '9fb40911b4fd46e3892da8fdc7dbe5c0', 'review_creation_date__995': '2017-11-06 21:31:59', 'review_comment_message__996': 'Loja confiável ', 'review_comment_title__996': 'Ótimo atendimento ', 'review_score__996': 5, 'order_id__996': '96cf7ecd8c676d61be261ffdda90b008', 'review_id__996': '9d25a54a1a44411d1adf1a5fa4557445', 'review_creation_date__996': '2018-07-23 20:39:23', 'review_comment_message__997': None, 'review_comment_title__997': None, 'review_score__997': 2, 'order_id__997': 'cd590811ef0d897b885bc97352f1b34f', 'review_id__997': '758b8a4c6a46d5704212e3cd5c085756', 'review_creation_date__997': '2018-04-09 11:36:53', 'review_comment_message__998': None, 'review_comment_title__998': None, 'review_score__998': 5, 'order_id__998': 'c8aae809e0a82502cf4254f5b9fd463e', 'review_id__998': '8a4fd6880a33d3f4d8e740b4411bf563', 'review_creation_date__998': '2017-06-23 03:33:40', 'review_comment_message__999': 'Produto de excelente qualidade, vendedores responsáveis, entrega no prazo... Super satisfeita.', 'review_comment_title__999': None, 'review_score__999': 5, 'order_id__999': 'a91cd0ab480b37e19e1f6910d623d54c', 'review_id__999': 'bda1fa3708f9c6433b7ed6d068e03c92', 'review_creation_date__999': '2018-03-27 01:42:14'}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 23:54:14,746 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:54:14,753 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 23:54:14,753 - DEBUG - Asking scheduler for work...
2024-12-21 23:54:14,755 - DEBUG - Done
2024-12-21 23:54:14,755 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:54:14,755 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 23:54:14,755 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 23:54:14,756 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 23:54:14,756 - INFO - Worker Worker(salt=3010515216, workers=1, host=MSI, username=alfayyedh, pid=105639) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:54:14,758 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:54:18,211 - INFO - Read Transform Query - SUCCESS
2024-12-21 23:54:18,250 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:54:18,250 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 23:54:18,317 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-21 23:54:18,318 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-21 23:54:18,320 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-21 23:54:18,328 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 23:54:18,360 - ERROR - Transform Tables - FAILED
2024-12-21 23:54:18,361 - ERROR - [pid 105813] Worker Worker(salt=5540185103, workers=1, host=MSI, username=alfayyedh, pid=105813) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidColumnReference: there is no unique or exclusion constraint matching the ON CONFLICT specification


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.InvalidColumnReference) there is no unique or exclusion constraint matching the ON CONFLICT specification

[SQL: INSERT INTO final.fct_order (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_status,
    price,
    order_purchase_date,
    order_purchase_time,
    order_approved_at_date,
    order_approved_at_time,
    order_delivered_carrier_date,
    order_delivered_carrier_time,
    order_delivered_customer_date,
    order_delivered_customer_time,
    order_estimated_delivery_date,
    order_estimated_delivery_time,
    shipping_limit_date,
    shipping_limit_time
)

SELECT 
	so.order_id::uuid,
	dc.customer_id,
    so.order_status,
    oi.price,
    dd1.date_id AS order_purchase_date,
    dt1.time_id AS order_purchase_time,
    dd2.date_id AS order_approved_at_date,
    dt2.time_id AS order_approved_at_time,
    dd3.date_id AS order_delivered_carrier_date,
    dt3.time_id AS order_delivered_carrier_time,
    dd4.date_id AS order_delivered_customer_date,
    dt4.time_id AS order_delivered_customer_time,
    dd5.date_id AS order_estimated_delivery_date,
    dt5.time_id AS order_estimated_delivery_time,
    dd6.date_id AS shipping_limit_date,
    dt6.time_id AS shipping_limit_time
FROM 
	stg.orders so
JOIN
    stg.order_items oi ON so.order_id = oi.order_id
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN 
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order_purchase_timestamp::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt1 ON dt1.time_actual::time = (so.order_purchase_timestamp)::time
JOIN 
    final.dim_date dd2 ON dd2.date_actual = TO_DATE(so.order_approved_at::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt2 ON dt2.time_actual::time = (so.order_approved_at)::time
JOIN 
    final.dim_date dd3 ON dd3.date_actual = TO_DATE(so.order_delivered_carrier_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt3 ON dt3.time_actual::time = (so.order_delivered_carrier_date)::time
JOIN 
    final.dim_date dd4 ON dd4.date_actual = TO_DATE(so.order_delivered_customer_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt4 ON dt4.time_actual::time = (so.order_delivered_customer_date)::time
JOIN 
    final.dim_date dd5 ON dd5.date_actual = TO_DATE(so.order_estimated_delivery_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt5 ON dt5.time_actual::time = (so.order_estimated_delivery_date)::time
JOIN 
    final.dim_date dd6 ON dd6.date_actual = TO_DATE(oi.shipping_limit_date::text, 'YYYY-MM-DD')
JOIN 
    final.dim_time dt6 ON dt6.time_actual::time = (oi.shipping_limit_date)::time


ON CONFLICT(order_id, customer_id, order_status) 
DO UPDATE SET
    price = EXCLUDED.price,
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_purchase_time = EXCLUDED.order_purchase_time,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_approved_at_time = EXCLUDED.order_approved_at_time,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_carrier_time = EXCLUDED.order_delivered_carrier_time,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_delivered_customer_time = EXCLUDED.order_delivered_customer_time,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    order_estimated_delivery_time = EXCLUDED.order_estimated_delivery_time,
    shipping_limit_date = EXCLUDED.shipping_limit_date,
    shipping_limit_time = EXCLUDED.shipping_limit_time,
    updated_at = CASE WHEN 
                        final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order.order_purchase_time <> EXCLUDED.order_purchase_time
                        OR final.fct_order.order_approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order.order_approved_at_time <> EXCLUDED.order_approved_at_time
                        OR final.fct_order.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order.order_delivered_carrier_time <> EXCLUDED.order_delivered_carrier_time
                        OR final.fct_order.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order.order_delivered_customer_time <> EXCLUDED.order_delivered_customer_time
                        OR final.fct_order.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                        OR final.fct_order.order_estimated_delivery_time <> EXCLUDED.order_estimated_delivery_time
                        OR final.fct_order.shipping_limit_date <> EXCLUDED.shipping_limit_date
                        OR final.fct_order.shipping_limit_time <> EXCLUDED.shipping_limit_time
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 23:54:18,377 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:54:18,382 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 23:54:18,382 - DEBUG - Asking scheduler for work...
2024-12-21 23:54:18,384 - DEBUG - Done
2024-12-21 23:54:18,384 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:54:18,384 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 23:54:18,384 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 23:54:18,384 - INFO - Worker Worker(salt=5540185103, workers=1, host=MSI, username=alfayyedh, pid=105813) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:54:18,385 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:55:51,093 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-21 23:55:51,140 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-21 23:55:51,450 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-21 23:55:51,650 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-21 23:55:51,683 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-21 23:55:53,461 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-21 23:55:54,545 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-21 23:55:55,064 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-21 23:55:56,162 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-21 23:55:57,739 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-21 23:55:57,739 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-21 23:55:57,741 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-21 23:55:57,779 - INFO - [pid 106795] Worker Worker(salt=6893078808, workers=1, host=MSI, username=alfayyedh, pid=106795) done      Extract()
2024-12-21 23:55:57,781 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:55:57,784 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-21 23:55:57,784 - DEBUG - Asking scheduler for work...
2024-12-21 23:55:57,786 - DEBUG - Pending tasks: 2
2024-12-21 23:55:57,786 - INFO - [pid 106795] Worker Worker(salt=6893078808, workers=1, host=MSI, username=alfayyedh, pid=106795) running   Load()
2024-12-21 23:55:57,787 - INFO - Read Load Query - SUCCESS
2024-12-21 23:55:59,391 - INFO - Read Extracted Data - SUCCESS
2024-12-21 23:55:59,392 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:55:59,736 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-21 23:55:59,736 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-21 23:55:59,755 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-21 23:56:01,749 - INFO - LOAD 'public.products' - SUCCESS
2024-12-21 23:56:02,169 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-21 23:56:02,245 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-21 23:56:03,371 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-21 23:56:03,409 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-21 23:56:03,415 - ERROR - [pid 106795] Worker Worker(salt=6893078808, workers=1, host=MSI, username=alfayyedh, pid=106795) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (review_id)=(3242cc306a9218d0377831e175d62fbf) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (review_id)=(3242cc306a9218d0377831e175d62fbf) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_comment_message__0': None, 'review_id__0': '74b2b4f91de0e6e32992cb4c90e84e5a', 'order_id__0': 'b94cb06da9bf9f6f9b4324b294666f4d', 'review_creation_date__0': '2018-02-09 10:19:32', 'review_score__0': 5, 'review_comment_title__0': None, 'review_comment_message__1': None, 'review_id__1': '1be2c78f16566eab262a8aa39a6d73c4', 'order_id__1': 'ca68bb718c53d348a0428fa415efae79', 'review_creation_date__1': '2017-12-12 16:44:23', 'review_score__1': 4, 'review_comment_title__1': None, 'review_comment_message__2': 'Boa qualidade, chegou no tempo previsto, tudo certinho...', 'review_id__2': '8292b3112397241abd44d49c2fd59d98', 'order_id__2': 'f15aaf3f7b6fbea7b5a454a4f9a38170', 'review_creation_date__2': '2018-07-26 01:35:30', 'review_score__2': 4, 'review_comment_title__2': 'Super recomendo', 'review_comment_message__3': None, 'review_id__3': '6e0686abc9dca38e61f6dc6e304b3df1', 'order_id__3': '369eee143e91ca7a46cdddc5f679e1f3', 'review_creation_date__3': '2017-08-11 03:22:16', 'review_score__3': 1, 'review_comment_title__3': None, 'review_comment_message__4': None, 'review_id__4': 'd2485d102ef7babcce7efc320bb8fbcb', 'order_id__4': '43bc72f5380889da97627ba5ac7806be', 'review_creation_date__4': '2018-05-26 04:19:27', 'review_score__4': 5, 'review_comment_title__4': None, 'review_comment_message__5': None, 'review_id__5': 'a83018e0964b3a60c963040a094537bf', 'order_id__5': '560da5b17e4bf4f4a9eb6794a8d71eeb', 'review_creation_date__5': '2018-04-12 22:55:21', 'review_score__5': 5, 'review_comment_title__5': None, 'review_comment_message__6': None, 'review_id__6': 'bffba33345820ccfaf1f783a1d577ab1', 'order_id__6': '654c5ae02ab59fb154d128fc5d5418e4', 'review_creation_date__6': '2018-07-16 12:08:53', 'review_score__6': 5, 'review_comment_title__6': None, 'review_comment_message__7': 'Adoro comprar nas lojas lannister.\r\n\r\nOs produtos são de ótima qualidade.\r\n', 'review_id__7': 'c8a7db4cda86160a9be40b85f728692b', 'order_id__7': '6951ff62555bae27cada94716835c9fa', 'review_creation_date__7': '2018-04-16 20:31:30', 'review_score__7': 5, 'review_comment_title__7': None, 'review_comment_message__8': 'pedi duas unidades do mesmo produto e só veio uma unidade e ninguém me da um retorno', 'review_id__8': '4447b3ada801a6efd7595076585aa40a' ... 5900 parameters truncated ... 'review_score__991': 5, 'review_comment_title__991': None, 'review_comment_message__992': 'otimo', 'review_id__992': 'd89a3d0bceedafcf82d211ced0c55aa2', 'order_id__992': '5849d6026e6ef0340a5e699086dbfbc7', 'review_creation_date__992': '2017-05-31 00:47:24', 'review_score__992': 5, 'review_comment_title__992': None, 'review_comment_message__993': 'Parabéns ! Excelente produto.', 'review_id__993': 'bb0671cb34e263ae3f9aae0c147e9296', 'order_id__993': '0dc561ba424a8d639e1d4d7840c530c0', 'review_creation_date__993': '2017-11-17 14:20:30', 'review_score__993': 5, 'review_comment_title__993': None, 'review_comment_message__994': None, 'review_id__994': '49193248e63bd80256d68e9ba1e5011e', 'order_id__994': '13f33384dcc1bd99cabcc96a491fe5e3', 'review_creation_date__994': '2018-04-09 20:26:53', 'review_score__994': 5, 'review_comment_title__994': None, 'review_comment_message__995': 'Bom', 'review_id__995': '2151dce1a3b0e1ea35e8b40d77adfbe0', 'order_id__995': '42053bb062d0d5bb79db26888b6f50aa', 'review_creation_date__995': '2017-12-07 09:49:41', 'review_score__995': 5, 'review_comment_title__995': None, 'review_comment_message__996': 'Não recebi o produto', 'review_id__996': '85a3e50c678d1d8aac6021c039d5ad13', 'order_id__996': 'd23cd4e593b64b8b4d86ac59181f1c75', 'review_creation_date__996': '2018-08-25 13:05:34', 'review_score__996': 1, 'review_comment_title__996': 'Pedido não entregue ', 'review_comment_message__997': None, 'review_id__997': '5c1b51a85488f5170d78d58e593d6531', 'order_id__997': '10b69324b897d3c3504084db8492f98a', 'review_creation_date__997': '2018-02-16 21:02:17', 'review_score__997': 5, 'review_comment_title__997': None, 'review_comment_message__998': None, 'review_id__998': '3503b935dc5bb2856adfb6e3a44a8297', 'order_id__998': 'babf4b3af8749669fef673abd352fab9', 'review_creation_date__998': '2018-04-05 13:22:02', 'review_score__998': 5, 'review_comment_title__998': None, 'review_comment_message__999': None, 'review_id__999': 'e576e669a763d89bbba7941f243c519b', 'order_id__999': 'b9fd88cc9e5734657e5b5936d5227367', 'review_creation_date__999': '2018-07-27 14:55:38', 'review_score__999': 5, 'review_comment_title__999': None}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-21 23:56:03,521 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:56:03,529 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-21 23:56:03,529 - DEBUG - Asking scheduler for work...
2024-12-21 23:56:03,531 - DEBUG - Done
2024-12-21 23:56:03,531 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:56:03,531 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-21 23:56:03,531 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-21 23:56:03,531 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-21 23:56:03,531 - INFO - Worker Worker(salt=6893078808, workers=1, host=MSI, username=alfayyedh, pid=106795) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:56:03,532 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-21 23:56:31,812 - INFO - Read Transform Query - SUCCESS
2024-12-21 23:56:31,852 - INFO - Connect to DWH - SUCCESS
2024-12-21 23:56:31,853 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-21 23:56:31,880 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-21 23:56:31,914 - ERROR - Transform Tables - FAILED
2024-12-21 23:56:31,916 - ERROR - [pid 107237] Worker Worker(salt=8336348812, workers=1, host=MSI, username=alfayyedh, pid=107237) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "stg.geolocation" does not exist
LINE 18:     stg.geolocation g
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 93, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "stg.geolocation" does not exist
LINE 18:     stg.geolocation g
             ^

[SQL: INSERT INTO final.dim_geolocation (
    geolocation_id,
    geolocation_zip_code_prefix,
    geolocation_lat,
    geolocation_lng,
    geolocation_city,
    geolocation_state
)

SELECT
    g.id as geolocation_id,
    g.geolocation_zip_code_prefix,
    g.geolocation_lat,
    g.geolocation_lng,
    g.geolocation_city,
    g.geolocation_state
FROM 
    stg.geolocation g

ON CONFLICT(geolocation_id)
DO UPDATE SET 
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix, 
    geolocation_lat = EXCLUDED.geolocation_lat,
    geolocation_lng = EXCLUDED.geolocation_lng,
    geolocation_city = EXCLUDED.geolocation_city,
    geolocation_state = EXCLUDED.geolocation_state,
    updated_at = CASE WHEN
                        final.dim_geolocation.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                        OR final.dim_geolocation.geolocation_lat <> EXCLUDED.geolocation_lat
                        OR final.dim_geolocation.geolocation_lng <> EXCLUDED.geolocation_lng
                        OR final.dim_geolocation.geolocation_city <> EXCLUDED.geolocation_city
                        OR final.dim_geolocation.geolocation_state <> EXCLUDED.geolocation_state
                THEN
                        CURRENT_TIMESTAMP
                ELSE
                        final.dim_geolocation.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-21 23:56:31,936 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-21 23:56:31,941 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-21 23:56:31,942 - DEBUG - Asking scheduler for work...
2024-12-21 23:56:31,945 - DEBUG - Done
2024-12-21 23:56:31,945 - DEBUG - There are no more tasks to run at this time
2024-12-21 23:56:31,945 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-21 23:56:31,945 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-21 23:56:31,945 - INFO - Worker Worker(salt=8336348812, workers=1, host=MSI, username=alfayyedh, pid=107237) was stopped. Shutting down Keep-Alive thread
2024-12-21 23:56:31,946 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 00:00:02,090 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 00:00:02,117 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-22 00:00:02,419 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-22 00:00:02,543 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-22 00:00:02,559 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-22 00:00:03,273 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-22 00:00:03,952 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-22 00:00:04,422 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-22 00:00:05,349 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-22 00:00:06,367 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-22 00:00:06,367 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-22 00:00:06,369 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-22 00:00:06,401 - INFO - [pid 108843] Worker Worker(salt=103417168, workers=1, host=MSI, username=alfayyedh, pid=108843) done      Extract()
2024-12-22 00:00:06,401 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 00:00:06,404 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-22 00:00:06,404 - DEBUG - Asking scheduler for work...
2024-12-22 00:00:06,406 - DEBUG - Pending tasks: 2
2024-12-22 00:00:06,406 - INFO - [pid 108843] Worker Worker(salt=103417168, workers=1, host=MSI, username=alfayyedh, pid=108843) running   Load()
2024-12-22 00:00:06,406 - INFO - Read Load Query - SUCCESS
2024-12-22 00:00:07,655 - INFO - Read Extracted Data - SUCCESS
2024-12-22 00:00:07,656 - INFO - Connect to DWH - SUCCESS
2024-12-22 00:00:07,848 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-22 00:00:07,848 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-22 00:00:07,868 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-22 00:00:10,001 - INFO - LOAD 'public.products' - SUCCESS
2024-12-22 00:00:10,534 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-22 00:00:10,635 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-22 00:00:11,847 - ERROR - LOAD All Tables To DWH-Olist - FAILED
2024-12-22 00:00:11,890 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-22 00:00:11,897 - ERROR - [pid 108843] Worker Worker(salt=103417168, workers=1, host=MSI, username=alfayyedh, pid=108843) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (review_id)=(3242cc306a9218d0377831e175d62fbf) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 206, in run
    order_reviews.to_sql('order_reviews',
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/core/generic.py", line 3084, in to_sql
    return sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 842, in to_sql
    return pandas_sql.to_sql(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 2018, in to_sql
    total_inserted = sql_engine.insert_records(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1567, in insert_records
    raise err
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1558, in insert_records
    return table.insert(chunksize=chunksize, method=method)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1119, in insert
    num_inserted = exec_insert(conn, keys, chunk_iter)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1010, in _execute_insert
    result = conn.execute(self.table.insert(), data)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1847, in _execute_context
    return self._exec_insertmany_context(dialect, context)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2127, in _exec_insertmany_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2119, in _exec_insertmany_context
    dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "order_reviews_pkey"
DETAIL:  Key (review_id)=(3242cc306a9218d0377831e175d62fbf) already exists.

[SQL: INSERT INTO public.order_reviews (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date) VALUES (%(review_id__0)s, %(order_id__0)s, %(review_score__0)s, %(review_comment_title__0)s, %(review_comment_mes ... 158132 characters truncated ... 99)s, %(review_comment_title__999)s, %(review_comment_message__999)s, %(review_creation_date__999)s)]
[parameters: {'review_id__0': '74b2b4f91de0e6e32992cb4c90e84e5a', 'review_score__0': 5, 'review_creation_date__0': '2018-02-09 10:19:32', 'review_comment_title__0': None, 'order_id__0': 'b94cb06da9bf9f6f9b4324b294666f4d', 'review_comment_message__0': None, 'review_id__1': '1be2c78f16566eab262a8aa39a6d73c4', 'review_score__1': 4, 'review_creation_date__1': '2017-12-12 16:44:23', 'review_comment_title__1': None, 'order_id__1': 'ca68bb718c53d348a0428fa415efae79', 'review_comment_message__1': None, 'review_id__2': '8292b3112397241abd44d49c2fd59d98', 'review_score__2': 4, 'review_creation_date__2': '2018-07-26 01:35:30', 'review_comment_title__2': 'Super recomendo', 'order_id__2': 'f15aaf3f7b6fbea7b5a454a4f9a38170', 'review_comment_message__2': 'Boa qualidade, chegou no tempo previsto, tudo certinho...', 'review_id__3': '6e0686abc9dca38e61f6dc6e304b3df1', 'review_score__3': 1, 'review_creation_date__3': '2017-08-11 03:22:16', 'review_comment_title__3': None, 'order_id__3': '369eee143e91ca7a46cdddc5f679e1f3', 'review_comment_message__3': None, 'review_id__4': 'd2485d102ef7babcce7efc320bb8fbcb', 'review_score__4': 5, 'review_creation_date__4': '2018-05-26 04:19:27', 'review_comment_title__4': None, 'order_id__4': '43bc72f5380889da97627ba5ac7806be', 'review_comment_message__4': None, 'review_id__5': 'a83018e0964b3a60c963040a094537bf', 'review_score__5': 5, 'review_creation_date__5': '2018-04-12 22:55:21', 'review_comment_title__5': None, 'order_id__5': '560da5b17e4bf4f4a9eb6794a8d71eeb', 'review_comment_message__5': None, 'review_id__6': 'bffba33345820ccfaf1f783a1d577ab1', 'review_score__6': 5, 'review_creation_date__6': '2018-07-16 12:08:53', 'review_comment_title__6': None, 'order_id__6': '654c5ae02ab59fb154d128fc5d5418e4', 'review_comment_message__6': None, 'review_id__7': 'c8a7db4cda86160a9be40b85f728692b', 'review_score__7': 5, 'review_creation_date__7': '2018-04-16 20:31:30', 'review_comment_title__7': None, 'order_id__7': '6951ff62555bae27cada94716835c9fa', 'review_comment_message__7': 'Adoro comprar nas lojas lannister.\r\n\r\nOs produtos são de ótima qualidade.\r\n', 'review_id__8': '4447b3ada801a6efd7595076585aa40a', 'review_score__8': 2 ... 5900 parameters truncated ... 'order_id__991': '7068f14af6c3c575490f74b6aaf2f05c', 'review_comment_message__991': None, 'review_id__992': 'd89a3d0bceedafcf82d211ced0c55aa2', 'review_score__992': 5, 'review_creation_date__992': '2017-05-31 00:47:24', 'review_comment_title__992': None, 'order_id__992': '5849d6026e6ef0340a5e699086dbfbc7', 'review_comment_message__992': 'otimo', 'review_id__993': 'bb0671cb34e263ae3f9aae0c147e9296', 'review_score__993': 5, 'review_creation_date__993': '2017-11-17 14:20:30', 'review_comment_title__993': None, 'order_id__993': '0dc561ba424a8d639e1d4d7840c530c0', 'review_comment_message__993': 'Parabéns ! Excelente produto.', 'review_id__994': '49193248e63bd80256d68e9ba1e5011e', 'review_score__994': 5, 'review_creation_date__994': '2018-04-09 20:26:53', 'review_comment_title__994': None, 'order_id__994': '13f33384dcc1bd99cabcc96a491fe5e3', 'review_comment_message__994': None, 'review_id__995': '2151dce1a3b0e1ea35e8b40d77adfbe0', 'review_score__995': 5, 'review_creation_date__995': '2017-12-07 09:49:41', 'review_comment_title__995': None, 'order_id__995': '42053bb062d0d5bb79db26888b6f50aa', 'review_comment_message__995': 'Bom', 'review_id__996': '85a3e50c678d1d8aac6021c039d5ad13', 'review_score__996': 1, 'review_creation_date__996': '2018-08-25 13:05:34', 'review_comment_title__996': 'Pedido não entregue ', 'order_id__996': 'd23cd4e593b64b8b4d86ac59181f1c75', 'review_comment_message__996': 'Não recebi o produto', 'review_id__997': '5c1b51a85488f5170d78d58e593d6531', 'review_score__997': 5, 'review_creation_date__997': '2018-02-16 21:02:17', 'review_comment_title__997': None, 'order_id__997': '10b69324b897d3c3504084db8492f98a', 'review_comment_message__997': None, 'review_id__998': '3503b935dc5bb2856adfb6e3a44a8297', 'review_score__998': 5, 'review_creation_date__998': '2018-04-05 13:22:02', 'review_comment_title__998': None, 'order_id__998': 'babf4b3af8749669fef673abd352fab9', 'review_comment_message__998': None, 'review_id__999': 'e576e669a763d89bbba7941f243c519b', 'review_score__999': 5, 'review_creation_date__999': '2018-07-27 14:55:38', 'review_comment_title__999': None, 'order_id__999': 'b9fd88cc9e5734657e5b5936d5227367', 'review_comment_message__999': None}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 253, in run
    raise Exception('Failed Load Tables To DWH-Olist')
Exception: Failed Load Tables To DWH-Olist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-22 00:00:12,019 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 00:00:12,025 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-22 00:00:12,025 - DEBUG - Asking scheduler for work...
2024-12-22 00:00:12,027 - DEBUG - Done
2024-12-22 00:00:12,027 - DEBUG - There are no more tasks to run at this time
2024-12-22 00:00:12,027 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-22 00:00:12,027 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-22 00:00:12,027 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-22 00:00:12,027 - INFO - Worker Worker(salt=103417168, workers=1, host=MSI, username=alfayyedh, pid=108843) was stopped. Shutting down Keep-Alive thread
2024-12-22 00:00:12,028 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 00:54:37,145 - INFO - Read Transform Query - SUCCESS
2024-12-22 00:54:37,204 - INFO - Connect to DWH - SUCCESS
2024-12-22 00:54:37,205 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-22 00:54:37,233 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-22 00:54:37,235 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-22 00:54:37,237 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-22 00:54:37,248 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-22 00:54:37,250 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-22 00:54:37,296 - ERROR - Transform Tables - FAILED
2024-12-22 00:54:37,300 - ERROR - [pid 133186] Worker Worker(salt=5649934141, workers=1, host=MSI, username=alfayyedh, pid=133186) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: syntax error at or near ","
LINE 10:     review_creation_date,,
                                  ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near ","
LINE 10:     review_creation_date,,
                                  ^

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,,
    payment_value    
)

SELECT 
	so.order_id,
    dc.customer_id,,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_actual AS review_creation_date,
    sop.payment_value,,
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_id
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = DATE(sor.review_creation_date)
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_id = so.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-22 00:54:37,335 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 00:54:37,342 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-22 00:54:37,343 - DEBUG - Asking scheduler for work...
2024-12-22 00:54:37,345 - DEBUG - Done
2024-12-22 00:54:37,345 - DEBUG - There are no more tasks to run at this time
2024-12-22 00:54:37,345 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-22 00:54:37,345 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-22 00:54:37,345 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-22 00:54:37,346 - INFO - Worker Worker(salt=5649934141, workers=1, host=MSI, username=alfayyedh, pid=133186) was stopped. Shutting down Keep-Alive thread
2024-12-22 00:54:37,347 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 01:04:23,517 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 01:04:23,588 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-22 01:04:23,947 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-22 01:04:24,071 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-22 01:04:24,088 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-22 01:04:25,124 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-22 01:04:26,950 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-22 01:04:28,034 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-22 01:04:28,818 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-22 01:04:31,212 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-22 01:04:31,212 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-22 01:04:31,216 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-22 01:04:31,235 - INFO - [pid 137328] Worker Worker(salt=6209475613, workers=1, host=MSI, username=alfayyedh, pid=137328) done      Extract()
2024-12-22 01:04:31,236 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:04:31,239 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-22 01:04:31,239 - DEBUG - Asking scheduler for work...
2024-12-22 01:04:31,240 - DEBUG - Pending tasks: 2
2024-12-22 01:04:31,241 - INFO - [pid 137328] Worker Worker(salt=6209475613, workers=1, host=MSI, username=alfayyedh, pid=137328) running   Load()
2024-12-22 01:04:31,242 - INFO - Read Load Query - SUCCESS
2024-12-22 01:04:33,572 - INFO - Read Extracted Data - SUCCESS
2024-12-22 01:04:33,574 - INFO - Connect to DWH - SUCCESS
2024-12-22 01:04:33,851 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-22 01:04:33,852 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-22 01:04:33,882 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-22 01:04:36,477 - INFO - LOAD 'public.products' - SUCCESS
2024-12-22 01:04:36,887 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-22 01:04:36,975 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-22 01:04:40,568 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-22 01:04:45,750 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-22 01:04:50,355 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-22 01:04:54,485 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-22 01:05:01,955 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-22 01:05:01,955 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-22 01:05:01,968 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-22 01:05:02,016 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-22 01:05:02,023 - ERROR - [pid 137328] Worker Worker(salt=6209475613, workers=1, host=MSI, username=alfayyedh, pid=137328) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "src.product_category_name_translation" does not exist
LINE 8:     src.product_category_name_translation
            ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "src.product_category_name_translation" does not exist
LINE 8:     src.product_category_name_translation
            ^

[SQL: INSERT INTO stg.product_category_name_translation
    (product_category_name, product_category_name_english) 

SELECT
    product_category_name, 
    product_category_name_english 
FROM
    src.product_category_name_translation

ON CONFLICT(product_category_name) 
DO UPDATE SET
    product_category_name_english = EXCLUDED.product_category_name_english,
    updated_at = CASE WHEN 
                        stg.product_category_name_translation.product_category_name_english <> EXCLUDED.product_category_name_english
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.product_category_name_translation.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-22 01:05:02,050 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:05:02,061 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-22 01:05:02,061 - DEBUG - Asking scheduler for work...
2024-12-22 01:05:02,064 - DEBUG - Done
2024-12-22 01:05:02,064 - DEBUG - There are no more tasks to run at this time
2024-12-22 01:05:02,064 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-22 01:05:02,064 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-22 01:05:02,064 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-22 01:05:02,065 - INFO - Worker Worker(salt=6209475613, workers=1, host=MSI, username=alfayyedh, pid=137328) was stopped. Shutting down Keep-Alive thread
2024-12-22 01:05:02,066 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 01:09:14,366 - INFO - Read Transform Query - SUCCESS
2024-12-22 01:09:14,418 - INFO - Connect to DWH - SUCCESS
2024-12-22 01:09:14,418 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-22 01:09:14,458 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-22 01:09:14,462 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-22 01:09:14,464 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-22 01:09:14,494 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-22 01:09:14,496 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-22 01:09:14,528 - ERROR - Transform Tables - FAILED
2024-12-22 01:09:14,529 - ERROR - [pid 139646] Worker Worker(salt=6105879179, workers=1, host=MSI, username=alfayyedh, pid=139646) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: syntax error at or near ","
LINE 10:     review_creation_date,,
                                  ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near ","
LINE 10:     review_creation_date,,
                                  ^

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,,
    payment_value    
)

SELECT 
	so.order_id,
    dc.customer_id,,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_actual AS review_creation_date,
    sop.payment_value,,
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_id
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = DATE(sor.review_creation_date)
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_id = so.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-22 01:09:14,545 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:09:14,552 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-22 01:09:14,552 - DEBUG - Asking scheduler for work...
2024-12-22 01:09:14,554 - DEBUG - Done
2024-12-22 01:09:14,554 - DEBUG - There are no more tasks to run at this time
2024-12-22 01:09:14,554 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-22 01:09:14,554 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-22 01:09:14,554 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-22 01:09:14,555 - INFO - Worker Worker(salt=6105879179, workers=1, host=MSI, username=alfayyedh, pid=139646) was stopped. Shutting down Keep-Alive thread
2024-12-22 01:09:14,556 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 01:10:39,041 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 01:10:39,090 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-22 01:10:39,335 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-22 01:10:39,439 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-22 01:10:39,453 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-22 01:10:39,974 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-22 01:10:40,917 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-22 01:10:41,648 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-22 01:10:42,161 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-22 01:10:43,035 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-22 01:10:43,035 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-22 01:10:43,037 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-22 01:10:43,054 - INFO - [pid 140312] Worker Worker(salt=1760740786, workers=1, host=MSI, username=alfayyedh, pid=140312) done      Extract()
2024-12-22 01:10:43,055 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:10:43,058 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-22 01:10:43,058 - DEBUG - Asking scheduler for work...
2024-12-22 01:10:43,059 - DEBUG - Pending tasks: 2
2024-12-22 01:10:43,059 - INFO - [pid 140312] Worker Worker(salt=1760740786, workers=1, host=MSI, username=alfayyedh, pid=140312) running   Load()
2024-12-22 01:10:43,060 - INFO - Read Load Query - SUCCESS
2024-12-22 01:10:44,303 - INFO - Read Extracted Data - SUCCESS
2024-12-22 01:10:44,304 - INFO - Connect to DWH - SUCCESS
2024-12-22 01:10:44,562 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-22 01:10:44,562 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-22 01:10:44,576 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-22 01:10:46,367 - INFO - LOAD 'public.products' - SUCCESS
2024-12-22 01:10:46,826 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-22 01:10:46,919 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-22 01:10:50,522 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-22 01:10:56,728 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-22 01:11:00,786 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-22 01:11:04,472 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-22 01:11:12,024 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-22 01:11:12,024 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-22 01:11:12,045 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-22 01:11:12,083 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-22 01:11:12,089 - ERROR - [pid 140312] Worker Worker(salt=1760740786, workers=1, host=MSI, username=alfayyedh, pid=140312) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: missing FROM-clause entry for table "product"
LINE 28:                         stg.product.product_category_name <>...
                                 ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) missing FROM-clause entry for table "product"
LINE 28:                         stg.product.product_category_name <>...
                                 ^

[SQL: INSERT INTO stg.products 
    (product_id, product_category_name, product_name_lenght, product_description_lenght, product_photos_qty, product_weight_g, product_length_cm, product_height_cm, product_width_cm) 

SELECT
    product_id, 
    product_category_name, 
    product_name_lenght, 
    product_description_lenght, 
    product_photos_qty, 
    product_weight_g, 
    product_length_cm, 
    product_height_cm, 
    product_width_cm
FROM
    public.products

ON CONFLICT(product_id) 
DO UPDATE SET
    product_category_name = EXCLUDED.product_category_name,
    product_name_lenght = EXCLUDED.product_name_lenght,
    product_description_lenght = EXCLUDED.product_description_lenght,
    product_photos_qty = EXCLUDED.product_photos_qty,
    product_weight_g = EXCLUDED.product_weight_g,
    product_length_cm = EXCLUDED.product_length_cm,
    product_height_cm = EXCLUDED.product_height_cm,
    product_width_cm = EXCLUDED.product_width_cm,
    updated_at = CASE WHEN 
                        stg.product.product_category_name <> EXCLUDED.product_category_name
                        OR stg.product.product_name_lenght <> EXCLUDED.product_name_lenght
                        OR stg.product.product_description_lenght <> EXCLUDED.product_description_lenght
                        OR stg.product.product_photos_qty <> EXCLUDED.product_photos_qty
                        OR stg.product.product_weight_g <> EXCLUDED.product_weight_g
                        OR stg.product.product_length_cm <> EXCLUDED.product_length_cm
                        OR stg.product.product_height_cm <> EXCLUDED.product_height_cm
                        OR stg.product.product_width_cm <> EXCLUDED.product_width_cm
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.product.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-22 01:11:12,111 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:11:12,118 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-22 01:11:12,118 - DEBUG - Asking scheduler for work...
2024-12-22 01:11:12,120 - DEBUG - Done
2024-12-22 01:11:12,120 - DEBUG - There are no more tasks to run at this time
2024-12-22 01:11:12,121 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-22 01:11:12,121 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-22 01:11:12,121 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-22 01:11:12,121 - INFO - Worker Worker(salt=1760740786, workers=1, host=MSI, username=alfayyedh, pid=140312) was stopped. Shutting down Keep-Alive thread
2024-12-22 01:11:12,122 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 01:12:56,209 - INFO - Read Transform Query - SUCCESS
2024-12-22 01:12:56,253 - INFO - Connect to DWH - SUCCESS
2024-12-22 01:12:56,253 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-22 01:12:56,287 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-22 01:12:56,292 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-22 01:12:56,294 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-22 01:12:56,306 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-22 01:12:56,308 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-22 01:12:56,344 - ERROR - Transform Tables - FAILED
2024-12-22 01:12:56,347 - ERROR - [pid 141505] Worker Worker(salt=8494330839, workers=1, host=MSI, username=alfayyedh, pid=141505) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: operator does not exist: text = uuid
LINE 27:  final.dim_customer dc ON so.customer_id = dc.customer_id
                                                  ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) operator does not exist: text = uuid
LINE 27:  final.dim_customer dc ON so.customer_id = dc.customer_id
                                                  ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT 
	so.order_id,
    dc.customer_id,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_actual AS review_creation_date,
    sop.payment_value
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_id
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_id = so.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-22 01:12:56,364 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:12:56,370 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-22 01:12:56,370 - DEBUG - Asking scheduler for work...
2024-12-22 01:12:56,372 - DEBUG - Done
2024-12-22 01:12:56,373 - DEBUG - There are no more tasks to run at this time
2024-12-22 01:12:56,373 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-22 01:12:56,373 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-22 01:12:56,373 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-22 01:12:56,373 - INFO - Worker Worker(salt=8494330839, workers=1, host=MSI, username=alfayyedh, pid=141505) was stopped. Shutting down Keep-Alive thread
2024-12-22 01:12:56,374 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-22 01:16:13,296 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-22 01:16:13,327 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-22 01:16:13,577 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-22 01:16:13,698 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-22 01:16:13,714 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-22 01:16:14,238 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-22 01:16:15,103 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-22 01:16:15,721 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-22 01:16:16,208 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-22 01:16:17,083 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-22 01:16:17,083 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-22 01:16:17,085 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-22 01:16:17,101 - INFO - [pid 142953] Worker Worker(salt=8328653675, workers=1, host=MSI, username=alfayyedh, pid=142953) done      Extract()
2024-12-22 01:16:17,104 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:16:17,106 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-22 01:16:17,106 - DEBUG - Asking scheduler for work...
2024-12-22 01:16:17,108 - DEBUG - Pending tasks: 2
2024-12-22 01:16:17,108 - INFO - [pid 142953] Worker Worker(salt=8328653675, workers=1, host=MSI, username=alfayyedh, pid=142953) running   Load()
2024-12-22 01:16:17,109 - INFO - Read Load Query - SUCCESS
2024-12-22 01:16:18,096 - INFO - Read Extracted Data - SUCCESS
2024-12-22 01:16:18,097 - INFO - Connect to DWH - SUCCESS
2024-12-22 01:16:18,333 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-22 01:16:18,333 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-22 01:16:18,346 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-22 01:16:20,048 - INFO - LOAD 'public.products' - SUCCESS
2024-12-22 01:16:20,447 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-22 01:16:20,526 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-22 01:16:24,047 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-22 01:16:28,454 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-22 01:16:32,436 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-22 01:16:36,589 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-22 01:16:43,818 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-22 01:16:43,818 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-22 01:16:47,031 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-22 01:16:47,067 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-22 01:16:47,072 - ERROR - [pid 142953] Worker Worker(salt=8328653675, workers=1, host=MSI, username=alfayyedh, pid=142953) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: syntax error at or near ","
LINE 31: ...ed_customer_date <> EXCLUDED.order_delivered_customer_date, 
                                                                      ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near ","
LINE 31: ...ed_customer_date <> EXCLUDED.order_delivered_customer_date, 
                                                                      ^

[SQL: INSERT INTO stg.orders 
    (order_id, customer_id, order_status, order_purchase_timestamp, order_approved_at, order_approved_at, order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date) 

SELECT
    order_id, 
    customer_id, 
    order_status, 
    order_purchase_timestamp, 
    order_approved_at, 
    order_delivered_carrier_date, 
    order_delivered_customer_date, 
    order_estimated_delivery_date
FROM
    public.orders

ON CONFLICT(order_id) 
DO UPDATE SET
    customer_id = EXCLUDED.customer_id, 
    order_status = EXCLUDED.order_status, 
    order_purchase_timestamp = EXCLUDED.order_purchase_timestamp, 
    order_approved_at = EXCLUDED.order_approved_at, 
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date, 
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date, 
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    updated_at = CASE WHEN 
                        stg.orders.customer_id <> EXCLUDED.customer_id 
                        OR stg.orders.order_status <> EXCLUDED.order_status 
                        OR stg.orders.order_purchase_timestamp <> EXCLUDED.order_purchase_timestamp 
                        OR stg.orders.order_approved_at <> EXCLUDED.order_approved_at
                        OR stg.orders.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date 
                        OR stg.orders.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date, 
                        OR stg.orders.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.orders.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-22 01:16:47,091 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-22 01:16:47,096 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-22 01:16:47,096 - DEBUG - Asking scheduler for work...
2024-12-22 01:16:47,099 - DEBUG - Done
2024-12-22 01:16:47,099 - DEBUG - There are no more tasks to run at this time
2024-12-22 01:16:47,099 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-22 01:16:47,099 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-22 01:16:47,100 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-22 01:16:47,100 - INFO - Worker Worker(salt=8328653675, workers=1, host=MSI, username=alfayyedh, pid=142953) was stopped. Shutting down Keep-Alive thread
2024-12-22 01:16:47,101 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 16:42:12,205 - INFO - Read Transform Query - SUCCESS
2024-12-25 16:42:12,325 - INFO - Connect to DWH - SUCCESS
2024-12-25 16:42:12,325 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 16:42:12,375 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 16:42:12,380 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 16:42:12,385 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 16:42:12,421 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 16:42:12,424 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 16:42:12,464 - ERROR - Transform Tables - FAILED
2024-12-25 16:42:12,466 - ERROR - [pid 4220] Worker Worker(salt=7728747308, workers=1, host=MSI, username=alfayyedh, pid=4220) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column so.product_id does not exist
LINE 35:     final.dim_product dp ON dp.product_id = so.product_id
                                                     ^
HINT:  Perhaps you meant to reference the column "dp.product_id".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column so.product_id does not exist
LINE 35:     final.dim_product dp ON dp.product_id = so.product_id
                                                     ^
HINT:  Perhaps you meant to reference the column "dp.product_id".

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT 
	so.order_id,
    dc.customer_id,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_actual AS review_creation_date,
    sop.payment_value
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_id = so.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 16:42:12,510 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 16:42:12,516 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 16:42:12,516 - DEBUG - Asking scheduler for work...
2024-12-25 16:42:12,520 - DEBUG - Done
2024-12-25 16:42:12,521 - DEBUG - There are no more tasks to run at this time
2024-12-25 16:42:12,521 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 16:42:12,521 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 16:42:12,521 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 16:42:12,522 - INFO - Worker Worker(salt=7728747308, workers=1, host=MSI, username=alfayyedh, pid=4220) was stopped. Shutting down Keep-Alive thread
2024-12-25 16:42:12,523 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 16:52:07,494 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 16:52:07,632 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 16:52:08,228 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 16:52:08,366 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 16:52:08,386 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 16:52:09,249 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 16:52:10,752 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 16:52:11,596 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 16:52:12,213 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 16:52:13,347 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 16:52:13,347 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 16:52:13,351 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 16:52:13,369 - INFO - [pid 8554] Worker Worker(salt=4591094997, workers=1, host=MSI, username=alfayyedh, pid=8554) done      Extract()
2024-12-25 16:52:13,371 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 16:52:13,375 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 16:52:13,375 - DEBUG - Asking scheduler for work...
2024-12-25 16:52:13,376 - DEBUG - Pending tasks: 2
2024-12-25 16:52:13,377 - INFO - [pid 8554] Worker Worker(salt=4591094997, workers=1, host=MSI, username=alfayyedh, pid=8554) running   Load()
2024-12-25 16:52:13,382 - INFO - Read Load Query - SUCCESS
2024-12-25 16:52:14,845 - INFO - Read Extracted Data - SUCCESS
2024-12-25 16:52:14,847 - INFO - Connect to DWH - SUCCESS
2024-12-25 16:52:15,101 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 16:52:15,101 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 16:52:15,123 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 16:52:18,224 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 16:52:18,648 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 16:52:18,725 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 16:52:21,932 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 16:52:26,657 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 16:52:31,823 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 16:52:35,969 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 16:52:42,823 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 16:52:42,824 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 16:52:46,145 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-25 16:52:46,184 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-25 16:52:46,190 - ERROR - [pid 8554] Worker Worker(salt=4591094997, workers=1, host=MSI, username=alfayyedh, pid=8554) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DuplicateColumn: column "order_approved_at" specified more than once
LINE 2: ...tus, order_purchase_timestamp, order_approved_at, order_appr...
                                                             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DuplicateColumn) column "order_approved_at" specified more than once
LINE 2: ...tus, order_purchase_timestamp, order_approved_at, order_appr...
                                                             ^

[SQL: INSERT INTO stg.orders 
    (order_id, customer_id, order_status, order_purchase_timestamp, order_approved_at, order_approved_at, order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date) 

SELECT
    order_id, 
    customer_id, 
    order_status, 
    order_purchase_timestamp, 
    order_approved_at, 
    order_delivered_carrier_date, 
    order_delivered_customer_date, 
    order_estimated_delivery_date
FROM
    public.orders

ON CONFLICT(order_id) 
DO UPDATE SET
    customer_id = EXCLUDED.customer_id, 
    order_status = EXCLUDED.order_status, 
    order_purchase_timestamp = EXCLUDED.order_purchase_timestamp, 
    order_approved_at = EXCLUDED.order_approved_at, 
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date, 
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date, 
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    updated_at = CASE WHEN 
                        stg.orders.customer_id <> EXCLUDED.customer_id 
                        OR stg.orders.order_status <> EXCLUDED.order_status 
                        OR stg.orders.order_purchase_timestamp <> EXCLUDED.order_purchase_timestamp 
                        OR stg.orders.order_approved_at <> EXCLUDED.order_approved_at
                        OR stg.orders.order_delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date 
                        OR stg.orders.order_delivered_customer_date <> EXCLUDED.order_delivered_customer_date 
                        OR stg.orders.order_estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.orders.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-25 16:52:46,219 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 16:52:46,228 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-25 16:52:46,229 - DEBUG - Asking scheduler for work...
2024-12-25 16:52:46,231 - DEBUG - Done
2024-12-25 16:52:46,232 - DEBUG - There are no more tasks to run at this time
2024-12-25 16:52:46,232 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-25 16:52:46,232 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-25 16:52:46,232 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-25 16:52:46,232 - INFO - Worker Worker(salt=4591094997, workers=1, host=MSI, username=alfayyedh, pid=8554) was stopped. Shutting down Keep-Alive thread
2024-12-25 16:52:46,233 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 16:54:17,869 - INFO - Read Transform Query - SUCCESS
2024-12-25 16:54:17,912 - INFO - Connect to DWH - SUCCESS
2024-12-25 16:54:17,912 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 16:54:17,965 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 16:54:17,970 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 16:54:17,973 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 16:54:18,009 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 16:54:18,011 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 16:54:18,049 - ERROR - Transform Tables - FAILED
2024-12-25 16:54:18,051 - ERROR - [pid 10115] Worker Worker(salt=1157607988, workers=1, host=MSI, username=alfayyedh, pid=10115) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedFunction: operator does not exist: uuid = text
LINE 37:     final.dim_product dp ON dp.product_id = soi.product_id
                                                   ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedFunction) operator does not exist: uuid = text
LINE 37:     final.dim_product dp ON dp.product_id = soi.product_id
                                                   ^
HINT:  No operator matches the given name and argument types. You might need to add explicit type casts.

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT 
	so.order_id,
    dc.customer_id,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_actual AS review_creation_date,
    sop.payment_value
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    stg.order_items soi ON soi.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_id = soi.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 16:54:18,071 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 16:54:18,079 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 16:54:18,079 - DEBUG - Asking scheduler for work...
2024-12-25 16:54:18,082 - DEBUG - Done
2024-12-25 16:54:18,082 - DEBUG - There are no more tasks to run at this time
2024-12-25 16:54:18,082 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 16:54:18,082 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 16:54:18,082 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 16:54:18,083 - INFO - Worker Worker(salt=1157607988, workers=1, host=MSI, username=alfayyedh, pid=10115) was stopped. Shutting down Keep-Alive thread
2024-12-25 16:54:18,084 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 16:54:47,636 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 16:54:47,744 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 16:54:48,060 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 16:54:48,223 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 16:54:48,238 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 16:54:48,809 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 16:54:49,693 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 16:54:50,299 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 16:54:50,835 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 16:54:51,705 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 16:54:51,705 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 16:54:51,707 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 16:54:51,723 - INFO - [pid 10422] Worker Worker(salt=2783894370, workers=1, host=MSI, username=alfayyedh, pid=10422) done      Extract()
2024-12-25 16:54:51,724 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 16:54:51,728 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 16:54:51,728 - DEBUG - Asking scheduler for work...
2024-12-25 16:54:51,730 - DEBUG - Pending tasks: 2
2024-12-25 16:54:51,730 - INFO - [pid 10422] Worker Worker(salt=2783894370, workers=1, host=MSI, username=alfayyedh, pid=10422) running   Load()
2024-12-25 16:54:51,734 - INFO - Read Load Query - SUCCESS
2024-12-25 16:54:52,977 - INFO - Read Extracted Data - SUCCESS
2024-12-25 16:54:52,978 - INFO - Connect to DWH - SUCCESS
2024-12-25 16:54:53,220 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 16:54:53,220 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 16:54:53,237 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 16:54:54,915 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 16:54:55,342 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 16:54:55,444 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 16:54:58,502 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 16:55:02,987 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 16:55:06,898 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 16:55:10,471 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 16:55:17,311 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 16:55:17,312 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 16:55:24,772 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-25 16:55:24,807 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-25 16:55:24,812 - ERROR - [pid 10422] Worker Worker(salt=2783894370, workers=1, host=MSI, username=alfayyedh, pid=10422) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidColumnReference: there is no unique or exclusion constraint matching the ON CONFLICT specification


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.InvalidColumnReference) there is no unique or exclusion constraint matching the ON CONFLICT specification

[SQL: INSERT INTO stg.order_reviews
    (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date)

SELECT 
    review_id,
    order_id,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date
FROM
    public.order_reviews

ON CONFLICT(review_id) 
DO UPDATE SET
    order_id = EXCLUDED.order_id,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    updated_at = CASE WHEN 
                        stg.order_reviews.order_id <> EXCLUDED.order_id
                        OR stg.order_reviews.review_score <> EXCLUDED.review_score
                        OR stg.order_reviews.review_comment_title <> EXCLUDED.review_comment_title
                        OR stg.order_reviews.review_comment_message <> EXCLUDED.review_comment_message
                        OR stg.order_reviews.review_creation_date <> EXCLUDED.review_creation_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.order_reviews.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-25 16:55:24,838 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 16:55:24,845 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-25 16:55:24,845 - DEBUG - Asking scheduler for work...
2024-12-25 16:55:24,848 - DEBUG - Done
2024-12-25 16:55:24,848 - DEBUG - There are no more tasks to run at this time
2024-12-25 16:55:24,848 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-25 16:55:24,849 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 16:55:24,849 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-25 16:55:24,849 - INFO - Worker Worker(salt=2783894370, workers=1, host=MSI, username=alfayyedh, pid=10422) was stopped. Shutting down Keep-Alive thread
2024-12-25 16:55:24,851 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 17:19:46,426 - INFO - Read Transform Query - SUCCESS
2024-12-25 17:19:46,507 - INFO - Connect to DWH - SUCCESS
2024-12-25 17:19:46,508 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 17:19:46,563 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 17:19:46,566 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 17:19:46,569 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 17:19:46,581 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 17:19:46,586 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 17:19:46,626 - ERROR - Transform Tables - FAILED
2024-12-25 17:19:46,627 - ERROR - [pid 21092] Worker Worker(salt=5291183188, workers=1, host=MSI, username=alfayyedh, pid=21092) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_id" is of type uuid but expression is of type text
LINE 15:  so.order_id,
          ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_id" is of type uuid but expression is of type text
LINE 15:  so.order_id,
          ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT 
	so.order_id,
    dc.customer_id,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_actual AS review_creation_date,
    sop.payment_value
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    stg.order_items soi ON soi.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 17:19:46,646 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 17:19:46,657 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 17:19:46,657 - DEBUG - Asking scheduler for work...
2024-12-25 17:19:46,659 - DEBUG - Done
2024-12-25 17:19:46,659 - DEBUG - There are no more tasks to run at this time
2024-12-25 17:19:46,659 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 17:19:46,659 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 17:19:46,659 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 17:19:46,660 - INFO - Worker Worker(salt=5291183188, workers=1, host=MSI, username=alfayyedh, pid=21092) was stopped. Shutting down Keep-Alive thread
2024-12-25 17:19:46,661 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 17:32:19,451 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 17:32:19,504 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 17:32:19,743 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 17:32:19,845 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 17:32:19,858 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 17:32:20,353 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 17:32:21,179 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 17:32:21,815 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 17:32:22,313 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 17:32:23,170 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 17:32:23,170 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 17:32:23,174 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 17:32:23,191 - INFO - [pid 26558] Worker Worker(salt=058427775, workers=1, host=MSI, username=alfayyedh, pid=26558) done      Extract()
2024-12-25 17:32:23,192 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 17:32:23,195 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 17:32:23,195 - DEBUG - Asking scheduler for work...
2024-12-25 17:32:23,197 - DEBUG - Pending tasks: 2
2024-12-25 17:32:23,197 - INFO - [pid 26558] Worker Worker(salt=058427775, workers=1, host=MSI, username=alfayyedh, pid=26558) running   Load()
2024-12-25 17:32:23,198 - INFO - Read Load Query - SUCCESS
2024-12-25 17:32:24,404 - INFO - Read Extracted Data - SUCCESS
2024-12-25 17:32:24,406 - INFO - Connect to DWH - SUCCESS
2024-12-25 17:32:24,619 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 17:32:24,619 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 17:32:24,633 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 17:32:26,843 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 17:32:27,229 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 17:32:27,304 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 17:32:31,005 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 17:32:36,567 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 17:32:40,784 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 17:32:44,348 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 17:32:51,551 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 17:32:51,551 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 17:32:58,665 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-25 17:32:58,714 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-25 17:32:58,720 - ERROR - [pid 26558] Worker Worker(salt=058427775, workers=1, host=MSI, username=alfayyedh, pid=26558) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.CardinalityViolation: ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CardinalityViolation) ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.

[SQL: INSERT INTO stg.order_reviews
    (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date)

SELECT 
    review_id,
    order_id,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date
FROM
    public.order_reviews

ON CONFLICT(review_id) 
DO UPDATE SET
    order_id = EXCLUDED.order_id,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    updated_at = CASE WHEN 
                        stg.order_reviews.order_id <> EXCLUDED.order_id
                        OR stg.order_reviews.review_score <> EXCLUDED.review_score
                        OR stg.order_reviews.review_comment_title <> EXCLUDED.review_comment_title
                        OR stg.order_reviews.review_comment_message <> EXCLUDED.review_comment_message
                        OR stg.order_reviews.review_creation_date <> EXCLUDED.review_creation_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.order_reviews.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-25 17:32:58,753 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 17:32:58,765 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-25 17:32:58,766 - DEBUG - Asking scheduler for work...
2024-12-25 17:32:58,770 - DEBUG - Done
2024-12-25 17:32:58,771 - DEBUG - There are no more tasks to run at this time
2024-12-25 17:32:58,771 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-25 17:32:58,771 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-25 17:32:58,771 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-25 17:32:58,772 - INFO - Worker Worker(salt=058427775, workers=1, host=MSI, username=alfayyedh, pid=26558) was stopped. Shutting down Keep-Alive thread
2024-12-25 17:32:58,774 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 17:45:44,242 - INFO - Read Transform Query - SUCCESS
2024-12-25 17:45:44,304 - INFO - Connect to DWH - SUCCESS
2024-12-25 17:45:44,305 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 17:45:44,336 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 17:45:44,337 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 17:45:44,339 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 17:45:44,350 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 17:45:44,352 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 17:45:44,388 - ERROR - Transform Tables - FAILED
2024-12-25 17:45:44,390 - ERROR - [pid 32482] Worker Worker(salt=2217215222, workers=1, host=MSI, username=alfayyedh, pid=32482) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "review_creation_date" is of type integer but expression is of type date
LINE 22:     dd1.date_actual AS review_creation_date,
             ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "review_creation_date" is of type integer but expression is of type date
LINE 22:     dd1.date_actual AS review_creation_date,
             ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT 
	so.order_id::uuid,
    dc.customer_id,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_actual AS review_creation_date,
    sop.payment_value
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    stg.order_items soi ON soi.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 17:45:44,407 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 17:45:44,414 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 17:45:44,414 - DEBUG - Asking scheduler for work...
2024-12-25 17:45:44,417 - DEBUG - Done
2024-12-25 17:45:44,417 - DEBUG - There are no more tasks to run at this time
2024-12-25 17:45:44,417 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 17:45:44,417 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 17:45:44,417 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 17:45:44,417 - INFO - Worker Worker(salt=2217215222, workers=1, host=MSI, username=alfayyedh, pid=32482) was stopped. Shutting down Keep-Alive thread
2024-12-25 17:45:44,418 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 17:49:04,232 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 17:49:04,259 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 17:49:04,533 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 17:49:04,637 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 17:49:04,653 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 17:49:05,128 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 17:49:05,936 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 17:49:06,607 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 17:49:07,186 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 17:49:07,990 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 17:49:07,990 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 17:49:07,992 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 17:49:08,006 - INFO - [pid 33936] Worker Worker(salt=6984847421, workers=1, host=MSI, username=alfayyedh, pid=33936) done      Extract()
2024-12-25 17:49:08,007 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 17:49:08,009 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 17:49:08,009 - DEBUG - Asking scheduler for work...
2024-12-25 17:49:08,011 - DEBUG - Pending tasks: 2
2024-12-25 17:49:08,011 - INFO - [pid 33936] Worker Worker(salt=6984847421, workers=1, host=MSI, username=alfayyedh, pid=33936) running   Load()
2024-12-25 17:49:08,012 - INFO - Read Load Query - SUCCESS
2024-12-25 17:49:09,063 - INFO - Read Extracted Data - SUCCESS
2024-12-25 17:49:09,065 - INFO - Connect to DWH - SUCCESS
2024-12-25 17:49:09,237 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 17:49:09,237 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 17:49:09,250 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 17:49:10,943 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 17:49:11,327 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 17:49:11,413 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 17:49:14,414 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 17:49:19,094 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 17:49:23,190 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 17:49:26,756 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 17:49:33,975 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 17:49:33,976 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 17:49:40,671 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-25 17:49:40,709 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-25 17:49:40,715 - ERROR - [pid 33936] Worker Worker(salt=6984847421, workers=1, host=MSI, username=alfayyedh, pid=33936) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.CardinalityViolation: ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CardinalityViolation) ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.

[SQL: INSERT INTO stg.order_reviews
    (review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date)

SELECT 
    review_id,
    order_id,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date
FROM
    public.order_reviews

ON CONFLICT(review_id) 
DO UPDATE SET
    order_id = EXCLUDED.order_id,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    updated_at = CASE WHEN 
                        stg.order_reviews.order_id <> EXCLUDED.order_id
                        OR stg.order_reviews.review_score <> EXCLUDED.review_score
                        OR stg.order_reviews.review_comment_title <> EXCLUDED.review_comment_title
                        OR stg.order_reviews.review_comment_message <> EXCLUDED.review_comment_message
                        OR stg.order_reviews.review_creation_date <> EXCLUDED.review_creation_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.order_reviews.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-25 17:49:40,733 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 17:49:40,739 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-25 17:49:40,739 - DEBUG - Asking scheduler for work...
2024-12-25 17:49:40,741 - DEBUG - Done
2024-12-25 17:49:40,741 - DEBUG - There are no more tasks to run at this time
2024-12-25 17:49:40,741 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-25 17:49:40,741 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-25 17:49:40,741 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-25 17:49:40,742 - INFO - Worker Worker(salt=6984847421, workers=1, host=MSI, username=alfayyedh, pid=33936) was stopped. Shutting down Keep-Alive thread
2024-12-25 17:49:40,743 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 18:44:13,706 - INFO - Read Transform Query - SUCCESS
2024-12-25 18:44:13,770 - INFO - Connect to DWH - SUCCESS
2024-12-25 18:44:13,770 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 18:44:13,808 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 18:44:13,810 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 18:44:13,811 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 18:44:13,822 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 18:44:13,824 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 18:44:13,871 - ERROR - Transform Tables - FAILED
2024-12-25 18:44:13,874 - ERROR - [pid 56732] Worker Worker(salt=9689378366, workers=1, host=MSI, username=alfayyedh, pid=56732) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column excluded.geolocation_zip_code_prefix does not exist
LINE 47:     geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_c...
                                           ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column excluded.geolocation_zip_code_prefix does not exist
LINE 47:     geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_c...
                                           ^

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT 
	so.order_id::uuid,
    dc.customer_id,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_id AS review_creation_date,
    sop.payment_value
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    stg.order_items soi ON soi.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    product_id = EXCLUDED.product_id,
    geolocation_zip_code_prefix = EXCLUDED.geolocation_zip_code_prefix,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                        OR final.fct_review.geolocation_zip_code_prefix <> EXCLUDED.geolocation_zip_code_prefix
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 18:44:13,915 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 18:44:13,925 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 18:44:13,926 - DEBUG - Asking scheduler for work...
2024-12-25 18:44:13,928 - DEBUG - Done
2024-12-25 18:44:13,928 - DEBUG - There are no more tasks to run at this time
2024-12-25 18:44:13,928 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 18:44:13,928 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 18:44:13,928 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 18:44:13,928 - INFO - Worker Worker(salt=9689378366, workers=1, host=MSI, username=alfayyedh, pid=56732) was stopped. Shutting down Keep-Alive thread
2024-12-25 18:44:13,930 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 18:46:16,367 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 18:46:16,401 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 18:46:16,642 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 18:46:16,738 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 18:46:16,752 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 18:46:17,230 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 18:46:18,026 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 18:46:18,684 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 18:46:19,211 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 18:46:20,062 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 18:46:20,062 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 18:46:20,064 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 18:46:20,080 - INFO - [pid 57655] Worker Worker(salt=3577235628, workers=1, host=MSI, username=alfayyedh, pid=57655) done      Extract()
2024-12-25 18:46:20,081 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 18:46:20,083 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 18:46:20,083 - DEBUG - Asking scheduler for work...
2024-12-25 18:46:20,085 - DEBUG - Pending tasks: 2
2024-12-25 18:46:20,085 - INFO - [pid 57655] Worker Worker(salt=3577235628, workers=1, host=MSI, username=alfayyedh, pid=57655) running   Load()
2024-12-25 18:46:20,086 - INFO - Read Load Query - SUCCESS
2024-12-25 18:46:21,049 - INFO - Read Extracted Data - SUCCESS
2024-12-25 18:46:21,050 - INFO - Connect to DWH - SUCCESS
2024-12-25 18:46:21,304 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 18:46:21,304 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 18:46:21,320 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 18:46:23,045 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 18:46:23,431 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 18:46:23,511 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 18:46:26,584 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 18:46:30,913 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 18:46:35,079 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 18:46:38,731 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 18:46:45,732 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 18:46:45,732 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 18:46:55,958 - ERROR - LOAD All Tables To DWH-Staging - FAILED
2024-12-25 18:46:55,995 - ERROR - LOAD All Tables To DWH - FAILED
2024-12-25 18:46:56,002 - ERROR - [pid 57655] Worker Worker(salt=3577235628, workers=1, host=MSI, username=alfayyedh, pid=57655) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidColumnReference: there is no unique or exclusion constraint matching the ON CONFLICT specification


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 271, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.InvalidColumnReference) there is no unique or exclusion constraint matching the ON CONFLICT specification

[SQL: INSERT INTO stg.order_payments
    (order_id, payment_sequential, payment_type, payment_installments, payment_value) 

SELECT
    order_id,
    payment_sequential,
    payment_type,
    payment_installments,
    payment_value
FROM
    public.order_payments

ON CONFLICT(order_id)
DO UPDATE SET
    payment_sequential = EXCLUDED.payment_sequential,
    payment_type = EXCLUDED.payment_type,
    payment_installments = EXCLUDED.payment_installments,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        stg.order_payments.payment_sequential <> EXCLUDED.payment_sequential
                        OR stg.order_payments.payment_type <> EXCLUDED.payment_type
                        OR stg.order_payments.payment_installments <> EXCLUDED.payment_installments
                        OR stg.order_payments.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        stg.order_payments.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 282, in run
    raise Exception('Failed Load Tables To DWH-Staging')
Exception: Failed Load Tables To DWH-Staging

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 330, in run
    raise Exception('Failed Load Tables To DWH')
Exception: Failed Load Tables To DWH
2024-12-25 18:46:56,025 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 18:46:56,033 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-25 18:46:56,033 - DEBUG - Asking scheduler for work...
2024-12-25 18:46:56,036 - DEBUG - Done
2024-12-25 18:46:56,036 - DEBUG - There are no more tasks to run at this time
2024-12-25 18:46:56,036 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-25 18:46:56,036 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-25 18:46:56,036 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-25 18:46:56,036 - INFO - Worker Worker(salt=3577235628, workers=1, host=MSI, username=alfayyedh, pid=57655) was stopped. Shutting down Keep-Alive thread
2024-12-25 18:46:56,038 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 18:55:05,860 - INFO - Read Transform Query - SUCCESS
2024-12-25 18:55:05,910 - INFO - Connect to DWH - SUCCESS
2024-12-25 18:55:05,910 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 18:55:05,938 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 18:55:05,940 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 18:55:05,942 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 18:55:05,952 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 18:55:05,955 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-25 18:55:05,956 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-25 18:55:05,959 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-25 18:55:05,960 - INFO - [pid 61829] Worker Worker(salt=2059956789, workers=1, host=MSI, username=alfayyedh, pid=61829) done      Transform()
2024-12-25 18:55:05,961 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 18:55:05,964 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-25 18:55:05,964 - DEBUG - Asking scheduler for work...
2024-12-25 18:55:05,965 - DEBUG - Done
2024-12-25 18:55:05,965 - DEBUG - There are no more tasks to run at this time
2024-12-25 18:55:05,966 - INFO - Worker Worker(salt=2059956789, workers=1, host=MSI, username=alfayyedh, pid=61829) was stopped. Shutting down Keep-Alive thread
2024-12-25 18:55:05,966 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 complete ones were encountered:
    - 1 Extract()
    - 1 Load()
* 1 ran successfully:
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-25 18:59:43,527 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 18:59:43,569 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 18:59:43,808 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 18:59:43,907 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 18:59:43,922 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 18:59:44,390 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 18:59:45,250 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 18:59:45,882 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 18:59:46,367 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 18:59:47,347 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 18:59:47,347 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 18:59:47,349 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 18:59:47,364 - INFO - [pid 63810] Worker Worker(salt=7982727866, workers=1, host=MSI, username=alfayyedh, pid=63810) done      Extract()
2024-12-25 18:59:47,365 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 18:59:47,368 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 18:59:47,368 - DEBUG - Asking scheduler for work...
2024-12-25 18:59:47,370 - DEBUG - Pending tasks: 2
2024-12-25 18:59:47,370 - INFO - [pid 63810] Worker Worker(salt=7982727866, workers=1, host=MSI, username=alfayyedh, pid=63810) running   Load()
2024-12-25 18:59:47,371 - INFO - Read Load Query - SUCCESS
2024-12-25 18:59:48,347 - INFO - Read Extracted Data - SUCCESS
2024-12-25 18:59:48,348 - INFO - Connect to DWH - SUCCESS
2024-12-25 18:59:48,558 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 18:59:48,558 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 18:59:48,570 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 18:59:50,330 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 18:59:50,726 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 18:59:50,811 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 18:59:53,831 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 18:59:58,971 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 19:00:03,398 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 19:00:07,214 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 19:00:14,577 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 19:00:14,577 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 19:00:35,088 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 19:00:35,096 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 19:00:35,148 - INFO - [pid 63810] Worker Worker(salt=7982727866, workers=1, host=MSI, username=alfayyedh, pid=63810) done      Load()
2024-12-25 19:00:35,149 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:00:35,152 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 19:00:35,152 - DEBUG - Asking scheduler for work...
2024-12-25 19:00:35,155 - DEBUG - Pending tasks: 1
2024-12-25 19:00:35,155 - INFO - [pid 63810] Worker Worker(salt=7982727866, workers=1, host=MSI, username=alfayyedh, pid=63810) running   Transform()
2024-12-25 19:00:35,158 - INFO - Read Transform Query - SUCCESS
2024-12-25 19:00:35,160 - INFO - Connect to DWH - SUCCESS
2024-12-25 19:00:35,160 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 19:00:35,312 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 19:00:35,389 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 19:00:35,428 - ERROR - Transform Tables - FAILED
2024-12-25 19:00:35,432 - ERROR - [pid 63810] Worker Worker(salt=7982727866, workers=1, host=MSI, username=alfayyedh, pid=63810) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.StringDataRightTruncation: value too long for type character varying(20)


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 98, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.DataError: (psycopg2.errors.StringDataRightTruncation) value too long for type character varying(20)

[SQL: INSERT INTO final.dim_customer (
    customer_id,   
    customer_nk,
    geolocation_id
)

SELECT
    c.id AS customer_id,
    c.customer_id AS customer_nk,
    dg.geolocation_id
FROM
    stg.customers c
JOIN
    final.dim_geolocation dg ON dg.geolocation_zip_code_prefix = c.customer_zip_code_prefix
    
ON CONFLICT(customer_id) 
DO UPDATE SET
    customer_nk = EXCLUDED.customer_nk,
    geolocation_id = EXCLUDED.geolocation_id,
    updated_at = CASE WHEN 
                        final.dim_customer.customer_nk <> EXCLUDED.customer_nk
                        OR final.dim_customer.geolocation_id <> EXCLUDED.geolocation_id
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.dim_customer.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/9h9h)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 19:00:35,461 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:00:35,466 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 19:00:35,466 - DEBUG - Asking scheduler for work...
2024-12-25 19:00:35,470 - DEBUG - Done
2024-12-25 19:00:35,470 - DEBUG - There are no more tasks to run at this time
2024-12-25 19:00:35,470 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 19:00:35,470 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 19:00:35,470 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 19:00:35,471 - INFO - Worker Worker(salt=7982727866, workers=1, host=MSI, username=alfayyedh, pid=63810) was stopped. Shutting down Keep-Alive thread
2024-12-25 19:00:35,472 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 19:04:01,326 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 19:04:01,357 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 19:04:01,590 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 19:04:01,694 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 19:04:01,708 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 19:04:02,195 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 19:04:02,984 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 19:04:03,592 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 19:04:04,060 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 19:04:04,939 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 19:04:04,939 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 19:04:04,941 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 19:04:04,957 - INFO - [pid 66147] Worker Worker(salt=9829248401, workers=1, host=MSI, username=alfayyedh, pid=66147) done      Extract()
2024-12-25 19:04:04,959 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:04:04,962 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 19:04:04,962 - DEBUG - Asking scheduler for work...
2024-12-25 19:04:04,963 - DEBUG - Pending tasks: 2
2024-12-25 19:04:04,963 - INFO - [pid 66147] Worker Worker(salt=9829248401, workers=1, host=MSI, username=alfayyedh, pid=66147) running   Load()
2024-12-25 19:04:04,965 - INFO - Read Load Query - SUCCESS
2024-12-25 19:04:05,905 - INFO - Read Extracted Data - SUCCESS
2024-12-25 19:04:05,906 - INFO - Connect to DWH - SUCCESS
2024-12-25 19:04:06,104 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 19:04:06,104 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 19:04:06,115 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 19:04:08,008 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 19:04:08,391 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 19:04:08,472 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 19:04:11,456 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 19:04:15,722 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 19:04:19,784 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 19:04:23,267 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 19:04:30,047 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 19:04:30,047 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 19:04:48,782 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 19:04:48,785 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 19:04:48,830 - INFO - [pid 66147] Worker Worker(salt=9829248401, workers=1, host=MSI, username=alfayyedh, pid=66147) done      Load()
2024-12-25 19:04:48,831 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:04:48,833 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 19:04:48,833 - DEBUG - Asking scheduler for work...
2024-12-25 19:04:48,835 - DEBUG - Pending tasks: 1
2024-12-25 19:04:48,835 - INFO - [pid 66147] Worker Worker(salt=9829248401, workers=1, host=MSI, username=alfayyedh, pid=66147) running   Transform()
2024-12-25 19:04:48,836 - INFO - Read Transform Query - SUCCESS
2024-12-25 19:04:48,836 - INFO - Connect to DWH - SUCCESS
2024-12-25 19:04:48,836 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 19:04:48,962 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 19:04:50,526 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 19:04:50,696 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 19:04:51,148 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 19:04:52,177 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-25 19:04:52,178 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-25 19:04:52,181 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-25 19:04:52,182 - INFO - [pid 66147] Worker Worker(salt=9829248401, workers=1, host=MSI, username=alfayyedh, pid=66147) done      Transform()
2024-12-25 19:04:52,182 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:04:52,185 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-25 19:04:52,185 - DEBUG - Asking scheduler for work...
2024-12-25 19:04:52,188 - DEBUG - Done
2024-12-25 19:04:52,188 - DEBUG - There are no more tasks to run at this time
2024-12-25 19:04:52,188 - INFO - Worker Worker(salt=9829248401, workers=1, host=MSI, username=alfayyedh, pid=66147) was stopped. Shutting down Keep-Alive thread
2024-12-25 19:04:52,190 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-25 19:08:13,401 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 19:08:13,428 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 19:08:13,669 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 19:08:13,769 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 19:08:13,782 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 19:08:14,281 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 19:08:15,091 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 19:08:15,692 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 19:08:16,162 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 19:08:17,056 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 19:08:17,056 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 19:08:17,058 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 19:08:17,073 - INFO - [pid 68166] Worker Worker(salt=2783232388, workers=1, host=MSI, username=alfayyedh, pid=68166) done      Extract()
2024-12-25 19:08:17,075 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:08:17,077 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 19:08:17,078 - DEBUG - Asking scheduler for work...
2024-12-25 19:08:17,079 - DEBUG - Pending tasks: 2
2024-12-25 19:08:17,080 - INFO - [pid 68166] Worker Worker(salt=2783232388, workers=1, host=MSI, username=alfayyedh, pid=68166) running   Load()
2024-12-25 19:08:17,080 - INFO - Read Load Query - SUCCESS
2024-12-25 19:08:18,230 - INFO - Read Extracted Data - SUCCESS
2024-12-25 19:08:18,231 - INFO - Connect to DWH - SUCCESS
2024-12-25 19:08:18,457 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 19:08:18,457 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 19:08:18,473 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 19:08:20,339 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 19:08:20,736 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 19:08:20,814 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 19:08:23,950 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 19:08:31,138 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 19:08:35,109 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 19:08:38,762 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 19:08:45,728 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 19:08:45,728 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 19:09:08,875 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 19:09:08,882 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 19:09:08,929 - INFO - [pid 68166] Worker Worker(salt=2783232388, workers=1, host=MSI, username=alfayyedh, pid=68166) done      Load()
2024-12-25 19:09:08,930 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:09:08,933 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 19:09:08,933 - DEBUG - Asking scheduler for work...
2024-12-25 19:09:08,935 - DEBUG - Pending tasks: 1
2024-12-25 19:09:08,935 - INFO - [pid 68166] Worker Worker(salt=2783232388, workers=1, host=MSI, username=alfayyedh, pid=68166) running   Transform()
2024-12-25 19:09:08,938 - INFO - Read Transform Query - SUCCESS
2024-12-25 19:09:08,939 - INFO - Connect to DWH - SUCCESS
2024-12-25 19:09:08,940 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 19:09:09,210 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 19:09:12,574 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 19:09:13,727 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 19:09:14,319 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 19:09:14,332 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-25 19:09:14,333 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-25 19:09:14,335 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-25 19:09:14,335 - INFO - [pid 68166] Worker Worker(salt=2783232388, workers=1, host=MSI, username=alfayyedh, pid=68166) done      Transform()
2024-12-25 19:09:14,336 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 19:09:14,339 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-25 19:09:14,339 - DEBUG - Asking scheduler for work...
2024-12-25 19:09:14,341 - DEBUG - Done
2024-12-25 19:09:14,341 - DEBUG - There are no more tasks to run at this time
2024-12-25 19:09:14,341 - INFO - Worker Worker(salt=2783232388, workers=1, host=MSI, username=alfayyedh, pid=68166) was stopped. Shutting down Keep-Alive thread
2024-12-25 19:09:14,343 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-25 20:17:03,396 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 20:17:03,439 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 20:17:03,690 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 20:17:03,788 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 20:17:03,802 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 20:17:04,284 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 20:17:05,091 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 20:17:05,791 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 20:17:06,273 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 20:17:07,098 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 20:17:07,098 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 20:17:07,102 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 20:17:07,119 - INFO - [pid 97009] Worker Worker(salt=2926151129, workers=1, host=MSI, username=alfayyedh, pid=97009) done      Extract()
2024-12-25 20:17:07,119 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 20:17:07,123 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 20:17:07,124 - DEBUG - Asking scheduler for work...
2024-12-25 20:17:07,125 - DEBUG - Pending tasks: 2
2024-12-25 20:17:07,126 - INFO - [pid 97009] Worker Worker(salt=2926151129, workers=1, host=MSI, username=alfayyedh, pid=97009) running   Load()
2024-12-25 20:17:07,127 - INFO - Read Load Query - SUCCESS
2024-12-25 20:17:08,135 - INFO - Read Extracted Data - SUCCESS
2024-12-25 20:17:08,136 - INFO - Connect to DWH - SUCCESS
2024-12-25 20:17:08,332 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 20:17:08,332 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 20:17:08,344 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 20:17:10,020 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 20:17:10,419 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 20:17:10,501 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 20:17:15,433 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 20:17:20,121 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 20:17:24,203 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 20:17:27,840 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 20:17:35,020 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 20:17:35,021 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 20:17:54,575 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 20:17:54,578 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 20:17:54,617 - INFO - [pid 97009] Worker Worker(salt=2926151129, workers=1, host=MSI, username=alfayyedh, pid=97009) done      Load()
2024-12-25 20:17:54,618 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 20:17:54,622 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 20:17:54,622 - DEBUG - Asking scheduler for work...
2024-12-25 20:17:54,623 - DEBUG - Pending tasks: 1
2024-12-25 20:17:54,624 - INFO - [pid 97009] Worker Worker(salt=2926151129, workers=1, host=MSI, username=alfayyedh, pid=97009) running   Transform()
2024-12-25 20:17:54,624 - INFO - Read Transform Query - SUCCESS
2024-12-25 20:17:54,625 - INFO - Connect to DWH - SUCCESS
2024-12-25 20:17:54,625 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 20:17:54,765 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 20:17:57,047 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 20:17:57,729 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 20:17:59,745 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 20:18:02,619 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 20:18:02,665 - ERROR - Transform Tables - FAILED
2024-12-25 20:18:02,670 - ERROR - [pid 97009] Worker Worker(salt=2926151129, workers=1, host=MSI, username=alfayyedh, pid=97009) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.CardinalityViolation: ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CardinalityViolation) ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT 
	so.order_id::uuid,
    dc.customer_id,
    dp.product_id,
    so.order_status,
    sor.review_score,
    sor.review_comment_title,
    sor.review_comment_message,
    dd1.date_id AS review_creation_date,
    sop.payment_value
FROM 
	stg.orders so
JOIN
	final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
    stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
    final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
    stg.order_payments sop ON sop.order_id = so.order_id
JOIN
    stg.order_items soi ON soi.order_id = so.order_id
JOIN
    final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 20:18:02,697 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 20:18:02,706 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 20:18:02,706 - DEBUG - Asking scheduler for work...
2024-12-25 20:18:02,709 - DEBUG - Done
2024-12-25 20:18:02,709 - DEBUG - There are no more tasks to run at this time
2024-12-25 20:18:02,709 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 20:18:02,709 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 20:18:02,709 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 20:18:02,709 - INFO - Worker Worker(salt=2926151129, workers=1, host=MSI, username=alfayyedh, pid=97009) was stopped. Shutting down Keep-Alive thread
2024-12-25 20:18:02,711 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 21:24:24,590 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 21:24:24,631 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 21:24:24,875 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 21:24:24,974 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 21:24:24,987 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 21:24:25,465 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 21:24:26,281 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 21:24:26,928 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 21:24:27,422 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 21:24:28,323 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 21:24:28,323 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 21:24:28,326 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 21:24:28,342 - INFO - [pid 124748] Worker Worker(salt=4984350569, workers=1, host=MSI, username=alfayyedh, pid=124748) done      Extract()
2024-12-25 21:24:28,344 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 21:24:28,360 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 21:24:28,360 - DEBUG - Asking scheduler for work...
2024-12-25 21:24:28,362 - DEBUG - Pending tasks: 2
2024-12-25 21:24:28,362 - INFO - [pid 124748] Worker Worker(salt=4984350569, workers=1, host=MSI, username=alfayyedh, pid=124748) running   Load()
2024-12-25 21:24:28,416 - INFO - Read Load Query - SUCCESS
2024-12-25 21:24:29,526 - INFO - Read Extracted Data - SUCCESS
2024-12-25 21:24:29,527 - INFO - Connect to DWH - SUCCESS
2024-12-25 21:24:31,484 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 21:24:31,484 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 21:24:31,495 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 21:24:33,202 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 21:24:33,581 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 21:24:33,662 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 21:24:36,571 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 21:24:41,022 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 21:24:44,903 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 21:24:49,015 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 21:24:56,193 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 21:24:56,193 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 21:25:15,862 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 21:25:15,865 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 21:25:15,913 - INFO - [pid 124748] Worker Worker(salt=4984350569, workers=1, host=MSI, username=alfayyedh, pid=124748) done      Load()
2024-12-25 21:25:15,913 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 21:25:15,916 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 21:25:15,916 - DEBUG - Asking scheduler for work...
2024-12-25 21:25:15,918 - DEBUG - Pending tasks: 1
2024-12-25 21:25:15,918 - INFO - [pid 124748] Worker Worker(salt=4984350569, workers=1, host=MSI, username=alfayyedh, pid=124748) running   Transform()
2024-12-25 21:25:15,919 - INFO - Read Transform Query - SUCCESS
2024-12-25 21:25:15,920 - INFO - Connect to DWH - SUCCESS
2024-12-25 21:25:15,920 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 21:25:16,063 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 21:25:17,483 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 21:25:17,654 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 21:25:19,455 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 21:25:21,614 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 21:25:21,650 - ERROR - Transform Tables - FAILED
2024-12-25 21:25:21,655 - ERROR - [pid 124748] Worker Worker(salt=4984350569, workers=1, host=MSI, username=alfayyedh, pid=124748) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.CardinalityViolation: ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CardinalityViolation) ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.

[SQL: INSERT INTO final.fct_review (
    -- review_id is generated by UUID
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT DISTINCT 
     so.order_id::uuid,
     dc.customer_id,
     dp.product_id,
     so.order_status,
     sor.review_score,
     sor.review_comment_title,
     sor.review_comment_message,
     dd1.date_id AS review_creation_date,
     sop.payment_value
FROM 
     stg.orders so
JOIN
     final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
     stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
     final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
     stg.order_payments sop ON sop.order_id = so.order_id
JOIN
     stg.order_items soi ON soi.order_id = so.order_id
JOIN
     final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(order_id, customer_id, order_status, product_id) 
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 21:25:21,679 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 21:25:21,686 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 21:25:21,686 - DEBUG - Asking scheduler for work...
2024-12-25 21:25:21,688 - DEBUG - Done
2024-12-25 21:25:21,688 - DEBUG - There are no more tasks to run at this time
2024-12-25 21:25:21,688 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 21:25:21,689 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 21:25:21,689 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 21:25:21,689 - INFO - Worker Worker(salt=4984350569, workers=1, host=MSI, username=alfayyedh, pid=124748) was stopped. Shutting down Keep-Alive thread
2024-12-25 21:25:21,690 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 21:51:27,527 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 21:51:27,589 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 21:51:27,847 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 21:51:27,949 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 21:51:27,962 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 21:51:28,466 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 21:51:29,248 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 21:51:29,895 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 21:51:30,455 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 21:51:31,361 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 21:51:31,361 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 21:51:31,365 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 21:51:31,380 - INFO - [pid 136033] Worker Worker(salt=3896052331, workers=1, host=MSI, username=alfayyedh, pid=136033) done      Extract()
2024-12-25 21:51:31,381 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 21:51:31,385 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 21:51:31,385 - DEBUG - Asking scheduler for work...
2024-12-25 21:51:31,387 - DEBUG - Pending tasks: 2
2024-12-25 21:51:31,388 - INFO - [pid 136033] Worker Worker(salt=3896052331, workers=1, host=MSI, username=alfayyedh, pid=136033) running   Load()
2024-12-25 21:51:31,390 - INFO - Read Load Query - SUCCESS
2024-12-25 21:51:32,476 - INFO - Read Extracted Data - SUCCESS
2024-12-25 21:51:32,477 - INFO - Connect to DWH - SUCCESS
2024-12-25 21:51:32,733 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 21:51:32,734 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 21:51:32,751 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 21:51:35,006 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 21:51:35,413 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 21:51:35,500 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 21:51:38,580 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 21:51:45,175 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 21:51:50,426 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 21:51:54,664 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 21:52:02,904 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 21:52:02,904 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 21:52:25,764 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 21:52:25,766 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 21:52:25,813 - INFO - [pid 136033] Worker Worker(salt=3896052331, workers=1, host=MSI, username=alfayyedh, pid=136033) done      Load()
2024-12-25 21:52:25,813 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 21:52:25,817 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 21:52:25,817 - DEBUG - Asking scheduler for work...
2024-12-25 21:52:25,818 - DEBUG - Pending tasks: 1
2024-12-25 21:52:25,818 - INFO - [pid 136033] Worker Worker(salt=3896052331, workers=1, host=MSI, username=alfayyedh, pid=136033) running   Transform()
2024-12-25 21:52:25,819 - INFO - Read Transform Query - SUCCESS
2024-12-25 21:52:25,820 - INFO - Connect to DWH - SUCCESS
2024-12-25 21:52:25,820 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 21:52:26,060 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 21:52:29,252 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 21:52:31,415 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 21:52:32,642 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 21:52:33,416 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 21:52:33,456 - ERROR - Transform Tables - FAILED
2024-12-25 21:52:33,461 - ERROR - [pid 136033] Worker Worker(salt=3896052331, workers=1, host=MSI, username=alfayyedh, pid=136033) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.CardinalityViolation: ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CardinalityViolation) ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.

[SQL: WITH unique_reviews AS (
    SELECT
        so.order_id::uuid,
        dc.customer_id,
        dp.product_id,
        so.order_status,
        sor.review_score,
        sor.review_comment_title,
        sor.review_comment_message,
        dd1.date_id AS review_creation_date,
        sop.payment_value
    FROM
        stg.orders so
    JOIN final.dim_customer dc ON so.customer_id = dc.customer_nk
    JOIN stg.order_reviews sor ON sor.order_id = so.order_id
    JOIN final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
    JOIN stg.order_payments sop ON sop.order_id = so.order_id
    JOIN stg.order_items soi ON soi.order_id = so.order_id
    JOIN final.dim_product dp ON dp.product_nk = soi.product_id
)
INSERT INTO final.fct_review (
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value
)
SELECT * FROM unique_reviews
ON CONFLICT(order_id, customer_id, order_status, product_id)
DO UPDATE SET
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                    THEN 
                        CURRENT_TIMESTAMP
                    ELSE
                        final.fct_review.updated_at
                    END;
]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 21:52:33,484 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 21:52:33,494 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 21:52:33,494 - DEBUG - Asking scheduler for work...
2024-12-25 21:52:33,497 - DEBUG - Done
2024-12-25 21:52:33,498 - DEBUG - There are no more tasks to run at this time
2024-12-25 21:52:33,498 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 21:52:33,498 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 21:52:33,498 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 21:52:33,498 - INFO - Worker Worker(salt=3896052331, workers=1, host=MSI, username=alfayyedh, pid=136033) was stopped. Shutting down Keep-Alive thread
2024-12-25 21:52:33,500 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 22:14:48,477 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 22:14:48,533 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 22:14:48,820 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 22:14:48,924 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 22:14:48,940 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 22:14:49,502 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 22:14:50,355 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 22:14:51,134 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 22:14:51,878 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 22:14:52,822 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 22:14:52,823 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 22:14:52,826 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 22:14:52,843 - INFO - [pid 145944] Worker Worker(salt=9123403423, workers=1, host=MSI, username=alfayyedh, pid=145944) done      Extract()
2024-12-25 22:14:52,844 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:14:52,847 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 22:14:52,847 - DEBUG - Asking scheduler for work...
2024-12-25 22:14:52,849 - DEBUG - Pending tasks: 2
2024-12-25 22:14:52,849 - INFO - [pid 145944] Worker Worker(salt=9123403423, workers=1, host=MSI, username=alfayyedh, pid=145944) running   Load()
2024-12-25 22:14:52,850 - INFO - Read Load Query - SUCCESS
2024-12-25 22:14:54,632 - INFO - Read Extracted Data - SUCCESS
2024-12-25 22:14:54,633 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:14:54,927 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 22:14:54,928 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 22:14:54,950 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 22:14:57,166 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 22:14:57,572 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 22:14:57,668 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 22:15:01,050 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 22:15:06,239 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 22:15:10,509 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 22:15:16,802 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 22:15:25,747 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 22:15:25,747 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 22:15:56,804 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 22:15:56,807 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 22:15:56,867 - INFO - [pid 145944] Worker Worker(salt=9123403423, workers=1, host=MSI, username=alfayyedh, pid=145944) done      Load()
2024-12-25 22:15:56,868 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:15:56,871 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 22:15:56,871 - DEBUG - Asking scheduler for work...
2024-12-25 22:15:56,873 - DEBUG - Pending tasks: 1
2024-12-25 22:15:56,873 - INFO - [pid 145944] Worker Worker(salt=9123403423, workers=1, host=MSI, username=alfayyedh, pid=145944) running   Transform()
2024-12-25 22:15:56,874 - INFO - Read Transform Query - SUCCESS
2024-12-25 22:15:56,875 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:15:56,875 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 22:15:57,107 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 22:15:59,291 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 22:16:00,679 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 22:16:02,688 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 22:16:02,693 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 22:16:02,763 - ERROR - Transform Tables - FAILED
2024-12-25 22:16:02,769 - ERROR - [pid 145944] Worker Worker(salt=9123403423, workers=1, host=MSI, username=alfayyedh, pid=145944) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "dd_review_id" of relation "fct_review" does not exist
LINE 3:     dd_review_id,
            ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "dd_review_id" of relation "fct_review" does not exist
LINE 3:     dd_review_id,
            ^

[SQL: INSERT INTO final.fct_review (
    review_id,
    dd_review_id,
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT DISTINCT 
     sor.id as review_id,
     review_id as dd_review_id,
     so.order_id::uuid,
     dc.customer_id,
     dp.product_id,
     so.order_status,
     sor.review_score,
     sor.review_comment_title,
     sor.review_comment_message,
     dd1.date_id AS review_creation_date,
     sop.payment_value
FROM 
     stg.orders so
JOIN
     final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
     stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
     final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
     stg.order_payments sop ON sop.order_id = so.order_id
JOIN
     stg.order_items soi ON soi.order_id = so.order_id
JOIN
     final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(review_id, dd_review_id, order_id, customer_id, product_id) 
DO UPDATE SET
    order_status = EXCLUDED.order_status,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.order_status <> EXCLUDED.order_status
                        OR final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 22:16:02,794 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:16:02,803 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 22:16:02,803 - DEBUG - Asking scheduler for work...
2024-12-25 22:16:02,805 - DEBUG - Done
2024-12-25 22:16:02,806 - DEBUG - There are no more tasks to run at this time
2024-12-25 22:16:02,806 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 22:16:02,806 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 22:16:02,806 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 22:16:02,806 - INFO - Worker Worker(salt=9123403423, workers=1, host=MSI, username=alfayyedh, pid=145944) was stopped. Shutting down Keep-Alive thread
2024-12-25 22:16:02,807 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 22:18:05,982 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 22:18:06,011 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 22:18:06,268 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 22:18:06,374 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 22:18:06,387 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 22:18:06,924 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 22:18:07,798 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 22:18:08,453 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 22:18:08,947 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 22:18:09,846 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 22:18:09,846 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 22:18:09,847 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 22:18:09,863 - INFO - [pid 147992] Worker Worker(salt=9271166854, workers=1, host=MSI, username=alfayyedh, pid=147992) done      Extract()
2024-12-25 22:18:09,865 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:18:09,867 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 22:18:09,867 - DEBUG - Asking scheduler for work...
2024-12-25 22:18:09,869 - DEBUG - Pending tasks: 2
2024-12-25 22:18:09,869 - INFO - [pid 147992] Worker Worker(salt=9271166854, workers=1, host=MSI, username=alfayyedh, pid=147992) running   Load()
2024-12-25 22:18:09,870 - INFO - Read Load Query - SUCCESS
2024-12-25 22:18:10,921 - INFO - Read Extracted Data - SUCCESS
2024-12-25 22:18:10,922 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:18:11,140 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 22:18:11,141 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 22:18:11,151 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 22:18:12,876 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 22:18:13,385 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 22:18:13,560 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 22:18:16,697 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 22:18:21,402 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 22:18:26,194 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 22:18:30,107 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 22:18:37,913 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 22:18:37,913 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 22:19:00,787 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 22:19:00,789 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 22:19:00,840 - INFO - [pid 147992] Worker Worker(salt=9271166854, workers=1, host=MSI, username=alfayyedh, pid=147992) done      Load()
2024-12-25 22:19:00,841 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:19:00,843 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 22:19:00,844 - DEBUG - Asking scheduler for work...
2024-12-25 22:19:00,845 - DEBUG - Pending tasks: 1
2024-12-25 22:19:00,845 - INFO - [pid 147992] Worker Worker(salt=9271166854, workers=1, host=MSI, username=alfayyedh, pid=147992) running   Transform()
2024-12-25 22:19:00,846 - INFO - Read Transform Query - SUCCESS
2024-12-25 22:19:00,847 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:19:00,847 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 22:19:01,089 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 22:19:02,663 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 22:19:02,824 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 22:19:04,642 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 22:19:04,646 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 22:19:04,687 - ERROR - Transform Tables - FAILED
2024-12-25 22:19:04,693 - ERROR - [pid 147992] Worker Worker(salt=9271166854, workers=1, host=MSI, username=alfayyedh, pid=147992) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidColumnReference: there is no unique or exclusion constraint matching the ON CONFLICT specification


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.InvalidColumnReference) there is no unique or exclusion constraint matching the ON CONFLICT specification

[SQL: INSERT INTO final.fct_review (
    review_id,
    dd_review_id,
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT DISTINCT 
     sor.id as review_id,
     review_id as dd_review_id,
     so.order_id::uuid,
     dc.customer_id,
     dp.product_id,
     so.order_status,
     sor.review_score,
     sor.review_comment_title,
     sor.review_comment_message,
     dd1.date_id AS review_creation_date,
     sop.payment_value
FROM 
     stg.orders so
JOIN
     final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
     stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
     final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
     stg.order_payments sop ON sop.order_id = so.order_id
JOIN
     stg.order_items soi ON soi.order_id = so.order_id
JOIN
     final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(review_id, dd_review_id, order_id, customer_id, product_id) 
DO UPDATE SET
    order_status = EXCLUDED.order_status,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.order_status <> EXCLUDED.order_status
                        OR final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 22:19:04,721 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:19:04,728 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 22:19:04,728 - DEBUG - Asking scheduler for work...
2024-12-25 22:19:04,730 - DEBUG - Done
2024-12-25 22:19:04,731 - DEBUG - There are no more tasks to run at this time
2024-12-25 22:19:04,731 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 22:19:04,731 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 22:19:04,731 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 22:19:04,731 - INFO - Worker Worker(salt=9271166854, workers=1, host=MSI, username=alfayyedh, pid=147992) was stopped. Shutting down Keep-Alive thread
2024-12-25 22:19:04,733 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 22:24:00,745 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 22:24:00,794 - ERROR - EXTRACT 'public.product_category_name_translation' - FAILED.
2024-12-25 22:24:00,831 - INFO - Extract All Tables From Sources - FAILED
2024-12-25 22:24:00,836 - ERROR - [pid 150938] Worker Worker(salt=9088386018, workers=1, host=MSI, username=alfayyedh, pid=150938) failed    Extract()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5438 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/extract.py", line 56, in run
    df = pd.read_sql_query(extract_query.format(table_name = table_name), src_engine)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 525, in read_sql_query
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 906, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/pandas/io/sql.py", line 1636, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3279, in connect
    return self._connection_cls(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2443, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3303, in raw_connection
    return self.pool.connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 900, in __connect
    with util.safe_reraise():
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 896, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 643, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 616, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5438 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

(Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/extract.py", line 65, in run
    raise Exception(f"Failed to extract '{table_name}' tables")
Exception: Failed to extract 'public.product_category_name_translation' tables

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/extract.py", line 104, in run
    raise Exception(f"FAILED to execute EXTRACT TASK !!!")
Exception: FAILED to execute EXTRACT TASK !!!
2024-12-25 22:24:00,910 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:24:00,924 - INFO - Informed scheduler that task   Extract__99914b932b   has status   FAILED
2024-12-25 22:24:00,924 - DEBUG - Asking scheduler for work...
2024-12-25 22:24:00,927 - DEBUG - Done
2024-12-25 22:24:00,927 - DEBUG - There are no more tasks to run at this time
2024-12-25 22:24:00,927 - DEBUG - There are 3 pending tasks possibly being run by other workers
2024-12-25 22:24:00,927 - DEBUG - There are 3 pending tasks unique to this worker
2024-12-25 22:24:00,927 - DEBUG - There are 3 pending tasks last scheduled by this worker
2024-12-25 22:24:00,928 - INFO - Worker Worker(salt=9088386018, workers=1, host=MSI, username=alfayyedh, pid=150938) was stopped. Shutting down Keep-Alive thread
2024-12-25 22:24:00,929 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 failed:
    - 1 Extract()
* 2 were left pending, among these:
    * 2 had failed dependencies:
        - 1 Load()
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 22:25:22,297 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 22:25:22,328 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 22:25:22,594 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 22:25:22,692 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 22:25:22,707 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 22:25:23,172 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 22:25:23,938 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 22:25:24,591 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 22:25:25,198 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 22:25:26,126 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 22:25:26,126 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 22:25:26,128 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 22:25:26,144 - INFO - [pid 151575] Worker Worker(salt=7439171941, workers=1, host=MSI, username=alfayyedh, pid=151575) done      Extract()
2024-12-25 22:25:26,145 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:25:26,150 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 22:25:26,150 - DEBUG - Asking scheduler for work...
2024-12-25 22:25:26,151 - DEBUG - Pending tasks: 2
2024-12-25 22:25:26,152 - INFO - [pid 151575] Worker Worker(salt=7439171941, workers=1, host=MSI, username=alfayyedh, pid=151575) running   Load()
2024-12-25 22:25:26,153 - INFO - Read Load Query - SUCCESS
2024-12-25 22:25:27,128 - INFO - Read Extracted Data - SUCCESS
2024-12-25 22:25:27,129 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:25:27,358 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 22:25:27,359 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 22:25:27,378 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 22:25:29,046 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 22:25:29,447 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 22:25:29,528 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 22:25:32,608 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 22:25:37,020 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 22:25:40,757 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 22:25:44,443 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 22:25:51,199 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 22:25:51,200 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 22:26:09,555 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 22:26:09,558 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 22:26:09,612 - INFO - [pid 151575] Worker Worker(salt=7439171941, workers=1, host=MSI, username=alfayyedh, pid=151575) done      Load()
2024-12-25 22:26:09,612 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:26:09,615 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 22:26:09,615 - DEBUG - Asking scheduler for work...
2024-12-25 22:26:09,617 - DEBUG - Pending tasks: 1
2024-12-25 22:26:09,617 - INFO - [pid 151575] Worker Worker(salt=7439171941, workers=1, host=MSI, username=alfayyedh, pid=151575) running   Transform()
2024-12-25 22:26:09,618 - INFO - Read Transform Query - SUCCESS
2024-12-25 22:26:09,619 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:26:09,619 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 22:26:09,766 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 22:26:11,194 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 22:26:11,427 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 22:26:12,931 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 22:26:15,226 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 22:26:15,261 - ERROR - Transform Tables - FAILED
2024-12-25 22:26:15,266 - ERROR - [pid 151575] Worker Worker(salt=7439171941, workers=1, host=MSI, username=alfayyedh, pid=151575) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "fct_review_pkey"
DETAIL:  Key (review_id)=(000be3bc-cb0b-42c3-a074-280d7847f13f) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "fct_review_pkey"
DETAIL:  Key (review_id)=(000be3bc-cb0b-42c3-a074-280d7847f13f) already exists.

[SQL: INSERT INTO final.fct_review (
    review_id,
    dd_review_id,
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT DISTINCT 
     sor.id as review_id,
     review_id as dd_review_id,
     so.order_id::uuid,
     dc.customer_id,
     dp.product_id,
     so.order_status,
     sor.review_score,
     sor.review_comment_title,
     sor.review_comment_message,
     dd1.date_id AS review_creation_date,
     sop.payment_value
FROM 
     stg.orders so
JOIN
     final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
     stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
     final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
     stg.order_payments sop ON sop.order_id = so.order_id
JOIN
     stg.order_items soi ON soi.order_id = so.order_id
JOIN
     final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(review_id, dd_review_id, order_id, customer_id, product_id) 
DO UPDATE SET
    order_status = EXCLUDED.order_status,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.order_status <> EXCLUDED.order_status
                        OR final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 22:26:15,286 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:26:15,292 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 22:26:15,293 - DEBUG - Asking scheduler for work...
2024-12-25 22:26:15,295 - DEBUG - Done
2024-12-25 22:26:15,296 - DEBUG - There are no more tasks to run at this time
2024-12-25 22:26:15,296 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 22:26:15,296 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 22:26:15,296 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 22:26:15,296 - INFO - Worker Worker(salt=7439171941, workers=1, host=MSI, username=alfayyedh, pid=151575) was stopped. Shutting down Keep-Alive thread
2024-12-25 22:26:15,297 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 22:30:30,130 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 22:30:30,161 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 22:30:30,403 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 22:30:30,501 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 22:30:30,513 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 22:30:30,985 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 22:30:31,800 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 22:30:32,462 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 22:30:32,967 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 22:30:33,759 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 22:30:33,759 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 22:30:33,761 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 22:30:33,777 - INFO - [pid 153983] Worker Worker(salt=589082278, workers=1, host=MSI, username=alfayyedh, pid=153983) done      Extract()
2024-12-25 22:30:33,777 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:30:33,780 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 22:30:33,780 - DEBUG - Asking scheduler for work...
2024-12-25 22:30:33,781 - DEBUG - Pending tasks: 2
2024-12-25 22:30:33,781 - INFO - [pid 153983] Worker Worker(salt=589082278, workers=1, host=MSI, username=alfayyedh, pid=153983) running   Load()
2024-12-25 22:30:33,782 - INFO - Read Load Query - SUCCESS
2024-12-25 22:30:34,894 - INFO - Read Extracted Data - SUCCESS
2024-12-25 22:30:34,895 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:30:35,113 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 22:30:35,113 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 22:30:35,127 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 22:30:36,792 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 22:30:37,193 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 22:30:37,275 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 22:30:40,697 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 22:30:45,878 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 22:30:50,372 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 22:30:55,133 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 22:31:02,249 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 22:31:02,249 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 22:31:26,491 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 22:31:26,494 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 22:31:26,601 - INFO - [pid 153983] Worker Worker(salt=589082278, workers=1, host=MSI, username=alfayyedh, pid=153983) done      Load()
2024-12-25 22:31:26,601 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:31:26,606 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 22:31:26,606 - DEBUG - Asking scheduler for work...
2024-12-25 22:31:26,609 - DEBUG - Pending tasks: 1
2024-12-25 22:31:26,609 - INFO - [pid 153983] Worker Worker(salt=589082278, workers=1, host=MSI, username=alfayyedh, pid=153983) running   Transform()
2024-12-25 22:31:26,610 - INFO - Read Transform Query - SUCCESS
2024-12-25 22:31:26,611 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:31:26,611 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 22:31:26,870 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 22:31:28,615 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 22:31:29,805 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 22:31:30,648 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 22:31:31,755 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 22:31:31,796 - ERROR - Transform Tables - FAILED
2024-12-25 22:31:31,803 - ERROR - [pid 153983] Worker Worker(salt=589082278, workers=1, host=MSI, username=alfayyedh, pid=153983) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "fct_review_pkey"
DETAIL:  Key (review_id)=(000be3bc-cb0b-42c3-a074-280d7847f13f) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "fct_review_pkey"
DETAIL:  Key (review_id)=(000be3bc-cb0b-42c3-a074-280d7847f13f) already exists.

[SQL: INSERT INTO final.fct_review (
    review_id,
    dd_review_id,
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT DISTINCT 
     sor.id as review_id,
     review_id as dd_review_id,
     so.order_id::uuid,
     dc.customer_id,
     dp.product_id,
     so.order_status,
     sor.review_score,
     sor.review_comment_title,
     sor.review_comment_message,
     dd1.date_id AS review_creation_date,
     sop.payment_value
FROM 
     stg.orders so
JOIN
     final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
     stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
     final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
     stg.order_payments sop ON sop.order_id = so.order_id
JOIN
     stg.order_items soi ON soi.order_id = so.order_id
JOIN
     final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(review_id, dd_review_id, order_id, customer_id, product_id) 
DO UPDATE SET
    order_status = EXCLUDED.order_status,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.order_status <> EXCLUDED.order_status
                        OR final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/gkpj)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 22:31:31,827 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:31:31,835 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 22:31:31,835 - DEBUG - Asking scheduler for work...
2024-12-25 22:31:31,837 - DEBUG - Done
2024-12-25 22:31:31,837 - DEBUG - There are no more tasks to run at this time
2024-12-25 22:31:31,838 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 22:31:31,838 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 22:31:31,838 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 22:31:31,838 - INFO - Worker Worker(salt=589082278, workers=1, host=MSI, username=alfayyedh, pid=153983) was stopped. Shutting down Keep-Alive thread
2024-12-25 22:31:31,839 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 22:45:45,677 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 22:45:45,719 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 22:45:46,019 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 22:45:46,120 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 22:45:46,134 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 22:45:46,791 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 22:45:47,728 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 22:45:48,344 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 22:45:48,928 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 22:45:49,824 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 22:45:49,824 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 22:45:49,828 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 22:45:49,863 - INFO - [pid 160821] Worker Worker(salt=736032509, workers=1, host=MSI, username=alfayyedh, pid=160821) done      Extract()
2024-12-25 22:45:49,866 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:45:49,869 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 22:45:49,870 - DEBUG - Asking scheduler for work...
2024-12-25 22:45:49,872 - DEBUG - Pending tasks: 2
2024-12-25 22:45:49,872 - INFO - [pid 160821] Worker Worker(salt=736032509, workers=1, host=MSI, username=alfayyedh, pid=160821) running   Load()
2024-12-25 22:45:49,873 - INFO - Read Load Query - SUCCESS
2024-12-25 22:45:50,865 - INFO - Read Extracted Data - SUCCESS
2024-12-25 22:45:50,867 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:45:51,078 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 22:45:51,078 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 22:45:51,091 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 22:45:52,848 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 22:45:53,222 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 22:45:53,299 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 22:45:56,318 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 22:46:00,724 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 22:46:04,459 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 22:46:07,908 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 22:46:14,616 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 22:46:14,616 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 22:46:31,805 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 22:46:31,807 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 22:46:31,846 - INFO - [pid 160821] Worker Worker(salt=736032509, workers=1, host=MSI, username=alfayyedh, pid=160821) done      Load()
2024-12-25 22:46:31,847 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:46:31,851 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 22:46:31,851 - DEBUG - Asking scheduler for work...
2024-12-25 22:46:31,853 - DEBUG - Pending tasks: 1
2024-12-25 22:46:31,853 - INFO - [pid 160821] Worker Worker(salt=736032509, workers=1, host=MSI, username=alfayyedh, pid=160821) running   Transform()
2024-12-25 22:46:31,854 - INFO - Read Transform Query - SUCCESS
2024-12-25 22:46:31,855 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:46:31,855 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 22:46:32,032 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 22:46:33,725 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 22:46:33,898 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 22:46:35,746 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 22:46:38,515 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-25 22:46:38,561 - ERROR - Transform Tables - FAILED
2024-12-25 22:46:38,569 - ERROR - [pid 160821] Worker Worker(salt=736032509, workers=1, host=MSI, username=alfayyedh, pid=160821) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.CardinalityViolation: ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 116, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.CardinalityViolation) ON CONFLICT DO UPDATE command cannot affect row a second time
HINT:  Ensure that no rows proposed for insertion within the same command have duplicate constrained values.

[SQL: INSERT INTO final.fct_review (
    review_id,
    dd_review_id,
    order_id,
    customer_id,
    product_id,
    order_status,
    review_score,
    review_comment_title,
    review_comment_message,
    review_creation_date,
    payment_value    
)

SELECT DISTINCT 
     sor.id as review_id,
     review_id as dd_review_id,
     so.order_id::uuid,
     dc.customer_id,
     dp.product_id,
     so.order_status,
     sor.review_score,
     sor.review_comment_title,
     sor.review_comment_message,
     dd1.date_id AS review_creation_date,
     sop.payment_value
FROM 
     stg.orders so
JOIN
     final.dim_customer dc ON so.customer_id = dc.customer_nk
JOIN
     stg.order_reviews sor ON sor.order_id = so.order_id
JOIN
     final.dim_date dd1 ON dd1.date_actual = TO_DATE(sor.review_creation_date::text, 'YYYY-MM-DD')
JOIN
     stg.order_payments sop ON sop.order_id = so.order_id
JOIN
     stg.order_items soi ON soi.order_id = so.order_id
JOIN
     final.dim_product dp ON dp.product_nk = soi.product_id

ON CONFLICT(review_id, dd_review_id, order_id, customer_id, product_id) 
DO UPDATE SET
    order_status = EXCLUDED.order_status,
    review_score = EXCLUDED.review_score,
    review_comment_title = EXCLUDED.review_comment_title,
    review_comment_message = EXCLUDED.review_comment_message,
    review_creation_date = EXCLUDED.review_creation_date,
    payment_value = EXCLUDED.payment_value,
    updated_at = CASE WHEN 
                        final.fct_review.order_status <> EXCLUDED.order_status
                        OR final.fct_review.review_score <> EXCLUDED.review_score
                        OR final.fct_review.review_comment_title <> EXCLUDED.review_comment_title
                        OR final.fct_review.review_comment_message <> EXCLUDED.review_comment_message
                        OR final.fct_review.review_creation_date <> EXCLUDED.review_creation_date
                        OR final.fct_review.payment_value <> EXCLUDED.payment_value
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_review.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-25 22:46:38,601 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:46:38,610 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-25 22:46:38,610 - DEBUG - Asking scheduler for work...
2024-12-25 22:46:38,612 - DEBUG - Done
2024-12-25 22:46:38,612 - DEBUG - There are no more tasks to run at this time
2024-12-25 22:46:38,613 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-25 22:46:38,613 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-25 22:46:38,613 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-25 22:46:38,613 - INFO - Worker Worker(salt=736032509, workers=1, host=MSI, username=alfayyedh, pid=160821) was stopped. Shutting down Keep-Alive thread
2024-12-25 22:46:38,615 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-25 22:48:15,643 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 22:48:15,672 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 22:48:15,910 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 22:48:16,012 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 22:48:16,024 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 22:48:16,477 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 22:48:17,239 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 22:48:17,831 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 22:48:18,301 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 22:48:19,251 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 22:48:19,251 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 22:48:19,253 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 22:48:19,270 - INFO - [pid 162169] Worker Worker(salt=1079094084, workers=1, host=MSI, username=alfayyedh, pid=162169) done      Extract()
2024-12-25 22:48:19,272 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:48:19,274 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 22:48:19,275 - DEBUG - Asking scheduler for work...
2024-12-25 22:48:19,277 - DEBUG - Pending tasks: 2
2024-12-25 22:48:19,277 - INFO - [pid 162169] Worker Worker(salt=1079094084, workers=1, host=MSI, username=alfayyedh, pid=162169) running   Load()
2024-12-25 22:48:19,278 - INFO - Read Load Query - SUCCESS
2024-12-25 22:48:20,250 - INFO - Read Extracted Data - SUCCESS
2024-12-25 22:48:20,251 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:48:20,476 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-25 22:48:20,476 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-25 22:48:20,488 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-25 22:48:22,252 - INFO - LOAD 'public.products' - SUCCESS
2024-12-25 22:48:22,637 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-25 22:48:22,725 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-25 22:48:25,586 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-25 22:48:31,596 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-25 22:48:35,559 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-25 22:48:39,142 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-25 22:48:46,054 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-25 22:48:46,054 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-25 22:49:04,923 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-25 22:49:04,926 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-25 22:49:04,975 - INFO - [pid 162169] Worker Worker(salt=1079094084, workers=1, host=MSI, username=alfayyedh, pid=162169) done      Load()
2024-12-25 22:49:04,976 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:49:04,981 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-25 22:49:04,981 - DEBUG - Asking scheduler for work...
2024-12-25 22:49:04,983 - DEBUG - Pending tasks: 1
2024-12-25 22:49:04,984 - INFO - [pid 162169] Worker Worker(salt=1079094084, workers=1, host=MSI, username=alfayyedh, pid=162169) running   Transform()
2024-12-25 22:49:04,985 - INFO - Read Transform Query - SUCCESS
2024-12-25 22:49:04,985 - INFO - Connect to DWH - SUCCESS
2024-12-25 22:49:04,986 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-25 22:49:05,267 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-25 22:49:06,871 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-25 22:49:07,307 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-25 22:49:08,245 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-25 22:49:14,667 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-25 22:49:14,682 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-25 22:49:14,684 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-25 22:49:14,684 - INFO - [pid 162169] Worker Worker(salt=1079094084, workers=1, host=MSI, username=alfayyedh, pid=162169) done      Transform()
2024-12-25 22:49:14,685 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 22:49:14,687 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-25 22:49:14,688 - DEBUG - Asking scheduler for work...
2024-12-25 22:49:14,689 - DEBUG - Done
2024-12-25 22:49:14,690 - DEBUG - There are no more tasks to run at this time
2024-12-25 22:49:14,690 - INFO - Worker Worker(salt=1079094084, workers=1, host=MSI, username=alfayyedh, pid=162169) was stopped. Shutting down Keep-Alive thread
2024-12-25 22:49:14,693 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-25 23:02:44,963 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-25 23:02:45,001 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-25 23:02:45,231 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-25 23:02:45,342 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-25 23:02:45,358 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-25 23:02:46,118 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-25 23:02:47,393 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-25 23:02:48,188 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-25 23:02:48,749 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-25 23:02:49,725 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-25 23:02:49,726 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-25 23:02:49,729 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-25 23:02:49,745 - INFO - [pid 168698] Worker Worker(salt=9648748256, workers=1, host=MSI, username=alfayyedh, pid=168698) done      Extract()
2024-12-25 23:02:49,746 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 23:02:49,749 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-25 23:02:49,749 - DEBUG - Asking scheduler for work...
2024-12-25 23:02:49,750 - DEBUG - Pending tasks: 2
2024-12-25 23:02:49,751 - INFO - [pid 168698] Worker Worker(salt=9648748256, workers=1, host=MSI, username=alfayyedh, pid=168698) running   Load()
2024-12-25 23:02:49,752 - INFO - Read Load Query - SUCCESS
2024-12-25 23:02:50,953 - INFO - Read Extracted Data - SUCCESS
2024-12-25 23:02:50,954 - INFO - Connect to DWH - SUCCESS
2024-12-25 23:02:50,976 - ERROR - Truncate public Schema in DWH - FAILED
2024-12-25 23:02:51,016 - ERROR - [pid 168698] Worker Worker(salt=9648748256, workers=1, host=MSI, username=alfayyedh, pid=168698) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-12-25 23:02:51,039 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-25 23:02:51,093 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-25 23:02:51,093 - DEBUG - Asking scheduler for work...
2024-12-25 23:02:51,096 - DEBUG - Done
2024-12-25 23:02:51,096 - DEBUG - There are no more tasks to run at this time
2024-12-25 23:02:51,096 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-25 23:02:51,096 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-25 23:02:51,096 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-25 23:02:51,097 - INFO - Worker Worker(salt=9648748256, workers=1, host=MSI, username=alfayyedh, pid=168698) was stopped. Shutting down Keep-Alive thread
2024-12-25 23:02:51,098 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-26 00:00:02,211 - INFO - Read Load Query - SUCCESS
2024-12-26 00:00:03,309 - INFO - Read Extracted Data - SUCCESS
2024-12-26 00:00:03,350 - INFO - Connect to DWH - SUCCESS
2024-12-26 00:00:03,384 - ERROR - Truncate public Schema in DWH - FAILED
2024-12-26 00:00:03,412 - ERROR - [pid 191937] Worker Worker(salt=8605017289, workers=1, host=MSI, username=alfayyedh, pid=191937) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-12-26 00:00:03,429 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 00:00:03,434 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-26 00:00:03,434 - DEBUG - Asking scheduler for work...
2024-12-26 00:00:03,436 - DEBUG - Done
2024-12-26 00:00:03,436 - DEBUG - There are no more tasks to run at this time
2024-12-26 00:00:03,436 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-26 00:00:03,436 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-26 00:00:03,436 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-26 00:00:03,436 - INFO - Worker Worker(salt=8605017289, workers=1, host=MSI, username=alfayyedh, pid=191937) was stopped. Shutting down Keep-Alive thread
2024-12-26 00:00:03,437 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-26 02:00:16,397 - INFO - Read Load Query - SUCCESS
2024-12-26 02:00:17,466 - INFO - Read Extracted Data - SUCCESS
2024-12-26 02:00:17,508 - INFO - Connect to DWH - SUCCESS
2024-12-26 02:00:17,740 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-26 02:00:17,740 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-26 02:00:17,757 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-26 02:00:19,453 - INFO - LOAD 'public.products' - SUCCESS
2024-12-26 02:00:19,846 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-26 02:00:19,925 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-26 02:00:23,107 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-26 02:00:27,581 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-26 02:00:31,886 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-26 02:00:35,753 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-26 02:00:42,314 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-26 02:00:42,314 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-26 02:01:01,037 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-26 02:01:01,044 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-26 02:01:01,092 - INFO - [pid 240887] Worker Worker(salt=8052199569, workers=1, host=MSI, username=alfayyedh, pid=240887) done      Load()
2024-12-26 02:01:01,093 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 02:01:01,095 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-26 02:01:01,096 - DEBUG - Asking scheduler for work...
2024-12-26 02:01:01,097 - DEBUG - Pending tasks: 1
2024-12-26 02:01:01,097 - INFO - [pid 240887] Worker Worker(salt=8052199569, workers=1, host=MSI, username=alfayyedh, pid=240887) running   Transform()
2024-12-26 02:01:01,099 - INFO - Read Transform Query - SUCCESS
2024-12-26 02:01:01,101 - INFO - Connect to DWH - SUCCESS
2024-12-26 02:01:01,101 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-26 02:01:01,260 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-26 02:01:02,789 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-26 02:01:02,956 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-26 02:01:04,744 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-26 02:01:11,847 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-26 02:01:11,851 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-26 02:01:11,853 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-26 02:01:11,853 - INFO - [pid 240887] Worker Worker(salt=8052199569, workers=1, host=MSI, username=alfayyedh, pid=240887) done      Transform()
2024-12-26 02:01:11,853 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 02:01:11,856 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-26 02:01:11,856 - DEBUG - Asking scheduler for work...
2024-12-26 02:01:11,858 - DEBUG - Done
2024-12-26 02:01:11,858 - DEBUG - There are no more tasks to run at this time
2024-12-26 02:01:11,858 - INFO - Worker Worker(salt=8052199569, workers=1, host=MSI, username=alfayyedh, pid=240887) was stopped. Shutting down Keep-Alive thread
2024-12-26 02:01:11,859 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 2 ran successfully:
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-26 02:04:39,512 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-26 02:04:39,549 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-26 02:04:39,823 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-26 02:04:39,934 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-26 02:04:39,949 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-26 02:04:40,418 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-26 02:04:41,199 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-26 02:04:41,831 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-26 02:04:42,485 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-26 02:04:43,474 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-26 02:04:43,474 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-26 02:04:43,476 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-26 02:04:43,492 - INFO - [pid 243053] Worker Worker(salt=8659480750, workers=1, host=MSI, username=alfayyedh, pid=243053) done      Extract()
2024-12-26 02:04:43,494 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 02:04:43,498 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-26 02:04:43,498 - DEBUG - Asking scheduler for work...
2024-12-26 02:04:43,500 - DEBUG - Pending tasks: 2
2024-12-26 02:04:43,500 - INFO - [pid 243053] Worker Worker(salt=8659480750, workers=1, host=MSI, username=alfayyedh, pid=243053) running   Load()
2024-12-26 02:04:43,501 - INFO - Read Load Query - SUCCESS
2024-12-26 02:04:44,874 - INFO - Read Extracted Data - SUCCESS
2024-12-26 02:04:44,875 - INFO - Connect to DWH - SUCCESS
2024-12-26 02:04:45,125 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-26 02:04:45,126 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-26 02:04:45,143 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-26 02:04:46,835 - INFO - LOAD 'public.products' - SUCCESS
2024-12-26 02:04:47,214 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-26 02:04:47,291 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-26 02:04:50,449 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-26 02:04:55,209 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-26 02:04:59,146 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-26 02:05:02,814 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-26 02:05:09,553 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-26 02:05:09,553 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-26 02:05:31,315 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-26 02:05:31,318 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-26 02:05:31,385 - INFO - [pid 243053] Worker Worker(salt=8659480750, workers=1, host=MSI, username=alfayyedh, pid=243053) done      Load()
2024-12-26 02:05:31,385 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 02:05:31,388 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-26 02:05:31,388 - DEBUG - Asking scheduler for work...
2024-12-26 02:05:31,390 - DEBUG - Pending tasks: 1
2024-12-26 02:05:31,390 - INFO - [pid 243053] Worker Worker(salt=8659480750, workers=1, host=MSI, username=alfayyedh, pid=243053) running   Transform()
2024-12-26 02:05:31,391 - INFO - Read Transform Query - SUCCESS
2024-12-26 02:05:31,392 - INFO - Connect to DWH - SUCCESS
2024-12-26 02:05:31,392 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-26 02:05:31,733 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-26 02:05:34,885 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-26 02:05:35,340 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-26 02:05:36,886 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-26 02:05:45,457 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-26 02:05:45,462 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-26 02:05:45,465 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-26 02:05:45,465 - INFO - [pid 243053] Worker Worker(salt=8659480750, workers=1, host=MSI, username=alfayyedh, pid=243053) done      Transform()
2024-12-26 02:05:45,466 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 02:05:45,469 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-26 02:05:45,469 - DEBUG - Asking scheduler for work...
2024-12-26 02:05:45,471 - DEBUG - Done
2024-12-26 02:05:45,471 - DEBUG - There are no more tasks to run at this time
2024-12-26 02:05:45,472 - INFO - Worker Worker(salt=8659480750, workers=1, host=MSI, username=alfayyedh, pid=243053) was stopped. Shutting down Keep-Alive thread
2024-12-26 02:05:45,473 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-26 03:58:24,577 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-26 03:58:24,646 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-26 03:58:24,945 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-26 03:58:25,052 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-26 03:58:25,067 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-26 03:58:25,605 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-26 03:58:26,480 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-26 03:58:27,170 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-26 03:58:27,725 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-26 03:58:28,598 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-26 03:58:28,599 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-26 03:58:28,606 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-26 03:58:28,622 - INFO - [pid 290019] Worker Worker(salt=284865524, workers=1, host=MSI, username=alfayyedh, pid=290019) done      Extract()
2024-12-26 03:58:28,624 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 03:58:28,626 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-26 03:58:28,626 - DEBUG - Asking scheduler for work...
2024-12-26 03:58:28,628 - DEBUG - Pending tasks: 2
2024-12-26 03:58:28,629 - INFO - [pid 290019] Worker Worker(salt=284865524, workers=1, host=MSI, username=alfayyedh, pid=290019) running   Load()
2024-12-26 03:58:28,634 - INFO - Read Load Query - SUCCESS
2024-12-26 03:58:29,765 - INFO - Read Extracted Data - SUCCESS
2024-12-26 03:58:29,767 - INFO - Connect to DWH - SUCCESS
2024-12-26 03:58:29,975 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-26 03:58:29,975 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-26 03:58:29,987 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-26 03:58:31,682 - INFO - LOAD 'public.products' - SUCCESS
2024-12-26 03:58:32,064 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-26 03:58:32,146 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-26 03:58:35,045 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-26 03:58:40,056 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-26 03:58:44,524 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-26 03:58:48,173 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-26 03:58:55,329 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-26 03:58:55,330 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-26 03:59:12,713 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-26 03:59:12,716 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-26 03:59:12,764 - INFO - [pid 290019] Worker Worker(salt=284865524, workers=1, host=MSI, username=alfayyedh, pid=290019) done      Load()
2024-12-26 03:59:12,764 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 03:59:12,767 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-26 03:59:12,767 - DEBUG - Asking scheduler for work...
2024-12-26 03:59:12,769 - DEBUG - Pending tasks: 1
2024-12-26 03:59:12,769 - INFO - [pid 290019] Worker Worker(salt=284865524, workers=1, host=MSI, username=alfayyedh, pid=290019) running   Transform()
2024-12-26 03:59:12,770 - INFO - Read Transform Query - SUCCESS
2024-12-26 03:59:12,771 - INFO - Connect to DWH - SUCCESS
2024-12-26 03:59:12,771 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-26 03:59:12,900 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-26 03:59:14,608 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-26 03:59:15,136 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-26 03:59:16,984 - INFO - Transform to 'final.fct_order' - SUCCESS
2024-12-26 03:59:27,885 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-26 03:59:27,889 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-26 03:59:27,894 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-26 03:59:27,894 - INFO - [pid 290019] Worker Worker(salt=284865524, workers=1, host=MSI, username=alfayyedh, pid=290019) done      Transform()
2024-12-26 03:59:27,895 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-26 03:59:27,900 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-26 03:59:27,900 - DEBUG - Asking scheduler for work...
2024-12-26 03:59:27,902 - DEBUG - Done
2024-12-26 03:59:27,903 - DEBUG - There are no more tasks to run at this time
2024-12-26 03:59:27,903 - INFO - Worker Worker(salt=284865524, workers=1, host=MSI, username=alfayyedh, pid=290019) was stopped. Shutting down Keep-Alive thread
2024-12-26 03:59:27,905 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-28 17:05:07,293 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-28 17:05:07,420 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-28 17:05:07,915 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-28 17:05:08,056 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-28 17:05:08,073 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-28 17:05:08,943 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-28 17:05:10,282 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-28 17:05:11,098 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-28 17:05:11,646 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-28 17:05:12,621 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-28 17:05:12,621 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-28 17:05:12,624 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-28 17:05:12,640 - INFO - [pid 93539] Worker Worker(salt=1061744389, workers=1, host=MSI, username=alfayyedh, pid=93539) done      Extract()
2024-12-28 17:05:12,641 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:05:12,644 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-28 17:05:12,644 - DEBUG - Asking scheduler for work...
2024-12-28 17:05:12,646 - DEBUG - Pending tasks: 2
2024-12-28 17:05:12,646 - INFO - [pid 93539] Worker Worker(salt=1061744389, workers=1, host=MSI, username=alfayyedh, pid=93539) running   Load()
2024-12-28 17:05:12,652 - INFO - Read Load Query - SUCCESS
2024-12-28 17:05:14,049 - INFO - Read Extracted Data - SUCCESS
2024-12-28 17:05:14,051 - INFO - Connect to DWH - SUCCESS
2024-12-28 17:05:14,102 - ERROR - Truncate public Schema in DWH - FAILED
2024-12-28 17:05:14,143 - ERROR - [pid 93539] Worker Worker(salt=1061744389, workers=1, host=MSI, username=alfayyedh, pid=93539) failed    Load()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation "public.customers" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 145, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation "public.customers" does not exist

[SQL: TRUNCATE TABLE public.customers CASCADE]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/load.py", line 157, in run
    raise Exception("Failed to Truncate public Schema in DWH")
Exception: Failed to Truncate public Schema in DWH
2024-12-28 17:05:14,169 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:05:14,174 - INFO - Informed scheduler that task   Load__99914b932b   has status   FAILED
2024-12-28 17:05:14,174 - DEBUG - Asking scheduler for work...
2024-12-28 17:05:14,179 - DEBUG - Done
2024-12-28 17:05:14,179 - DEBUG - There are no more tasks to run at this time
2024-12-28 17:05:14,179 - DEBUG - There are 2 pending tasks possibly being run by other workers
2024-12-28 17:05:14,179 - DEBUG - There are 2 pending tasks unique to this worker
2024-12-28 17:05:14,179 - DEBUG - There are 2 pending tasks last scheduled by this worker
2024-12-28 17:05:14,180 - INFO - Worker Worker(salt=1061744389, workers=1, host=MSI, username=alfayyedh, pid=93539) was stopped. Shutting down Keep-Alive thread
2024-12-28 17:05:14,181 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 ran successfully:
    - 1 Extract()
* 1 failed:
    - 1 Load()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-28 17:16:39,266 - INFO - Read Load Query - SUCCESS
2024-12-28 17:16:41,503 - INFO - Read Extracted Data - SUCCESS
2024-12-28 17:16:41,554 - INFO - Connect to DWH - SUCCESS
2024-12-28 17:16:44,174 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-28 17:16:44,174 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-28 17:16:44,213 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-28 17:16:46,689 - INFO - LOAD 'public.products' - SUCCESS
2024-12-28 17:16:47,094 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-28 17:16:47,176 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-28 17:16:51,700 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-28 17:16:57,830 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-28 17:17:02,353 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-28 17:17:06,261 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-28 17:17:13,511 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-28 17:17:13,511 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-28 17:17:34,125 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-28 17:17:34,139 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-28 17:17:34,215 - INFO - [pid 98817] Worker Worker(salt=4880773074, workers=1, host=MSI, username=alfayyedh, pid=98817) done      Load()
2024-12-28 17:17:34,217 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:17:34,219 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-28 17:17:34,220 - DEBUG - Asking scheduler for work...
2024-12-28 17:17:34,221 - DEBUG - Pending tasks: 1
2024-12-28 17:17:34,221 - INFO - [pid 98817] Worker Worker(salt=4880773074, workers=1, host=MSI, username=alfayyedh, pid=98817) running   Transform()
2024-12-28 17:17:34,225 - INFO - Read Transform Query - SUCCESS
2024-12-28 17:17:34,227 - INFO - Connect to DWH - SUCCESS
2024-12-28 17:17:34,227 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-28 17:17:34,378 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-28 17:17:36,117 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-28 17:17:36,282 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-28 17:17:36,341 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-28 17:17:36,402 - ERROR - Transform Tables - FAILED
2024-12-28 17:17:36,407 - ERROR - [pid 98817] Worker Worker(salt=4880773074, workers=1, host=MSI, username=alfayyedh, pid=98817) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.SyntaxError: syntax error at or near ","
LINE 19:     dd3.date_id AS order_delivered_carrier_date,,
                                                         ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.SyntaxError) syntax error at or near ","
LINE 19:     dd3.date_id AS order_delivered_carrier_date,,
                                                         ^

[SQL: INSERT INTO final.fct_order_delivery (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_purchase_date,
    order_approved_at_date,
    order_delivered_carrier_date,
    order_delivered_customer_date,
    order_estimated_delivery_date,
    day_process,
    day_success
)

select 
    so.order_id,
    dc.customer_id,
    dd1.date_id AS order_purchase_date,
    dd2.date_id AS order_approved_at_date,
    dd3.date_id AS order_delivered_carrier_date,,
    dd4.date_id AS order_delivered_customer_date,
    dd5.date_id AS order_estimated_delivery_date,
    
    -- Calculate day process using direct date subtraction
    (TO_DATE(dd3.date_id::text, 'YYYYMMDD') - TO_DATE(dd1.date_id::text, 'YYYYMMDD')) AS day_process,

    -- Calculate day success
    (TO_DATE(dd4.date_id::text, 'YYYYMMDD') - TO_DATE(dd1.date_id::text, 'YYYYMMDD')) AS day_success
	FROM 
	    stg.orders so
	JOIN
	    final.dim_customer dc ON so.customer_id = dc.customer_nk
	JOIN 
	    final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order_purchase_timestamp::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd2 ON dd2.date_actual = TO_DATE(so.order_approved_at::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd3 ON dd3.date_actual = TO_DATE(so.order_delivered_carrier_date::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd4 ON dd4.date_actual = TO_DATE(so.order_delivered_customer_date::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd5 ON dd5.date_actual = TO_DATE(so.order_estimated_delivery_date::text, 'YYYY-MM-DD')

ON CONFLICT(order_delivery_id, order_id, customer_id) 
DO UPDATE SET
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    updated_at = CASE WHEN 
                        final.fct_order_delivery.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order_delivery.approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order_delivery.delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order_delivery.delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order_delivery.estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order_delivery.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-28 17:17:36,434 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:17:36,442 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-28 17:17:36,443 - DEBUG - Asking scheduler for work...
2024-12-28 17:17:36,446 - DEBUG - Done
2024-12-28 17:17:36,446 - DEBUG - There are no more tasks to run at this time
2024-12-28 17:17:36,446 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-28 17:17:36,446 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-28 17:17:36,446 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-28 17:17:36,446 - INFO - Worker Worker(salt=4880773074, workers=1, host=MSI, username=alfayyedh, pid=98817) was stopped. Shutting down Keep-Alive thread
2024-12-28 17:17:36,448 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 Extract()
* 1 ran successfully:
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-28 17:18:28,441 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-28 17:18:28,483 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-28 17:18:28,746 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-28 17:18:28,844 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-28 17:18:28,860 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-28 17:18:29,392 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-28 17:18:30,394 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-28 17:18:31,108 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-28 17:18:31,694 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-28 17:18:32,651 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-28 17:18:32,651 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-28 17:18:32,653 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-28 17:18:32,667 - INFO - [pid 99886] Worker Worker(salt=6038754211, workers=1, host=MSI, username=alfayyedh, pid=99886) done      Extract()
2024-12-28 17:18:32,668 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:18:32,671 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-28 17:18:32,671 - DEBUG - Asking scheduler for work...
2024-12-28 17:18:32,672 - DEBUG - Pending tasks: 2
2024-12-28 17:18:32,672 - INFO - [pid 99886] Worker Worker(salt=6038754211, workers=1, host=MSI, username=alfayyedh, pid=99886) running   Load()
2024-12-28 17:18:32,673 - INFO - Read Load Query - SUCCESS
2024-12-28 17:18:34,008 - INFO - Read Extracted Data - SUCCESS
2024-12-28 17:18:34,009 - INFO - Connect to DWH - SUCCESS
2024-12-28 17:18:34,250 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-28 17:18:34,250 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-28 17:18:34,262 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-28 17:18:36,023 - INFO - LOAD 'public.products' - SUCCESS
2024-12-28 17:18:36,430 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-28 17:18:36,507 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-28 17:18:39,831 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-28 17:18:46,547 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-28 17:18:51,951 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-28 17:18:56,299 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-28 17:19:03,253 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-28 17:19:03,253 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-28 17:19:24,019 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-28 17:19:24,021 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-28 17:19:24,078 - INFO - [pid 99886] Worker Worker(salt=6038754211, workers=1, host=MSI, username=alfayyedh, pid=99886) done      Load()
2024-12-28 17:19:24,078 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:19:24,081 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-28 17:19:24,081 - DEBUG - Asking scheduler for work...
2024-12-28 17:19:24,083 - DEBUG - Pending tasks: 1
2024-12-28 17:19:24,083 - INFO - [pid 99886] Worker Worker(salt=6038754211, workers=1, host=MSI, username=alfayyedh, pid=99886) running   Transform()
2024-12-28 17:19:24,087 - INFO - Read Transform Query - SUCCESS
2024-12-28 17:19:24,088 - INFO - Connect to DWH - SUCCESS
2024-12-28 17:19:24,088 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-28 17:19:24,326 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-28 17:19:26,051 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-28 17:19:27,498 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-28 17:19:27,652 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-28 17:19:27,688 - ERROR - Transform Tables - FAILED
2024-12-28 17:19:27,694 - ERROR - [pid 99886] Worker Worker(salt=6038754211, workers=1, host=MSI, username=alfayyedh, pid=99886) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DatatypeMismatch: column "order_id" is of type uuid but expression is of type text
LINE 15:     so.order_id,
             ^
HINT:  You will need to rewrite or cast the expression.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DatatypeMismatch) column "order_id" is of type uuid but expression is of type text
LINE 15:     so.order_id,
             ^
HINT:  You will need to rewrite or cast the expression.

[SQL: INSERT INTO final.fct_order_delivery (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_purchase_date,
    order_approved_at_date,
    order_delivered_carrier_date,
    order_delivered_customer_date,
    order_estimated_delivery_date,
    day_process,
    day_success
)

select 
    so.order_id,
    dc.customer_id,
    dd1.date_id AS order_purchase_date,
    dd2.date_id AS order_approved_at_date,
    dd3.date_id AS order_delivered_carrier_date,
    dd4.date_id AS order_delivered_customer_date,
    dd5.date_id AS order_estimated_delivery_date,
    
    -- Calculate day process using direct date subtraction
    (TO_DATE(dd3.date_id::text, 'YYYYMMDD') - TO_DATE(dd1.date_id::text, 'YYYYMMDD')) AS day_process,

    -- Calculate day success
    (TO_DATE(dd4.date_id::text, 'YYYYMMDD') - TO_DATE(dd1.date_id::text, 'YYYYMMDD')) AS day_success
	FROM 
	    stg.orders so
	JOIN
	    final.dim_customer dc ON so.customer_id = dc.customer_nk
	JOIN 
	    final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order_purchase_timestamp::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd2 ON dd2.date_actual = TO_DATE(so.order_approved_at::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd3 ON dd3.date_actual = TO_DATE(so.order_delivered_carrier_date::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd4 ON dd4.date_actual = TO_DATE(so.order_delivered_customer_date::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd5 ON dd5.date_actual = TO_DATE(so.order_estimated_delivery_date::text, 'YYYY-MM-DD')

ON CONFLICT(order_delivery_id, order_id, customer_id) 
DO UPDATE SET
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    updated_at = CASE WHEN 
                        final.fct_order_delivery.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order_delivery.approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order_delivery.delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order_delivery.delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order_delivery.estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order_delivery.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-28 17:19:27,724 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:19:27,730 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-28 17:19:27,730 - DEBUG - Asking scheduler for work...
2024-12-28 17:19:27,733 - DEBUG - Done
2024-12-28 17:19:27,734 - DEBUG - There are no more tasks to run at this time
2024-12-28 17:19:27,734 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-28 17:19:27,734 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-28 17:19:27,734 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-28 17:19:27,735 - INFO - Worker Worker(salt=6038754211, workers=1, host=MSI, username=alfayyedh, pid=99886) was stopped. Shutting down Keep-Alive thread
2024-12-28 17:19:27,738 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-28 17:20:31,448 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-28 17:20:31,488 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-28 17:20:31,775 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-28 17:20:31,871 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-28 17:20:31,883 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-28 17:20:32,387 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-28 17:20:33,229 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-28 17:20:33,848 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-28 17:20:34,324 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-28 17:20:35,164 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-28 17:20:35,164 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-28 17:20:35,165 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-28 17:20:35,182 - INFO - [pid 101066] Worker Worker(salt=6237320169, workers=1, host=MSI, username=alfayyedh, pid=101066) done      Extract()
2024-12-28 17:20:35,190 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:20:35,193 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-28 17:20:35,193 - DEBUG - Asking scheduler for work...
2024-12-28 17:20:35,195 - DEBUG - Pending tasks: 2
2024-12-28 17:20:35,195 - INFO - [pid 101066] Worker Worker(salt=6237320169, workers=1, host=MSI, username=alfayyedh, pid=101066) running   Load()
2024-12-28 17:20:35,196 - INFO - Read Load Query - SUCCESS
2024-12-28 17:20:36,413 - INFO - Read Extracted Data - SUCCESS
2024-12-28 17:20:36,414 - INFO - Connect to DWH - SUCCESS
2024-12-28 17:20:36,631 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-28 17:20:36,631 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-28 17:20:36,644 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-28 17:20:38,333 - INFO - LOAD 'public.products' - SUCCESS
2024-12-28 17:20:38,752 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-28 17:20:38,830 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-28 17:20:41,787 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-28 17:20:46,170 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-28 17:20:49,933 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-28 17:20:53,333 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-28 17:21:00,170 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-28 17:21:00,170 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-28 17:21:14,585 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-28 17:21:14,587 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-28 17:21:14,645 - INFO - [pid 101066] Worker Worker(salt=6237320169, workers=1, host=MSI, username=alfayyedh, pid=101066) done      Load()
2024-12-28 17:21:14,645 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:21:14,649 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-28 17:21:14,650 - DEBUG - Asking scheduler for work...
2024-12-28 17:21:14,652 - DEBUG - Pending tasks: 1
2024-12-28 17:21:14,652 - INFO - [pid 101066] Worker Worker(salt=6237320169, workers=1, host=MSI, username=alfayyedh, pid=101066) running   Transform()
2024-12-28 17:21:14,653 - INFO - Read Transform Query - SUCCESS
2024-12-28 17:21:14,654 - INFO - Connect to DWH - SUCCESS
2024-12-28 17:21:14,654 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-28 17:21:14,883 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-28 17:21:16,139 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-28 17:21:17,567 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-28 17:21:17,581 - ERROR - Transform to All Dimensions and Fact Tables - FAILED
2024-12-28 17:21:17,624 - ERROR - Transform Tables - FAILED
2024-12-28 17:21:17,629 - ERROR - [pid 101066] Worker Worker(salt=6237320169, workers=1, host=MSI, username=alfayyedh, pid=101066) failed    Transform()
Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column fct_order_delivery.approved_at_date does not exist
LINE 52:                         OR final.fct_order_delivery.approved...
                                    ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 111, in run
    session.execute(query)
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2306, in execute
    return self._execute_internal(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2200, in _execute_internal
    result = conn.execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1421, in execute
    return meth(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1849, in _execute_context
    return self._exec_single_context(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column fct_order_delivery.approved_at_date does not exist
LINE 52:                         OR final.fct_order_delivery.approved...
                                    ^

[SQL: INSERT INTO final.fct_order_delivery (
    -- fct_order_id is generated by UUID
    order_id,
    customer_id,
    order_purchase_date,
    order_approved_at_date,
    order_delivered_carrier_date,
    order_delivered_customer_date,
    order_estimated_delivery_date,
    day_process,
    day_success
)

select 
    so.order_id::uuid,
    dc.customer_id,
    dd1.date_id AS order_purchase_date,
    dd2.date_id AS order_approved_at_date,
    dd3.date_id AS order_delivered_carrier_date,
    dd4.date_id AS order_delivered_customer_date,
    dd5.date_id AS order_estimated_delivery_date,
    
    -- Calculate day process using direct date subtraction
    (TO_DATE(dd3.date_id::text, 'YYYYMMDD') - TO_DATE(dd1.date_id::text, 'YYYYMMDD')) AS day_process,

    -- Calculate day success
    (TO_DATE(dd4.date_id::text, 'YYYYMMDD') - TO_DATE(dd1.date_id::text, 'YYYYMMDD')) AS day_success
	FROM 
	    stg.orders so
	JOIN
	    final.dim_customer dc ON so.customer_id = dc.customer_nk
	JOIN 
	    final.dim_date dd1 ON dd1.date_actual = TO_DATE(so.order_purchase_timestamp::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd2 ON dd2.date_actual = TO_DATE(so.order_approved_at::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd3 ON dd3.date_actual = TO_DATE(so.order_delivered_carrier_date::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd4 ON dd4.date_actual = TO_DATE(so.order_delivered_customer_date::text, 'YYYY-MM-DD')
	JOIN 
	    final.dim_date dd5 ON dd5.date_actual = TO_DATE(so.order_estimated_delivery_date::text, 'YYYY-MM-DD')

ON CONFLICT(order_delivery_id, order_id, customer_id) 
DO UPDATE SET
    order_purchase_date = EXCLUDED.order_purchase_date,
    order_approved_at_date = EXCLUDED.order_approved_at_date,
    order_delivered_carrier_date = EXCLUDED.order_delivered_carrier_date,
    order_delivered_customer_date = EXCLUDED.order_delivered_customer_date,
    order_estimated_delivery_date = EXCLUDED.order_estimated_delivery_date,
    updated_at = CASE WHEN 
                        final.fct_order_delivery.order_purchase_date <> EXCLUDED.order_purchase_date
                        OR final.fct_order_delivery.approved_at_date <> EXCLUDED.order_approved_at_date
                        OR final.fct_order_delivery.delivered_carrier_date <> EXCLUDED.order_delivered_carrier_date
                        OR final.fct_order_delivery.delivered_customer_date <> EXCLUDED.order_delivered_customer_date
                        OR final.fct_order_delivery.estimated_delivery_date <> EXCLUDED.order_estimated_delivery_date
                THEN 
                        CURRENT_TIMESTAMP
                ELSE
                        final.fct_order_delivery.updated_at
                END;]
(Background on this error at: https://sqlalche.me/e/20/f405)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 210, in run
    new_deps = self._run_get_new_deps()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/venv/lib/python3.10/site-packages/luigi/worker.py", line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File "/home/alfayyedh/pacmann/data-warehouse/dataset-olist/pipeline/transform.py", line 163, in run
    raise Exception('Failed Transforming Tables')
Exception: Failed Transforming Tables
2024-12-28 17:21:17,655 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 17:21:17,661 - INFO - Informed scheduler that task   Transform__99914b932b   has status   FAILED
2024-12-28 17:21:17,661 - DEBUG - Asking scheduler for work...
2024-12-28 17:21:17,663 - DEBUG - Done
2024-12-28 17:21:17,663 - DEBUG - There are no more tasks to run at this time
2024-12-28 17:21:17,663 - DEBUG - There are 1 pending tasks possibly being run by other workers
2024-12-28 17:21:17,663 - DEBUG - There are 1 pending tasks unique to this worker
2024-12-28 17:21:17,663 - DEBUG - There are 1 pending tasks last scheduled by this worker
2024-12-28 17:21:17,663 - INFO - Worker Worker(salt=6237320169, workers=1, host=MSI, username=alfayyedh, pid=101066) was stopped. Shutting down Keep-Alive thread
2024-12-28 17:21:17,667 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 ran successfully:
    - 1 Extract()
    - 1 Load()
* 1 failed:
    - 1 Transform()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

2024-12-28 21:29:27,252 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-28 21:29:27,517 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-28 21:29:28,348 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-28 21:29:28,482 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-28 21:29:28,509 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-28 21:29:30,532 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-28 21:29:35,397 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-28 21:29:36,896 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-28 21:29:37,804 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-28 21:29:39,625 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-28 21:29:39,625 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-28 21:29:39,645 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-28 21:29:39,666 - INFO - [pid 202182] Worker Worker(salt=8551573351, workers=1, host=MSI, username=alfayyedh, pid=202182) done      Extract()
2024-12-28 21:29:39,667 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 21:29:39,671 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-28 21:29:39,671 - DEBUG - Asking scheduler for work...
2024-12-28 21:29:39,673 - DEBUG - Pending tasks: 2
2024-12-28 21:29:39,673 - INFO - [pid 202182] Worker Worker(salt=8551573351, workers=1, host=MSI, username=alfayyedh, pid=202182) running   Load()
2024-12-28 21:29:39,676 - INFO - Read Load Query - SUCCESS
2024-12-28 21:29:43,352 - INFO - Read Extracted Data - SUCCESS
2024-12-28 21:29:43,353 - INFO - Connect to DWH - SUCCESS
2024-12-28 21:29:43,831 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-28 21:29:43,831 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-28 21:29:43,891 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-28 21:29:48,686 - INFO - LOAD 'public.products' - SUCCESS
2024-12-28 21:29:49,309 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-28 21:29:49,446 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-28 21:29:55,859 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-28 21:30:05,017 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-28 21:30:11,136 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-28 21:30:15,151 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-28 21:30:24,975 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-28 21:30:24,975 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-28 21:30:48,373 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-28 21:30:48,384 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-28 21:30:48,549 - INFO - [pid 202182] Worker Worker(salt=8551573351, workers=1, host=MSI, username=alfayyedh, pid=202182) done      Load()
2024-12-28 21:30:48,550 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 21:30:48,554 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-28 21:30:48,554 - DEBUG - Asking scheduler for work...
2024-12-28 21:30:48,556 - DEBUG - Pending tasks: 1
2024-12-28 21:30:48,556 - INFO - [pid 202182] Worker Worker(salt=8551573351, workers=1, host=MSI, username=alfayyedh, pid=202182) running   Transform()
2024-12-28 21:30:48,557 - INFO - Read Transform Query - SUCCESS
2024-12-28 21:30:48,558 - INFO - Connect to DWH - SUCCESS
2024-12-28 21:30:48,558 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-28 21:30:48,871 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-28 21:30:50,412 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-28 21:30:50,912 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-28 21:30:57,882 - INFO - Transform to 'final.fct_order_delivery' - SUCCESS
2024-12-28 21:31:03,905 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-28 21:31:03,909 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-28 21:31:03,910 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-28 21:31:03,911 - INFO - [pid 202182] Worker Worker(salt=8551573351, workers=1, host=MSI, username=alfayyedh, pid=202182) done      Transform()
2024-12-28 21:31:03,911 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 21:31:03,913 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-28 21:31:03,913 - DEBUG - Asking scheduler for work...
2024-12-28 21:31:03,915 - DEBUG - Done
2024-12-28 21:31:03,915 - DEBUG - There are no more tasks to run at this time
2024-12-28 21:31:03,915 - INFO - Worker Worker(salt=8551573351, workers=1, host=MSI, username=alfayyedh, pid=202182) was stopped. Shutting down Keep-Alive thread
2024-12-28 21:31:03,918 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-28 23:13:03,301 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-28 23:13:03,568 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-28 23:13:04,026 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-28 23:13:04,177 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-28 23:13:04,222 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-28 23:13:05,154 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-28 23:13:09,253 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-28 23:13:11,015 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-28 23:13:12,087 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-28 23:13:14,160 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-28 23:13:14,160 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-28 23:13:14,188 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-28 23:13:14,206 - INFO - [pid 244808] Worker Worker(salt=380434806, workers=1, host=MSI, username=alfayyedh, pid=244808) done      Extract()
2024-12-28 23:13:14,207 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:13:14,212 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-28 23:13:14,212 - DEBUG - Asking scheduler for work...
2024-12-28 23:13:14,214 - DEBUG - Pending tasks: 2
2024-12-28 23:13:14,214 - INFO - [pid 244808] Worker Worker(salt=380434806, workers=1, host=MSI, username=alfayyedh, pid=244808) running   Load()
2024-12-28 23:13:14,218 - INFO - Read Load Query - SUCCESS
2024-12-28 23:13:17,637 - INFO - Read Extracted Data - SUCCESS
2024-12-28 23:13:17,638 - INFO - Connect to DWH - SUCCESS
2024-12-28 23:13:18,147 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-28 23:13:18,147 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-28 23:13:18,191 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-28 23:13:27,445 - INFO - LOAD 'public.products' - SUCCESS
2024-12-28 23:13:27,968 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-28 23:13:28,100 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-28 23:13:35,514 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-28 23:13:46,268 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-28 23:13:55,311 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-28 23:14:00,937 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-28 23:14:13,292 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-28 23:14:13,292 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-28 23:14:49,418 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-28 23:14:49,430 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-28 23:14:49,616 - INFO - [pid 244808] Worker Worker(salt=380434806, workers=1, host=MSI, username=alfayyedh, pid=244808) done      Load()
2024-12-28 23:14:49,617 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:14:49,620 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-28 23:14:49,621 - DEBUG - Asking scheduler for work...
2024-12-28 23:14:49,623 - DEBUG - Pending tasks: 1
2024-12-28 23:14:49,623 - INFO - [pid 244808] Worker Worker(salt=380434806, workers=1, host=MSI, username=alfayyedh, pid=244808) running   Transform()
2024-12-28 23:14:49,626 - INFO - Read Transform Query - SUCCESS
2024-12-28 23:14:49,627 - INFO - Connect to DWH - SUCCESS
2024-12-28 23:14:49,627 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-28 23:14:50,070 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-28 23:14:52,341 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-28 23:14:53,241 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-28 23:15:02,055 - INFO - Transform to 'final.fct_order_delivery' - SUCCESS
2024-12-28 23:15:08,635 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-28 23:15:08,638 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-28 23:15:08,641 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-28 23:15:08,641 - INFO - [pid 244808] Worker Worker(salt=380434806, workers=1, host=MSI, username=alfayyedh, pid=244808) done      Transform()
2024-12-28 23:15:08,641 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:15:08,645 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-28 23:15:08,645 - DEBUG - Asking scheduler for work...
2024-12-28 23:15:08,647 - DEBUG - Done
2024-12-28 23:15:08,647 - DEBUG - There are no more tasks to run at this time
2024-12-28 23:15:08,648 - INFO - Worker Worker(salt=380434806, workers=1, host=MSI, username=alfayyedh, pid=244808) was stopped. Shutting down Keep-Alive thread
2024-12-28 23:15:08,651 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-28 23:27:36,262 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-28 23:27:36,397 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-28 23:27:36,752 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-28 23:27:36,967 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-28 23:27:36,992 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-28 23:27:37,788 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-28 23:27:39,860 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-28 23:27:42,438 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-28 23:27:43,225 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-28 23:27:45,324 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-28 23:27:45,324 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-28 23:27:45,345 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-28 23:27:45,366 - INFO - [pid 251445] Worker Worker(salt=6018341463, workers=1, host=MSI, username=alfayyedh, pid=251445) done      Extract()
2024-12-28 23:27:45,367 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:27:45,372 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-28 23:27:45,373 - DEBUG - Asking scheduler for work...
2024-12-28 23:27:45,375 - DEBUG - Pending tasks: 2
2024-12-28 23:27:45,375 - INFO - [pid 251445] Worker Worker(salt=6018341463, workers=1, host=MSI, username=alfayyedh, pid=251445) running   Load()
2024-12-28 23:27:45,380 - INFO - Read Load Query - SUCCESS
2024-12-28 23:27:49,521 - INFO - Read Extracted Data - SUCCESS
2024-12-28 23:27:49,523 - INFO - Connect to DWH - SUCCESS
2024-12-28 23:27:50,205 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-28 23:27:50,205 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-28 23:27:50,248 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-28 23:27:55,207 - INFO - LOAD 'public.products' - SUCCESS
2024-12-28 23:27:55,838 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-28 23:27:56,016 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-28 23:28:03,210 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-28 23:28:13,633 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-28 23:28:22,195 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-28 23:28:27,061 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-28 23:28:36,783 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-28 23:28:36,783 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-28 23:29:09,021 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-28 23:29:09,028 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-28 23:29:09,146 - INFO - [pid 251445] Worker Worker(salt=6018341463, workers=1, host=MSI, username=alfayyedh, pid=251445) done      Load()
2024-12-28 23:29:09,147 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:29:09,151 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-28 23:29:09,151 - DEBUG - Asking scheduler for work...
2024-12-28 23:29:09,153 - DEBUG - Pending tasks: 1
2024-12-28 23:29:09,154 - INFO - [pid 251445] Worker Worker(salt=6018341463, workers=1, host=MSI, username=alfayyedh, pid=251445) running   Transform()
2024-12-28 23:29:09,155 - INFO - Read Transform Query - SUCCESS
2024-12-28 23:29:09,156 - INFO - Connect to DWH - SUCCESS
2024-12-28 23:29:09,156 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-28 23:29:09,573 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-28 23:29:11,523 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-28 23:29:12,132 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-28 23:29:22,462 - INFO - Transform to 'final.fct_order_delivery' - SUCCESS
2024-12-28 23:29:30,694 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-28 23:29:30,709 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-28 23:29:30,712 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-28 23:29:30,712 - INFO - [pid 251445] Worker Worker(salt=6018341463, workers=1, host=MSI, username=alfayyedh, pid=251445) done      Transform()
2024-12-28 23:29:30,713 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:29:30,715 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-28 23:29:30,716 - DEBUG - Asking scheduler for work...
2024-12-28 23:29:30,717 - DEBUG - Done
2024-12-28 23:29:30,717 - DEBUG - There are no more tasks to run at this time
2024-12-28 23:29:30,717 - INFO - Worker Worker(salt=6018341463, workers=1, host=MSI, username=alfayyedh, pid=251445) was stopped. Shutting down Keep-Alive thread
2024-12-28 23:29:30,722 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-28 23:35:21,548 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-28 23:35:21,715 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-28 23:35:22,066 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-28 23:35:22,256 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-28 23:35:22,288 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-28 23:35:23,050 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-28 23:35:24,993 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-28 23:35:26,400 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-28 23:35:27,364 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-28 23:35:29,103 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-28 23:35:29,104 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-28 23:35:29,124 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-28 23:35:29,141 - INFO - [pid 255098] Worker Worker(salt=5126216698, workers=1, host=MSI, username=alfayyedh, pid=255098) done      Extract()
2024-12-28 23:35:29,143 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:35:29,147 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-28 23:35:29,147 - DEBUG - Asking scheduler for work...
2024-12-28 23:35:29,149 - DEBUG - Pending tasks: 2
2024-12-28 23:35:29,149 - INFO - [pid 255098] Worker Worker(salt=5126216698, workers=1, host=MSI, username=alfayyedh, pid=255098) running   Load()
2024-12-28 23:35:29,152 - INFO - Read Load Query - SUCCESS
2024-12-28 23:35:33,519 - INFO - Read Extracted Data - SUCCESS
2024-12-28 23:35:33,520 - INFO - Connect to DWH - SUCCESS
2024-12-28 23:35:33,932 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-28 23:35:33,933 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-28 23:35:33,969 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-28 23:35:38,956 - INFO - LOAD 'public.products' - SUCCESS
2024-12-28 23:35:39,507 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-28 23:35:39,638 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-28 23:35:44,939 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-28 23:35:53,938 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-28 23:35:59,660 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-28 23:36:04,542 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-28 23:36:14,927 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-28 23:36:14,928 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-28 23:36:44,525 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-28 23:36:44,529 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-28 23:36:44,622 - INFO - [pid 255098] Worker Worker(salt=5126216698, workers=1, host=MSI, username=alfayyedh, pid=255098) done      Load()
2024-12-28 23:36:44,623 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:36:44,626 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-28 23:36:44,626 - DEBUG - Asking scheduler for work...
2024-12-28 23:36:44,628 - DEBUG - Pending tasks: 1
2024-12-28 23:36:44,628 - INFO - [pid 255098] Worker Worker(salt=5126216698, workers=1, host=MSI, username=alfayyedh, pid=255098) running   Transform()
2024-12-28 23:36:44,629 - INFO - Read Transform Query - SUCCESS
2024-12-28 23:36:44,629 - INFO - Connect to DWH - SUCCESS
2024-12-28 23:36:44,630 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-28 23:36:45,047 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-28 23:36:48,046 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-28 23:36:48,550 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-28 23:36:57,965 - INFO - Transform to 'final.fct_order_delivery' - SUCCESS
2024-12-28 23:37:04,969 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-28 23:37:04,972 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-28 23:37:04,974 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-28 23:37:04,974 - INFO - [pid 255098] Worker Worker(salt=5126216698, workers=1, host=MSI, username=alfayyedh, pid=255098) done      Transform()
2024-12-28 23:37:04,975 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-28 23:37:04,977 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-28 23:37:04,977 - DEBUG - Asking scheduler for work...
2024-12-28 23:37:04,979 - DEBUG - Done
2024-12-28 23:37:04,979 - DEBUG - There are no more tasks to run at this time
2024-12-28 23:37:04,979 - INFO - Worker Worker(salt=5126216698, workers=1, host=MSI, username=alfayyedh, pid=255098) was stopped. Shutting down Keep-Alive thread
2024-12-28 23:37:04,981 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-29 00:00:04,505 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-29 00:00:04,774 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-29 00:00:05,289 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-29 00:00:05,449 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-29 00:00:05,484 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-29 00:00:06,647 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-29 00:00:11,899 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-29 00:00:13,974 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-29 00:00:15,121 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-29 00:00:17,554 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-29 00:00:17,554 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-29 00:00:17,577 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-29 00:00:17,600 - INFO - [pid 265676] Worker Worker(salt=7211262060, workers=1, host=MSI, username=alfayyedh, pid=265676) done      Extract()
2024-12-29 00:00:17,603 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-29 00:00:17,607 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-29 00:00:17,607 - DEBUG - Asking scheduler for work...
2024-12-29 00:00:17,610 - DEBUG - Pending tasks: 2
2024-12-29 00:00:17,611 - INFO - [pid 265676] Worker Worker(salt=7211262060, workers=1, host=MSI, username=alfayyedh, pid=265676) running   Load()
2024-12-29 00:00:17,613 - INFO - Read Load Query - SUCCESS
2024-12-29 00:00:22,814 - INFO - Read Extracted Data - SUCCESS
2024-12-29 00:00:22,816 - INFO - Connect to DWH - SUCCESS
2024-12-29 00:00:23,239 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-29 00:00:23,239 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-29 00:00:23,312 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-29 00:00:27,890 - INFO - LOAD 'public.products' - SUCCESS
2024-12-29 00:00:28,368 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-29 00:00:28,453 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-29 00:00:35,751 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-29 00:00:44,509 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-29 00:00:50,937 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-29 00:00:55,545 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-29 00:01:06,186 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-29 00:01:06,187 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-29 00:01:38,168 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-29 00:01:38,188 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-29 00:01:38,276 - INFO - [pid 265676] Worker Worker(salt=7211262060, workers=1, host=MSI, username=alfayyedh, pid=265676) done      Load()
2024-12-29 00:01:38,279 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-29 00:01:38,286 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-29 00:01:38,286 - DEBUG - Asking scheduler for work...
2024-12-29 00:01:38,288 - DEBUG - Pending tasks: 1
2024-12-29 00:01:38,288 - INFO - [pid 265676] Worker Worker(salt=7211262060, workers=1, host=MSI, username=alfayyedh, pid=265676) running   Transform()
2024-12-29 00:01:38,291 - INFO - Read Transform Query - SUCCESS
2024-12-29 00:01:38,293 - INFO - Connect to DWH - SUCCESS
2024-12-29 00:01:38,293 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-29 00:01:38,653 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-29 00:01:40,546 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-29 00:01:40,983 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-29 00:01:51,661 - INFO - Transform to 'final.fct_order_delivery' - SUCCESS
2024-12-29 00:02:03,308 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-29 00:02:03,312 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-29 00:02:03,317 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-29 00:02:03,317 - INFO - [pid 265676] Worker Worker(salt=7211262060, workers=1, host=MSI, username=alfayyedh, pid=265676) done      Transform()
2024-12-29 00:02:03,318 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-29 00:02:03,320 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-29 00:02:03,320 - DEBUG - Asking scheduler for work...
2024-12-29 00:02:03,323 - DEBUG - Done
2024-12-29 00:02:03,323 - DEBUG - There are no more tasks to run at this time
2024-12-29 00:02:03,323 - INFO - Worker Worker(salt=7211262060, workers=1, host=MSI, username=alfayyedh, pid=265676) was stopped. Shutting down Keep-Alive thread
2024-12-29 00:02:03,328 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

2024-12-29 02:12:01,459 - INFO - ==================================STARTING EXTRACT DATA=======================================
2024-12-29 02:12:01,505 - INFO - EXTRACT 'public.product_category_name_translation' - SUCCESS.
2024-12-29 02:12:01,771 - INFO - EXTRACT 'public.products' - SUCCESS.
2024-12-29 02:12:01,878 - INFO - EXTRACT 'public.geolocation' - SUCCESS.
2024-12-29 02:12:01,892 - INFO - EXTRACT 'public.sellers' - SUCCESS.
2024-12-29 02:12:02,412 - INFO - EXTRACT 'public.customers' - SUCCESS.
2024-12-29 02:12:03,200 - INFO - EXTRACT 'public.orders' - SUCCESS.
2024-12-29 02:12:03,890 - INFO - EXTRACT 'public.order_reviews' - SUCCESS.
2024-12-29 02:12:04,436 - INFO - EXTRACT 'public.order_payments' - SUCCESS.
2024-12-29 02:12:05,395 - INFO - EXTRACT 'public.order_items' - SUCCESS.
2024-12-29 02:12:05,396 - INFO - Extract All Tables From Sources - SUCCESS
2024-12-29 02:12:05,402 - INFO - ==================================ENDING EXTRACT DATA=======================================
2024-12-29 02:12:05,420 - INFO - [pid 320990] Worker Worker(salt=6036307926, workers=1, host=MSI, username=alfayyedh, pid=320990) done      Extract()
2024-12-29 02:12:05,420 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-29 02:12:05,423 - INFO - Informed scheduler that task   Extract__99914b932b   has status   DONE
2024-12-29 02:12:05,423 - DEBUG - Asking scheduler for work...
2024-12-29 02:12:05,425 - DEBUG - Pending tasks: 2
2024-12-29 02:12:05,425 - INFO - [pid 320990] Worker Worker(salt=6036307926, workers=1, host=MSI, username=alfayyedh, pid=320990) running   Load()
2024-12-29 02:12:05,430 - INFO - Read Load Query - SUCCESS
2024-12-29 02:12:06,468 - INFO - Read Extracted Data - SUCCESS
2024-12-29 02:12:06,469 - INFO - Connect to DWH - SUCCESS
2024-12-29 02:12:06,690 - INFO - Truncate public Schema in DWH - SUCCESS
2024-12-29 02:12:06,690 - INFO - ==================================STARTING LOAD DATA=======================================
2024-12-29 02:12:06,704 - INFO - LOAD 'public.product_category_name_translation' - SUCCESS
2024-12-29 02:12:08,496 - INFO - LOAD 'public.products' - SUCCESS
2024-12-29 02:12:08,884 - INFO - LOAD 'public.geolocation' - SUCCESS
2024-12-29 02:12:08,965 - INFO - LOAD 'public.sellers' - SUCCESS
2024-12-29 02:12:12,045 - INFO - LOAD 'public.customers' - SUCCESS
2024-12-29 02:12:16,828 - INFO - LOAD 'public.orders' - SUCCESS
2024-12-29 02:12:21,793 - INFO - LOAD 'public.order_reviews' - SUCCESS
2024-12-29 02:12:25,673 - INFO - LOAD 'public.order_payments' - SUCCESS
2024-12-29 02:12:33,455 - INFO - LOAD 'public.order_items' - SUCCESS
2024-12-29 02:12:33,455 - INFO - LOAD All Tables To DWH-Olist - SUCCESS
2024-12-29 02:12:54,310 - INFO - LOAD All Tables To DWH-Staging - SUCCESS
2024-12-29 02:12:54,313 - INFO - ==================================ENDING LOAD DATA=======================================
2024-12-29 02:12:54,400 - INFO - [pid 320990] Worker Worker(salt=6036307926, workers=1, host=MSI, username=alfayyedh, pid=320990) done      Load()
2024-12-29 02:12:54,401 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-29 02:12:54,406 - INFO - Informed scheduler that task   Load__99914b932b   has status   DONE
2024-12-29 02:12:54,406 - DEBUG - Asking scheduler for work...
2024-12-29 02:12:54,410 - DEBUG - Pending tasks: 1
2024-12-29 02:12:54,411 - INFO - [pid 320990] Worker Worker(salt=6036307926, workers=1, host=MSI, username=alfayyedh, pid=320990) running   Transform()
2024-12-29 02:12:54,413 - INFO - Read Transform Query - SUCCESS
2024-12-29 02:12:54,415 - INFO - Connect to DWH - SUCCESS
2024-12-29 02:12:54,415 - INFO - ==================================STARTING TRANSFROM DATA=======================================
2024-12-29 02:12:54,601 - INFO - Transform to 'final.dim_geolocation' - SUCCESS
2024-12-29 02:12:56,060 - INFO - Transform to 'final.dim_customer' - SUCCESS
2024-12-29 02:12:56,232 - INFO - Transform to 'final.dim_product' - SUCCESS
2024-12-29 02:13:03,980 - INFO - Transform to 'final.fct_order_delivery' - SUCCESS
2024-12-29 02:13:12,323 - INFO - Transform to 'final.fct_review' - SUCCESS
2024-12-29 02:13:12,327 - INFO - Transform to All Dimensions and Fact Tables - SUCCESS
2024-12-29 02:13:12,329 - INFO - ==================================ENDING TRANSFROM DATA=======================================
2024-12-29 02:13:12,329 - INFO - [pid 320990] Worker Worker(salt=6036307926, workers=1, host=MSI, username=alfayyedh, pid=320990) done      Transform()
2024-12-29 02:13:12,329 - DEBUG - 1 running tasks, waiting for next task to finish
2024-12-29 02:13:12,332 - INFO - Informed scheduler that task   Transform__99914b932b   has status   DONE
2024-12-29 02:13:12,332 - DEBUG - Asking scheduler for work...
2024-12-29 02:13:12,334 - DEBUG - Done
2024-12-29 02:13:12,334 - DEBUG - There are no more tasks to run at this time
2024-12-29 02:13:12,334 - INFO - Worker Worker(salt=6036307926, workers=1, host=MSI, username=alfayyedh, pid=320990) was stopped. Shutting down Keep-Alive thread
2024-12-29 02:13:12,336 - INFO - 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 Extract()
    - 1 Load()
    - 1 Transform()

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

